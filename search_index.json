[
["index.html", "EST-46111: Fundamentos de Estad√≠stica con Remuestreo Informaci√≥n del curso Temario Evaluaci√≥n", " EST-46111: Fundamentos de Estad√≠stica con Remuestreo Teresa Ortiz (001), Alfredo Garbuno (002), Felipe Gonz√°lez Informaci√≥n del curso Notas del curso Fundamentos de Estad√≠stica con Remuestreo del programa de maestr√≠a en Ciencia de Datos del ITAM. En caso de encontrar errores o tener sugerencias del material se agradece la propuesta de correcciones mediante pull requests. Ligas Notas: https://fundamentos-est.netlify.app/, https://tereom.github.io/fundamentos/ Correos: teresa.ortiz.mancera@gmail.com, alfredo.garbuno@itam.mx. GitHub: https://github.com/tereom/fundamentos Foros de discusion: - Grupo Teresa: slack - Grupo Alfredo: canvas Este trabajo est√° bajo una Licencia Creative Commons Atribuci√≥n 4.0 Internacional. Temario Datos y an√°lisis exploratorio Referencias: (Cleveland 1994), (Chihara and Hesterberg 2018) Visualizaci√≥n y an√°lisis exploratorio Tipos de datos o estudios Muestras dise√±adas y muestras naturales Experimentos y datos observacionales Introducci√≥n a Pruebas de Hip√≥tesis Referencias: (Chihara and Hesterberg 2018) Introducci√≥n a pruebas de hip√≥tesis. Pruebas de permutaciones Muestras pareadas y otros ejemplos Estimaci√≥n y distribuci√≥n de muestreo Referencias: (Chihara and Hesterberg 2018), (Hesterberg 2015) Estimadores y su distribuci√≥n de muestreo Repaso de probabilidad y Teorema del l√≠mite central Introducci√≥n a estimaci√≥n por intervalos Referencias: (Chihara and Hesterberg 2018), (Efron and Tibshirani 1993), (Hesterberg 2015) El m√©todo plugin y el boostrap Bootstrap e Intervalos de confianza. Ejemplos. Estimaci√≥n Referencias: (Chihara and Hesterberg 2018), (Wasserman 2013) Estimaci√≥n por m√°xima verosimilitud Ejemplos de estimaci√≥n por m√°xima verosimilitud y Bootstrap param√©trico Propiedades de estimadores de m√°xima verosimilitud M√°s de pruebas de hip√≥tesis Referencias: (Chihara and Hesterberg 2018), (Wasserman 2013) Pruebas de hip√≥tesis para medias y proporciones: una y dos poblaciones. Introducci√≥n a inferencia bayesiana Referencias: (Kruschke 2015) Introducci√≥n a inferencia bayesiana Ejemplos de distribuciones conjugadas Introducci√≥n a m√©todos computacionales b√°sicos: Muestreadores Metr√≥polis y Gibbs Evaluaci√≥n Tareas semanales 20% Parcial te√≥rico + parcial a casa 40% Final a casa 40% Referencias "],
["principios-de-visualizacion.html", "Secci√≥n 1 Principios de visualizacion El cuarteto de Ascombe Introducci√≥n Visualizaci√≥n popular de datos Teor√≠a de visualizaci√≥n de datos Ejemplo: gr√°fica de Minard", " Secci√≥n 1 Principios de visualizacion El cuarteto de Ascombe En 1971 un estad√≠stico llamado Frank Anscombe (fundador del departamento de Estad√≠stica de la Universidad de Yale) public√≥ cuatro conjuntos de dato. Cada uno consiste de 11 observaciones. La peculariedad de estos conjuntos es que tienen las mismas propiedades estad√≠sticas. Sin embargo, cuando analizamos los datos de manera gr√°fica en un histograma encontramos r√°pidamente que los conjuntos de datos son muy distintos. Media de \\(x\\): 9 Varianza muestral de \\(x\\): 11 Media de \\(y\\): 7.50 Varianza muestral de \\(y\\): 4.12 Correlaci√≥n entre \\(x\\) y \\(y\\): 0.816 L√≠nea de regresi√≥n lineal: \\(y = 3.00 + 0.500x\\) En la gr√°fica del primer conjunto de datos, se ve clara una relaci√≥n lineal simple con un modelo que cumple los supuestos de normalidad. La segunda gr√°fica (arriba a la derecha) muestra unos datos que tienen una asociaci√≥n pero definitivamente no es lineal. En la tercera gr√°fica (abajo a la izquierda) est√°n puntos alineados perfectamente en una l√≠nea recta, excepto por uno de ellos. En la √∫ltima gr√°fica podemos ver un ejemplo en el cual basta tener una observaci√≥n at√≠pica para que se produzca un coeficiente de correlaci√≥n alto a√∫n cuando en realidad no existe una asociaci√≥n lineal entre las dos variables. El cuarteto de Ascombe inspir√≥ una t√©cnica reciente para crear datos que comparten las mismas propiedades estad√≠sticas al igual que en el cuarteto, pero que producen gr√°ficas muy distintas (Matejka, Fitzmaurice). Introducci√≥n La visualizaci√≥n de datos no trata de hacer gr√°ficas ‚Äúbonitas‚Äù o ‚Äúdivertidas‚Äù, ni de simplificar lo complejo o ayudar a una persona ‚Äúque no entiende mucho‚Äù a entender ideas complejas. M√°s bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. El siguiente ejemplo de (Tufte 2006), ilustra claramente la diferencia entre estos dos enfoques. A la izquierda est√°n gr√°ficas (m√°s o menos t√≠picas de Powerpoint) basadas en la filosof√≠a de simplificar, de intentar no ‚Äúahogar‚Äù al lector con datos. El resultado es una colecci√≥n incoherente, de bajo contenido, que no tiene mucho qu√© decir y que es, ‚Äúindeferente al contenido y la evidencia‚Äù. A la derecha est√° una variaci√≥n del redise√±o de Tufte en forma de tabla, que en este caso particular es una manera eficiente de mostrar claramente los patrones que hay en este conjunto simple de datos. ¬øQu√© principios son los que soportan la efectividad de esta tabla sobre la gr√°fica de la derecha? Veremos que hay dos conjuntos de principios importantes: unos relacionados con el dise√±o y otros con la naturaleza del an√°lisis de datos, independientemente del m√©todo de visualizaci√≥n. Visualizaci√≥n popular de datos Publicaciones populares (peri√≥dicos, revistas, sitios internet) muchas veces incluyen visualizaci√≥n de datos como parte de sus art√≠culos o reportajes. En general siguen el mismo patr√≥n que en la visi√≥n tradicionalista de la estad√≠stica: sirven m√°s para divertir que para explicar, tienden a explicar ideas simples y conjuntos chicos de datos, y se consideran como una ‚Äúayuda‚Äù para los ‚Äúlectores menos sofisticados‚Äù. Casi siempre se trata de gr√°ficas triviales (muchas veces con errores graves) que no aportan mucho a art√≠culos que tienen un nivel de complejidad mucho mayor (es la filosof√≠a: lo escrito para el adulto, lo graficado para el ni√±o). Teor√≠a de visualizaci√≥n de datos Existe teor√≠a fundamentada acerca de la visualizaci√≥n. Despu√©s del trabajo pionero de Tukey, los principios e indicadores de Tufte se basan en un estudio de la historia de la graficaci√≥n y ejercicios de muestreo de la pr√°ctica gr√°fica a lo largo de varias disciplinas (¬øcu√°les son las mejores gr√°ficas? ¬øpor qu√©? El trabajo de Cleveland es orientado a la pr√°ctica del an√°lisis de datos (¬øcu√°les gr√°ficas nos han ayudado a mostrar claramente los resultados del an√°lisis?), por una parte, y a algunos estudios de percepci√≥n visual. En resumen, hablaremos de las siguientes gu√≠as: Principios generales del dise√±o anal√≠tico Aplicables a una presentaci√≥n o an√°lisis completos, y como gu√≠a para construir nuevas visualizaciones (Tufte 2006). Principio 1. Muestra comparaciones, contrastes, diferencias. Principio 2. Muestra causalidad, mecanismo, explicaci√≥n, estructura sistem√°tica. Principio 3. Muestra datos multivariados, es decir, m√°s de una o dos variables. Principio 4. Integra palabras, n√∫meros, im√°genes y diagramas. Principio 5. Describe la totalidad de la evidencia. Muestra fuentes usadas y problemas relevantes. Principio 6. Las presentaciones anal√≠ticas, a fin de cuentas, se sostienen o caen dependiendo de la calidad, relevancia e integridad de su contenido. T√©cnicas de visualizaci√≥n Esta categor√≠a incluye t√©cnicas espec√≠ficas que dependen de la forma de nuestros datos y el tipo de pregunta que queremos investigar (Tukey (1977), Cleveland (1993), Cleveland (1994), Tufte (2006)). Tipos de gr√°ficas: cuantiles, histogramas, caja y brazos, gr√°ficas de dispersi√≥n, puntos/barras/ l√≠neas, series de tiempo. T√©cnicas para mejorar gr√°ficas: Transformaci√≥n de datos, transparencia, vibraci√≥n, banking 45, suavizamiento y bandas de confianza. Peque√±os m√∫ltiplos Indicadores de calidad gr√°fica Aplicables a cualquier gr√°fica en particular. Estas son gu√≠as concretas y relativamente objetivas para evaluar la calidad de una gr√°fica (Tufte 1986). Integridad Gr√°fica. El factor de enga√±o, es decir, la distorsi√≥n gr√°fica de las cantidades representadas, debe ser m√≠nimo. Chartjunk. Minimizar el uso de decoraci√≥n gr√°fica que interfiera con la interpretaci√≥n de los datos: 3D, rejillas, rellenos con patrones. Tinta de datos. Maximizar la proporci√≥n de tinta de datos vs. tinta total de la gr√°fica. For non-data- ink, less is more. For data-ink, less is a bore. Densidad de datos. Las mejores gr√°ficas tienen mayor densidad de datos, que es la raz√≥n entre el tama√±o del conjunto de datos y el √°rea de la gr√°fica. Las gr√°ficas se pueden encoger mucho. Percepci√≥n visual. Algunas tareas son m√°s f√°ciles para el ojo humano que otras (Cleveland 1994). Factor de enga√±o y Chartjunk El factor de enga√±o es el cociente entre el efecto mostrado en una gr√°fica y el efecto correspondiente en los datos. Idealmente, el factor de enga√±o debe ser 1 (ninguna distorsi√≥n). El chartjunk son aquellos elementos gr√°ficos que no corresponden a variaci√≥n de datos, o que entorpecen la interpretaci√≥n de una gr√°fica. Estos son los indicadores de calidad m√°s f√°ciles de entender y aplicar, y afortunadamente cada vez son menos comunes. Un dise√±o popular que califica como chartjunk y adem√°s introduce factores de enga√±o es el pie de 3D. En la gr√°fica de la derecha, podemos ver como la rebanada C se ve m√°s grande que la rebanada A, aunque claramente ese no es el caso (factor de enga√±o). La raz√≥n es la variaci√≥n en la perspectiva que no corresponde a variaci√≥n en los datos (chartjunk). Cr√≠tica gr√°fica: Gr√°fica de pie Todav√≠a elementos que pueden mejorar la comprensi√≥n de nuestra gr√°fica de pie: se trata de la decodificiaci√≥n que hay que hacer categor√≠a - color - cuantificaci√≥n. Podemos agregar las etiquetas como se muestra en la serie de la derecha, pero entonces: ¬øpor qu√© no mostrar simplemente la tabla de datos? ¬øqu√© agrega el pie a la interpretaci√≥n? La deficiencias en el pie se pueden ver claramente al intentar graficar m√°s categor√≠as (13) . En el primer pie no podemos distinguir realmente cu√°les son las categor√≠as grandes y cu√°les las chicas, y es muy dif√≠cil tener una imagen mental clara de estos datos. Agregar los porcentajes ayuda, pero entonces, otra vez, preguntamos cu√°l es el prop√≥sito del pie. La tabla de la izquierda hace todo el trabajo (una vez que ordenamos las categr√≠as de la m√°s grande a la m√°s chica). Es posible hacer una gr√°fica de barras como la de abajo a la izquierda. Hay otros tipos de chartjunk comunes: uno es la textura de barras, por ejemplo. El efecto es la producci√≥n de un efecto moir√© que es desagradable y quita la atenci√≥n de los datos, como en la gr√°fica de barras de abajo. Otro com√∫n son las rejillas, como mostramos en las gr√°ficas de la izquierda. N√≥tese como en estos casos hay efectos √≥pticos no planeados que degradan la percepci√≥n de los patrones en los datos. Peque√±os m√∫ltiplos y densidad gr√°fica La densidad de una gr√°fica es el tama√±o del conjunto de datos que se grafica comparado con el √°rea total de la gr√°fica. En el siguiente ejemplo, graficamos en logaritmo-10 de cabezas de ganado en Francia (cerdos, res, ovejas y caballos). La gr√°fica de la izquierda es pobre en densidad pues s√≥lo representa 4 datos. La manera m√°s f√°cil de mejorar la densidad es hacer m√°s chica la gr√°fica: La raz√≥n de este encogimiento es una que tiene qu√© ver con las oportunidades perdidas de una gr√°fica grande. Si repetimos este mismo patr√≥n (misma escala, mismos tipos de ganado) para distintos pa√≠ses obtenemos la siguiente gr√°fica: Esta es una gr√°fica de puntos. Es √∫til como sustituto de una gr√°fica de barras, y es superior en el sentido de que una mayor proporci√≥n de la tinta que se usa es tinta de datos. Otra vez, mayor proporci√≥n de tinta de datos representa m√°s oportunidades que se pueden capitalizar, como muestra la gr√°fica de punto y l√≠neas que mostramos al principio (rendimiento en campos de cebada). M√°s peque√±os m√∫ltiplos Los peque√±os m√∫ltiplos presentan oportunidades para mostrar m√°s acerca de nuestro problema de inter√©s. Consideramos por ejemplo la relaci√≥n de radiaci√≥n solar y niveles de ozono: ggplot(airquality, aes(x=Solar.R, y=Ozone)) + geom_point() + geom_smooth(method = &quot;loess&quot;, span = 1) En el ejemplo anterior incluyendo una variable adicional (velocidad del viento) podemos entender m√°s acerca de la relaci√≥n de radiaci√≥n solar y niveles de ozono: airquality$Wind_cat &lt;- cut(airquality$Wind, breaks = quantile(airquality$Wind, c(0, 1/3, 2/3, 1)), include.lowest = TRUE) ggplot(airquality, aes(x=Solar.R, y=Ozone)) + geom_point() + facet_wrap(~Wind_cat) + geom_smooth(method = &quot;loess&quot;, span = 0.8, se = FALSE, method.args = list(degree = 1, family=&quot;symmetric&quot;)) Tinta de datos Maximizar la proporci√≥n de tinta de datos en nuestras gr√°ficas tiene beneficios inmediatos. La regla es: si hay tinta que no representa variaci√≥n en los datos, o la eliminaci√≥n de esa tinta no representa p√©rdidas de significado, esa tinta debe ser eliminada. El ejemplo m√°s claro es el de las rejillas en gr√°ficas y tablas: ¬øPor qu√© usar grises en lugar de negros? La respuesta tiene qu√© ver con el principio de tinta de datos: si marcamos las diferencias sutil pero claramente, tenemos m√°s oportunidades abiertas para hacer √©nfasis en lo que nos interesa: a una gr√°fica o tabla saturada no se le puede hacer m√°s - es dif√≠cil agregar elementos adicionales que ayuden a la comprensi√≥n. Si comenzamos marcando con sutileza, entonces se puede hacer m√°s. Los mapas geogr√°ficos son un buen ejemplo de este principio. El espacio en blanco es suficientemente bueno para indicar las fronteras en una tabla, y facilita la lectura: Para un ejemplo del proceso de redise√±o de una tabla, ver aqu√≠. Finalmente, podemos ver un ejemplo que intenta incorporar los elementos del dise√±o anal√≠tico, incluyendo peque√±os m√∫ltiplos: Decoraci√≥n Percepci√≥n de escala Entre la percepci√≥n visual y la interpretaci√≥n de una gr√°fica est√°n impl√≠citas tareas visuales espec√≠ficas que las personas debemos realizar para ver correctamente la gr√°fica. En la d√©cada de los ochenta, William S. Cleveland y Robert McGill realizaron algunos experimentos identificando y clasificando estas tareas para diferentes tipos de gr√°ficos (Cleveland and McGill 1984). En estos, se le pregunta a la persona que compare dos valores dentro de una gr√°fica, por ejemplo, en dos barras en una gr√°fica de barras, o dos rebanadas de una gr√°fica de pie. Los resultados de Cleveland y McGill fueron replicados por Heer y Bostock en 2010 y los resultados se muestran en las gr√°ficas de la derecha: Ejemplo: gr√°fica de Minard Concluimos esta secci√≥n con una gr√°fica que, aunque poco com√∫n, ejemplifica los principios de una buena gr√°fica, y es reconocida como una de las mejores visualizaciones de la historia. Una gr√°fica excelente, presenta datos interesantes de forma bien dise√±ada: es una cuesti√≥n de fondo, de dise√±o, y estad√≠stica‚Ä¶ [Se] compone de ideas complejas comunicadas con claridad, precisi√≥n y eficiencia. ‚Ä¶ [Es] lo que da al espectador la mayor cantidad de ideas, en el menor tiempo, con la menor cantidad de tinta, y en el espacio m√°s peque√±o. ‚Ä¶ Es casi siempre multivariado. ‚Ä¶ Una excelente gr√°fica debe decir la verdad acerca de los datos. (Tufte, 1983) La famosa visualizaci√≥n de Charles Joseph Minard de la marcha de Napole√≥n sobre Mosc√∫, ilustra los principios de una buena gr√°fica. Tufte se√±ala que esta imagen ‚Äúbien podr√≠a ser el mejor gr√°fico estad√≠stico jam√°s dibujado‚Äù, y sostiene que ‚Äúcuenta una historia rica y coherente con sus datos multivariados, mucho m√°s esclarecedora que un solo n√∫mero que rebota en el tiempo‚Äù. Se representan seis variables: el tama√±o del ej√©rcito, su ubicaci√≥n en una superficie bidimensional, la direcci√≥n del movimiento del ej√©rcito y la temperatura en varias fechas durante la retirada de Mosc√∫\". Hoy en d√≠a Minard es reconocido como uno de los principales contribuyentes a la teor√≠a de an√°lisis de datos y creaci√≥n de infograf√≠as con un fundamento estad√≠stico. Se grafican 6 variables: el n√∫mero de tropas de Napole√≥n, la distancia, la temperatura, la latitud y la longitud, la direcci√≥n en que viajaban las tropas y la localizaci√≥n relativa a fechas espec√≠ficas. La gr√°fica de Minard, como la describe E.J. Marey, parece ‚Äúdesafiar la pluma del historiador con su brutal elocuencia‚Äù, la combinaci√≥n de datos del mapa, y la serie de tiempo, dibujados en 1869, ‚Äúretratan una secuencia de p√©rdidas devastadoras que sufrieron las tropas de Napole√≥n en 1812‚Äù. Comienza en la izquierda, en la frontera de Polonia y Rusia, cerca del r√≠o Niemen. La l√≠nea gruesa dorada muestra el tama√±o de la Gran Armada (422,000) en el momento en que invad√≠a Rusia en junio de 1812. El ancho de esta banda indica el tama√±o de la armada en cada punto del mapa. En septiembre, la armada lleg√≥ a Mosc√∫, que ya hab√≠a sido saqueada y dejada des√©rtica, con s√≥lo 100,000 hombres. El camino del retiro de Napole√≥n desde Mosc√∫ est√° representado por la l√≠nea oscura (gris) que est√° en la parte inferior, que est√° relacionada a su vez con la temperatura y las fechas en el diagrama de abajo. Fue un invierno muy fr√≠o, y muchos se congelaron en su salida de Rusia. Como se muestra en el mapa, cruzar el r√≠o Berezina fue un desastre, y el ej√©rcito de Napole√≥n logr√≥ regresar a Polonia con tan s√≥lo 10,000 hombres. Tambi√©n se muestran los movimientos de las tropas auxiliaries, que buscaban proteger por atr√°s y por la delantera mientras la armada avanzaba hacia Mosc√∫. La gr√°fica de Minard cuenta una historia rica y cohesiva, coherente con datos multivariados y con los hechos hist√≥ricos, y que puede ser m√°s ilustrativa que tan s√≥lo representar un n√∫mero rebotando a lo largo del tiempo. Referencias "],
["analisis-exploratorio.html", "Secci√≥n 2 Analisis exploratorio El papel de la exploraci√≥n en el an√°lisis de datos Algunos conceptos b√°sicos Ejemplos Interpretaci√≥n 2.1 Loess", " Secci√≥n 2 Analisis exploratorio ‚ÄúExploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone ‚Äìas the first step.‚Äù ‚Äî John Tukey ‚ÄúThe simple graph has brought more information to the data analyst‚Äôs mind than any other device.‚Äù ‚Äî John Tukey El papel de la exploraci√≥n en el an√°lisis de datos El est√°ndar cient√≠fico para contestar preguntas o tomar decisiones es uno que se basa en el an√°lisis de datos. Es decir, en primer lugar se deben reunir todos los datos disponibles que puedan contener o sugerir alguna gu√≠a para entender mejor la pregunta o la decisi√≥n a la que nos enfrentamos. Esta recopilaci√≥n de datos ‚Äîque pueden ser cualitativos, cuantitativos, o una mezcla de los dos‚Äî debe entonces ser analizada para extraer informaci√≥n relevante para nuestro problema. En an√°lisis de datos existen dos distintos tipos de trabajo: El trabajo exploratorio o de detective: ¬øcu√°les son los aspectos importantes de estos datos? ¬øqu√© indicaciones generales muestran los datos? ¬øqu√© tareas de an√°lisis debemos empezar haciendo? ¬øcu√°les son los caminos generales para formular con precisi√≥n y contestar algunas preguntas que nos interesen? El trabajo inferencial, confirmatorio, o de juez: ¬øc√≥mo evaluar el peso de la evidencia de los descubrimientos del paso anterior? ¬øqu√© tan bien soportadas est√°n las respuestas y conclusiones por nuestro conjunto de datos? Algunos conceptos b√°sicos Empezamos explicando algunas ideas que no ser√°n √∫tiles m√°s adelante. Por ejemplo, los siguientes datos fueron registrados en un restaurante durante cuatro d√≠as consecutivos: library(tidyverse) library(patchwork) source(&quot;R/funciones_auxiliares.R&quot;) # usamos los datos tips del paquete reshape2 tips &lt;- reshape2::tips # renombramos variables y niveles propinas &lt;- tips %&gt;% rename(cuenta_total = total_bill, propina = tip, sexo = sex, fumador = smoker, dia = day, momento = time, num_personas = size) %&gt;% mutate(sexo = recode(sexo, Female = &quot;Mujer&quot;, Male = &quot;Hombre&quot;), fumador = recode(fumador, No = &quot;No&quot;, Si = &quot;Si&quot;), dia = recode(dia, Sun = &quot;Dom&quot;, Sat = &quot;Sab&quot;, Thur = &quot;Jue&quot;, Fri = &quot;Vie&quot;), momento = recode(momento, Dinner = &quot;Cena&quot;, Lunch = &quot;Comida&quot;)) %&gt;% select(-sexo) %&gt;% mutate(dia = fct_relevel(dia, c(&quot;Jue&quot;, &quot;Vie&quot;, &quot;Sab&quot;, &quot;Dom&quot;))) Y vemos una muestra sample_n(propinas, 10) %&gt;% formatear_tabla() ## Warning in kableExtra::kable_styling(., latex_options = c(&quot;striped&quot;), ## bootstrap_options = c(&quot;striped&quot;, : Please specify format in kable. kableExtra ## can customize either HTML or LaTeX outputs. See https://haozhu233.github.io/ ## kableExtra/ for details. cuenta_total propina fumador dia momento num_personas 16.40 2.50 Yes Jue Comida 2 10.63 2.00 Yes Sab Cena 2 27.28 4.00 Yes Vie Cena 2 26.59 3.41 Yes Sab Cena 3 20.76 2.24 No Dom Cena 2 28.15 3.00 Yes Sab Cena 5 14.31 4.00 Yes Sab Cena 2 19.81 4.19 Yes Jue Comida 2 20.69 2.45 No Sab Cena 4 25.28 5.00 Yes Sab Cena 2 Aqu√≠ la unidad de observaci√≥n es una cuenta particular. Tenemos tres mediciones num√©ricas de cada cuenta: c√∫anto fue la cuenta total, la propina, y el n√∫mero de personas asociadas a la cuenta. Los datos est√°n separados seg√∫n se fum√≥ o no en la mesa, y temporalmente en dos partes: el d√≠a (Jueves, Viernes, S√°bado o Domingo), cada uno separado por Cena y Comida. Denotamos por \\(x\\) el valor de medici√≥n de una unidad de observaci√≥n. Usualmente utilizamos sub-√≠ndices para identificar entre diferentes puntos de datos (observaciones), por ejemplo, \\(x_n\\) para la \\(n-\\)√©sima observaci√≥n. De tal forma que una colecci√≥n de \\(N\\) observaciones la escribimos como \\[\\begin{align} \\{x_1, \\ldots, x_N\\}. \\end{align}\\] El primer tipo de comparaciones que nos interesa hacer es para una medici√≥n: ¬øVar√≠an mucho o poco los datos de un tipo de medici√≥n? ¬øCu√°les son valores t√≠picos o centrales? ¬øExisten valores at√≠picos? Supongamos entonces que consideramos simplemente la variable de cuenta_total. Podemos comenzar por ordenar los datos, y ver cu√°les datos est√°n en los extremos y cu√°les est√°n en los lugares centrales: En general la colecci√≥n de datos no est√° ordenada por sus valores. Esto es debido a que las observaciones en general se recopilan de manera aleatoria. Utilizamos la notaci√≥n de \\(\\sigma(n)\\) para denotar un reordenamiento de los datos de tal forma \\[\\begin{align} \\{x_{\\sigma(1)}, \\ldots, x_{\\sigma(N)}\\}, \\end{align}\\] y que satisface la siguiente serie de desigualdades \\[\\begin{align} x_{\\sigma(1)} \\leq \\ldots \\leq x_{\\sigma(N)}. \\end{align}\\] propinas &lt;- propinas %&gt;% mutate(orden_cuenta = rank(cuenta_total, ties.method = &quot;first&quot;), f = (orden_cuenta - 0.5) / n()) cuenta &lt;- propinas %&gt;% select(orden_cuenta, f, cuenta_total) %&gt;% arrange(f) bind_rows(head(cuenta), tail(cuenta)) %&gt;% formatear_tabla() orden_cuenta f cuenta_total 1 0.0020492 3.07 2 0.0061475 5.75 3 0.0102459 7.25 4 0.0143443 7.25 5 0.0184426 7.51 6 0.0225410 7.56 239 0.9774590 44.30 240 0.9815574 45.35 241 0.9856557 48.17 242 0.9897541 48.27 243 0.9938525 48.33 244 0.9979508 50.81 Tambi√©n podemos graficar los datos en orden, interpolando valores consecutivos. g_orden &lt;- ggplot(cuenta, aes(y = orden_cuenta, x = cuenta_total)) + geom_point(colour = &quot;red&quot;, alpha = 0.5) + labs(subtitle = &quot;Cuenta total&quot;) g_cuantiles &lt;- ggplot(cuenta, aes(y = f, x = cuenta_total)) + geom_point(colour = &quot;red&quot;, alpha = 0.5) + geom_line(alpha = 0.5) + labs(subtitle = &quot;&quot;) g_orden + g_cuantiles A esta funci√≥n le llamamos la funci√≥n de cuantiles para la variable cuenta_total. Nos sirve para comparar directamente los distintos valores que observamos los datos seg√∫n el orden que ocupan. La funci√≥n de cuantiles muestral esta definida por \\[\\begin{align} \\hat{F}(x) = \\frac1N \\sum_{n = 1}^N \\mathbb{1}\\{x_n \\leq x\\}, \\end{align}\\] donde la funcion indicadora est√° definida por \\[\\begin{align} 1\\{ x \\leq t\\} = \\begin{cases} 1, \\text{ si } x \\leq t \\\\ 0, \\text{ en otro caso} \\end{cases}. \\end{align}\\] El cuantil \\(p\\) es el valor \\(x = x(p)\\), esta notaci√≥n sirve para definir \\(x\\) como una funci√≥n de \\(p,\\) tal que \\[\\begin{align} \\hat F(x) = p. \\end{align}\\] Es decir, \\(x\\) acumula el \\(p\\)-% de los casos. Para una medici√≥n de inter√©s \\(x\\) con posibles valores en el intervalo \\([a, b]\\). Comprueba que \\(\\hat F(a) = 0\\) y \\(\\hat F(b) = 1\\) para cualquier colecci√≥n de datos de tama√±o \\(N.\\) La gr√°fica anterior, tambi√©n nos sirve para poder estudiar la dispersi√≥n y valores centrales de los datos observados. Por ejemplo, podemos notar que: El rango de datos va de unos 3 d√≥lares hasta 50 d√≥lares Los valores centrales ‚Äîdel cuantil 0.25 al 0.75, por decir un ejemplo‚Äî est√°n entre unos 13 y 25 d√≥lares El cuantil 0.5 (o tambi√©n conocido como mediana) est√° alrededor de 18 d√≥lares. ¬øC√≥mo definir√≠as la mediana en t√©rminos de la funci√≥n de cuantiles? Pista: Considera los casos por separado para \\(N\\) impar o par. √âste √∫ltimo puede ser utilizado para dar un valor central de la distribuci√≥n de valores para cuenta_total. Asimismo podemos dar res√∫menes m√°s refinados si es necesario. Por ejemplo, podemos reportar que: El cuantil 0.95 es de unos 35 d√≥lares ‚Äî s√≥lo 5% de las cuentas son de m√°s de 35 d√≥lares El cuantil 0.05 es de unos 8 d√≥lares ‚Äî s√≥lo 5% de las cuentas son de 8 d√≥lares o menos. Finalmente, la forma de la gr√°fica se interpreta usando su pendiente (tasa de cambio) haciendo comparaciones en diferentes partes de la gr√°fica: La distribuci√≥n de valores tiene asimetr√≠a: el 10% de las cuentas m√°s altas tiene considerablemente m√°s dispersi√≥n que el 10% de las cuentas m√°s bajas. Entre los cuantiles 0.2 y 0.5 es donde existe mayor densidad de datos: la pendiente (tasa de cambio) es alta, lo que significa que al avanzar en los valores observados, los cuantiles (el porcentaje de casos) aumenta r√°pidamente. Cuando la pendiente es casi plana, quiere decir que los datos tienen m√°s dispersi√≥n local o est√°n m√°s separados. En algunos casos, es m√°s natural hacer un histograma, donde dividimos el rango de la variable en cubetas o intervalos (en este caso de igual longitud), y graficamos por medio de barras cu√°ntos datos caen en cada cubeta: Es una gr√°fica m√°s popular, pero perdemos cierto nivel de detalle, y distintas particiones resaltan distintos aspectos de los datos. ¬øC√≥mo se ve la gr√°fica de cuantiles de las propinas? ¬øC√≥mo crees que esta gr√°fica se compara con distintos histogramas? g_1 &lt;- ggplot(propinas, aes(sample = propina)) + geom_qq(distribution = stats::qunif) + xlab(&quot;f&quot;) + ylab(&quot;propina&quot;) g_1 Observaci√≥n. Cuando hay datos repetidos, los cuantiles tienen que interpretarse como sigue: el cuantil-\\(f\\) con valor \\(q\\) satisface que existe una proporci√≥n aproximada \\(f\\) de los datos que est√°n en el valor \\(q\\) o por debajo de √©ste, pero no necesariamente exactamente una proporci√≥n \\(f\\) de los datos estan en \\(q\\) o por debajo. Finalmente, una gr√°fica m√°s compacta que resume la gr√°fica de cuantiles o el histograma es el diagrama de caja y brazos. Mostramos dos versiones, la cl√°sica de Tukey (T) y otra versi√≥n menos com√∫n de Spear/Tufte (ST): library(ggthemes) cuartiles &lt;- quantile(cuenta$cuenta_total) t(cuartiles) %&gt;% formatear_tabla() 0% 25% 50% 75% 100% 3.07 13.3475 17.795 24.1275 50.81 g_1 &lt;- ggplot(cuenta, aes(x = f, y = cuenta_total)) + labs(subtitle = &quot;Gr√°fica de cuantiles: Cuenta total&quot;) + geom_hline(yintercept = cuartiles[2], colour = &quot;gray&quot;) + geom_hline(yintercept = cuartiles[3], colour = &quot;gray&quot;) + geom_hline(yintercept = cuartiles[4], colour = &quot;gray&quot;) + geom_point(alpha = 0.5) + geom_line() g_2 &lt;- ggplot(cuenta, aes(x = factor(&quot;ST&quot;, levels =c(&quot;ST&quot;)), y = cuenta_total)) + geom_tufteboxplot() + labs(subtitle = &quot; &quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) g_3 &lt;- ggplot(cuenta, aes(x = factor(&quot;T&quot;), y = cuenta_total)) + geom_boxplot() + labs(subtitle = &quot; &quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) g_4 &lt;- ggplot(cuenta, aes(x = factor(&quot;P&quot;), y = cuenta_total)) + geom_jitter(height = 0, width =0.2, alpha = 0.5) + labs(subtitle = &quot; &quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) g_5 &lt;- ggplot(cuenta, aes(x = factor(&quot;V&quot;), y = cuenta_total)) + geom_violin() + labs(subtitle = &quot; &quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) g_1 + g_2 + g_3 + g_4 + plot_layout(widths = c(8, 2, 2, 2)) El diagrama de la derecha explica los elementos de la versi√≥n t√≠pica del diagrama de caja y brazos (boxplot). RIC se refiere al *Rango Intercuant√≠lico**, definido por la diferencia entre los cuantiles 25% y 75%. Figura: Jumanbar / CC BY-SA Hasta ahora hemos utilizado la definici√≥n general de cuantiles. Donde consideramos el cuantil \\(q\\), para buscar \\(x\\) tal que \\(\\hat F(x) = q.\\) Hay valores t√≠picos de inter√©s que corresponden a \\(q\\) igual a 25%, 50% y 75%. √âstos valores se denominan cuartiles. Ventajas en el an√°lisis inicial En un principio del an√°lisis, estos res√∫menes (cuantiles) pueden ser m√°s √∫tiles que utilizar medias y varianzas, por ejemplo. La raz√≥n es que los cuantiles: Son cantidades m√°s f√°cilmente interpretables Los cuantiles centrales son m√°s resistentes a valores at√≠picos que medias o varianzas Sin embargo, permite identificar valores extremos Es f√°cil comparar cuantiles de distintos bonches de datos Media y desviaci√≥n est√°ndar Las medidas m√°s comunes de localizaci√≥n y dispersi√≥n para un conjunto de datos son la media muestral y la desviaci√≥n est√°ndar muestral. En general, no son muy apropiadas para iniciar el an√°lisis exploratorio, pues: Son medidas m√°s dif√≠ciles de interpretar y explicar que los cuantiles. En este sentido, son medidas especializadas. Por ejemplo, intenta explicar intuitivamente qu√© es la media. No son resistentes a valores at√≠picos o err√≥neos. Su falta de resistencia los vuelve poco √∫tiles en las primeras etapas de limpieza y descripci√≥n. La media, o promedio, se denota por \\(\\bar x\\) y se define como \\[\\begin{align} \\bar x = \\frac1N \\sum_{n = 1}^N x_n. \\end{align}\\] La desviaci√≥n est√°ndar muestral se define como \\[\\begin{align} \\text{std}(x) = \\sqrt{\\frac1{N-1} \\sum_{n = 1}^N (x_n - \\bar x)^2}. \\end{align}\\] Sin embargo, La media y desviaci√≥n est√°ndar son computacionalmente convenientes. Para el trabajo de modelado estas medidas de resumen tienen ventajas claras (bajo ciertos supuestos te√≥ricos). En muchas ocasiones conviene usar estas medidas pues permite hacer comparaciones hist√≥ricas o tradicionales ‚Äîpues an√°lisis anteriores pudieran estar basados en √©stas. Considera el caso de tener \\(N\\) observaciones y asume que ya tienes calculado el promedio para dichas observaciones. Este promedio lo denotaremos por \\(\\bar x_N\\). Ahora, considera que has obtenido \\(M\\) observaciones m√°s. Escribe una f√≥rmula recursiva para la media del conjunto total de datos \\(\\bar x_{N+M}\\) en funci√≥n de lo que ya ten√≠as precalculado \\(\\bar x_N.\\) ¬øEn qu√© situaciones esta propiedad puede ser conveniente? Ejemplos Precios de casas En este ejemplo consideremos los datos de precios de ventas de la ciudad de Ames, Iowa. En particular nos interesa entender la variaci√≥n del precio de las casas. Por este motivo calculamos los cuantiles que corresponden al 25%, 50% y 75% (cuartiles), as√≠ como el m√≠nimo y m√°ximo de los precios de las casas: quantile(casas %&gt;% pull(precio_miles)) ## 0% 25% 50% 75% 100% ## 37.9 132.0 165.0 215.0 755.0 Comprueba que el m√≠nimo y m√°ximo est√°n asociados a los cuantiles 0% y 100%, respectivamente. Una posible comparaci√≥n es considerar los precios y sus variaci√≥n en funci√≥n de zona de la ciudad en que se encuentra una vivienda. Podemos usar diagramas de caja y brazos para hacer una comparaci√≥n burda de los precios en distintas zonas de la ciudad: ggplot(casas, aes(x = nombre_zona, y = precio_miles)) + geom_boxplot() + coord_flip() La primera pregunta que nos hacemos es c√≥mo pueden variar las caracter√≠sticas de las casas dentro de cada zona. Para esto, podemos considerar el √°rea de las casas. En lugar de graficar el precio, graficamos el precio por metro cuadrado, por ejemplo: ggplot(casas, aes(x = nombre_zona, y = precio_m2)) + geom_boxplot() + coord_flip() Podemos cuantificar la variaci√≥n que observamos de zona a zona y la variaci√≥n que hay dentro de cada una de las zonas. Una primera aproximaci√≥n es observar las variaci√≥n del precio al calcular la mediana dentro de cada zona, y despu√©s cuantificar por medio de cuantiles c√≥mo var√≠a la mediana entre zonas: casas %&gt;% group_by(nombre_zona) %&gt;% summarise(mediana_zona = median(precio_m2), .groups = &quot;drop&quot;) %&gt;% pull(mediana_zona) %&gt;% quantile() %&gt;% round() ## 0% 25% 50% 75% 100% ## 963 1219 1298 1420 1725 Tratar con datos por segmento es una situaci√≥n com√∫n en aplicaciones. Usualmente denotamos por \\[\\begin{align} x_{k, n} \\end{align}\\] a la \\(n\\)-√©sima observaci√≥n del \\(k\\)-√©simo grupo. Usualmente tenemos un universo de \\(K\\) posibles grupos y para cada grupo tenemos un total diferente de observaciones. Esto lo denotamos por \\(N_k\\), el n√∫mero total de observaciones del grupo \\(k\\) para cualquier \\(k = 1, \\ldots, K.\\) El n√∫mero total de muestras lo denotamos por \\(N\\), donde \\[\\begin{align} N = \\sum_{k=1}^K N_k. \\end{align}\\] Finalmente, nos puede interesar, como en el ejemplo, los promedios por grupo \\[\\begin{align} \\bar x_k = \\frac{1}{N_k} \\sum_{n = 1}^{N_k} x_{k, n}, \\end{align}\\] y contrastar contra el promedio total \\[\\begin{align} \\bar x = \\frac1K \\sum_{k = 1}^K \\bar x_k = \\frac1K \\sum_{k = 1}^K \\left( \\frac{1}{N_k} \\sum_{n = 1}^{N_k} x_{k, n} \\right). \\end{align}\\] Por otro lado, las variaciones con respecto a las medianas dentro de cada zona, por grupo, se resume como: quantile(casas %&gt;% group_by(nombre_zona) %&gt;% mutate(residual = precio_m2 - median(precio_m2)) %&gt;% pull(residual)) %&gt;% round() ## 0% 25% 50% 75% 100% ## -765 -166 0 172 1314 N√≥tese que este √∫ltimo paso tiene sentido pues la variaci√≥n dentro de las zonas, en t√©rminos de precio por metro cuadrado, es similar. Esto no lo podr√≠amos haber hecho de manera efectiva si se hubiera utilizado el precio de las casas sin ajustar por su tama√±o. Podemos resumir este primer an√°lisis con un par de gr√°ficas de cuantiles (Cleveland (1993)): mediana &lt;- median(casas$precio_m2) resumen &lt;- casas %&gt;% group_by(nombre_zona) %&gt;% mutate(mediana_zona = median(precio_m2)) %&gt;% mutate(residual = precio_m2 - mediana_zona) %&gt;% ungroup() %&gt;% mutate(mediana_zona = mediana_zona - mediana) %&gt;% select(nombre_zona, mediana_zona, residual) %&gt;% pivot_longer(mediana_zona:residual, names_to = &quot;tipo&quot;, values_to = &quot;valor&quot;) ggplot(resumen, aes(sample = valor)) + geom_qq(distribution = stats::qunif) + facet_wrap(~ tipo) + ylab(&quot;Precio por m2&quot;) + xlab(&quot;f&quot;) + labs(subtitle = &quot;Precio por m2 por zona&quot;, caption = paste0(&quot;Mediana total de &quot;, round(mediana))) Vemos que la mayor parte de la variaci√≥n del precio por metro cuadrado ocurre dentro de cada zona, una vez que controlamos por el tama√±o de las casas. La variaci√≥n dentro de cada zona es aproximadamente sim√©trica, aunque la cola derecha es ligeramente m√°s larga con algunos valores extremos. Podemos seguir con otro indicador importante: la calificaci√≥n de calidad de los terminados de las casas. Como primer intento podr√≠amos hacer: Lo que indica que las calificaciones de calidad est√°n distribuidas de manera muy distinta a lo largo de las zonas, y que probablemente no va ser simple desentra√±ar qu√© variaci√≥n del precio se debe a la zona y cu√°l se debe a la calidad. Prueba Enlace Consideremos la prueba Enlace (2011) de matem√°ticas para primarias. Una primera pregunta que alguien podr√≠a hacerse es: ¬øcu√°les escuelas son mejores en este rubro, las privadas o las p√∫blicas? enlace_tbl &lt;- enlace %&gt;% group_by(tipo) %&gt;% summarise(n_escuelas = n(), cuantiles = list(cuantil(mate_6, c(0.05, 0.25, 0.5, 0.75, 0.95)))) %&gt;% unnest(cols = cuantiles) %&gt;% mutate(valor = round(valor)) enlace_tbl %&gt;% spread(cuantil, valor) %&gt;% formatear_tabla() tipo n_escuelas 0.05 0.25 0.5 0.75 0.95 Ind√≠gena/Conafe 13599 304 358 412 478 588 General 60166 380 454 502 548 631 Particular 6816 479 551 593 634 703 Para un an√°lisis exploratorio podemos utilizar distintas gr√°ficas. Por ejemplo, podemos utilizar nuevamente las gr√°ficas de caja y brazos, as√≠ como graficar los percentiles. N√≥tese que en la gr√°fica 1 se utilizan los cuantiles 0.05, 0.25, 0.5, 0.75 y 0.95: Se puede discutir qu√© tan apropiada es cada gr√°fica con el objetivo de realizar comparaciones. Sin duda, graficar m√°s cuantiles es m√°s √∫til para hacer comparaciones. Por ejemplo, en la Gr√°fica 1 podemos ver que la mediana de las escuelas generales est√° cercana al cuantil 5% de las escuelas particulares. Por otro lado, el diagrama de caja y brazos muestra tambi√©n valores ‚Äúat√≠picos‚Äù. Es importante notar que una comparaci√≥n m√°s robusta se puede lograr por medio de pruebas de hip√≥tesis, las cuales veremos mas adelante en el curso. Regresando a nuestro an√°lisis exploratorio, notemos que la diferencia es considerable entre tipos de escuela. Antes de contestar prematuramente la pregunta: ¬øcu√°les son las mejores escuelas? busquemos mejorar la interpretabilidad de nuestras comparaciones usando los principios 2 y 3. Podemos comenzar por agregar, por ejemplo, el nivel del marginaci√≥n del municipio donde se encuentra la escuela. Para este objetivo, podemos usar p√°neles (peque√±os m√∫ltiplos √∫tiles para hacer comparaciones) y graficar: Esta gr√°fica pone en contexto la pregunta inicial, y permite evidenciar la dificultad de contestarla. En particular: Se√±ala que la pregunta no s√≥lo debe concentarse en el tipo de ‚Äúsistema‚Äù: p√∫blica, privada, etc. Por ejemplo, las escuelas p√∫blicas en zonas de marginaci√≥n baja no tienen una distribuci√≥n de calificaciones muy distinta a las privadas en zonas de marginaci√≥n alta. El contexto de la escuela es importante. Debemos de pensar qu√© factores ‚Äìpor ejemplo, el entorno familiar de los estudiantes‚Äì puede resultar en comparaciones que favorecen a las escuelas privadas. Un ejemplo de esto es considerar si los estudiantes tienen que trabajar o no. A su vez, esto puede o no ser reflejo de la calidad del sistema educativo. Si esto es cierto, entonces la pregunta inicial es demasiado vaga y mal planteada. Quiz√° deber√≠amos intentar entender cu√°nto ‚Äúaporta‚Äù cada escuela a cada estudiante, como medida de qu√© tan buena es cada escuela. Estados y calificaciones en SAT ¬øC√≥mo se relaciona el gasto por alumno, a nivel estatal, con sus resultados acad√©micos? Hay trabajo considerable en definir estos t√©rminos, pero supongamos que tenemos el siguiente conjunto de datos (Guber 1999), que son datos oficiales agregados por estado de Estados Unidos. Consideremos el subconjunto de variables sat, que es la calificaci√≥n promedio de los alumnos en cada estado (para 1997) y expend, que es el gasto en miles de d√≥lares por estudiante en (1994-1995). sat &lt;- read_csv(&quot;data/sat.csv&quot;) sat_tbl &lt;- sat %&gt;% select(state, expend, sat) %&gt;% gather(variable, valor, expend:sat) %&gt;% group_by(variable) %&gt;% summarise(cuantiles = list(cuantil(valor))) %&gt;% unnest(cols = c(cuantiles)) %&gt;% mutate(valor = round(valor, 1)) %&gt;% spread(cuantil, valor) sat_tbl %&gt;% formatear_tabla variable 0 0.25 0.5 0.75 1 expend 3.7 4.9 5.8 6.4 9.8 sat 844.0 897.2 945.5 1032.0 1107.0 Esta variaci√≥n es considerable para promedios del SAT: el percentil 75 es alrededor de 1050 puntos, mientras que el percentil 25 corresponde a alrededor de 800. Igualmente, hay diferencias considerables de gasto por alumno (miles de d√≥lares) a lo largo de los estados. Ahora hacemos nuestro primer ejercico de comparaci√≥n: ¬øC√≥mo se ven las calificaciones para estados en distintos niveles de gasto? Podemos usar una gr√°fica de dispersi√≥n: library(ggrepel) ggplot(sat, aes(x = expend, y = sat, label = state)) + geom_point(colour = &quot;red&quot;, size = 2) + geom_text_repel(colour = &quot;gray50&quot;) + xlab(&quot;Gasto por alumno (miles de d√≥lares)&quot;) + ylab(&quot;Calificaci√≥n promedio en SAT&quot;) Estas comparaciones no son de alta calidad, solo estamos usando 2 variables ‚Äîque son muy pocas‚Äî y no hay mucho que podamos decir en cuanto explicaci√≥n. Sin duda nos hace falta una imagen m√°s completa. Necesitar√≠amos entender la correlaci√≥n que existe entre las dem√°s caracter√≠sticas de nuestras unidades de estudio. Las unidades que estamos comparando pueden diferir fuertemente en otras propiedades importantes (aka, dimensiones), lo cual no permite interpretar la gr√°fica de manera sencilla. Sabemos que es posible que el IQ difiera en los estados. Pero no sabemos c√≥mo producir diferencias de este tipo. Sin embargo, ¬°descubrimos que existe una variable adicional! √âsta es el porcentaje de alumnos de cada estado que toma el SAT. Podemos agregar como sigue: ggplot(sat, aes(x = expend, y = math, label=state, colour = frac)) + geom_point() + geom_text_repel() + xlab(&quot;Gasto por alumno (miles de d√≥lares)&quot;) + ylab(&quot;Calificaci√≥n en matem√°ticas&quot;) Esto nos permite entender por qu√© nuestra comparaci√≥n inicial es relativamente pobre. Los estados con mejores resultados promedio en el SAT son aquellos donde una fracci√≥n relativamente baja de los estudiantes toma el examen. La diferencia es considerable. En este punto podemos hacer varias cosas. Una primera idea es intentar comparar estados m√°s similares en cuanto a la poblaci√≥n de alumnos que asiste. Podr√≠amos hacer grupos como sigue: set.seed(991) k_medias_sat &lt;- kmeans(sat %&gt;% select(frac), centers = 4, nstart = 100, iter.max = 100) sat$clase &lt;- k_medias_sat$cluster sat &lt;- sat %&gt;% group_by(clase) %&gt;% mutate(clase_media = round(mean(frac))) %&gt;% ungroup %&gt;% mutate(clase_media = factor(clase_media)) sat &lt;- sat %&gt;% mutate(rank_p = rank(frac, ties= &quot;first&quot;) / length(frac)) ggplot(sat, aes(x = rank_p, y = frac, label = state, colour = clase_media)) + geom_point(size = 2) Estos resultados indican que es m√°s probable que buenos alumnos decidan hacer el SAT. Lo interesante es que esto ocurre de manera diferente en cada estado. Por ejemplo, en algunos estados era m√°s com√∫n otro examen: el ACT. Si hacemos clusters de estados seg√∫n el % de alumnos, empezamos a ver otra historia. Para esto, ajustemos rectas de m√≠nimos cuadrados como referencia: Sin embargo, el resultado puede variar considerablemente si categorizamos de distintas maneras. Tablas de conteos Consideremos los siguientes datos de tomadores de t√© (del paquete FactoMineR (L√™ et al. 2008)): tea &lt;- read_csv(&quot;data/tea.csv&quot;) # nombres y c√≥digos te &lt;- tea %&gt;% select(how, price, sugar) %&gt;% rename(presentacion = how, precio = price, azucar = sugar) %&gt;% mutate( presentacion = fct_recode(presentacion, suelto = &quot;unpackaged&quot;, bolsas = &quot;tea bag&quot;, mixto = &quot;tea bag+unpackaged&quot;), precio = fct_recode(precio, marca = &quot;p_branded&quot;, variable = &quot;p_variable&quot;, barato = &quot;p_cheap&quot;, marca_propia = &quot;p_private label&quot;, desconocido = &quot;p_unknown&quot;, fino = &quot;p_upscale&quot;), azucar = fct_recode(azucar, sin_az√∫car = &quot;No.sugar&quot;, con_az√∫car = &quot;sugar&quot;)) sample_n(te, 10) ## [90m# A tibble: 10 x 3[39m ## presentacion precio azucar ## [3m[90m&lt;fct&gt;[39m[23m [3m[90m&lt;fct&gt;[39m[23m [3m[90m&lt;fct&gt;[39m[23m ## [90m 1[39m mixto variable sin_az√∫car ## [90m 2[39m suelto fino con_az√∫car ## [90m 3[39m bolsas fino con_az√∫car ## [90m 4[39m mixto variable sin_az√∫car ## [90m 5[39m bolsas variable sin_az√∫car ## [90m 6[39m suelto variable con_az√∫car ## [90m 7[39m bolsas variable con_az√∫car ## [90m 8[39m mixto fino sin_az√∫car ## [90m 9[39m bolsas marca con_az√∫car ## [90m10[39m mixto marca sin_az√∫car Nos interesa ver qu√© personas compran t√© suelto, y de qu√© tipo. Empezamos por ver las proporciones que compran t√© seg√∫n su empaque (en bolsita o suelto): precio &lt;- te %&gt;% group_by(precio) %&gt;% tally() %&gt;% mutate(prop = round(100 * n / sum(n))) %&gt;% select(-n) tipo &lt;- te %&gt;% group_by(presentacion) %&gt;% tally() %&gt;% mutate(pct = round(100 * n / sum(n))) tipo %&gt;% formatear_tabla presentacion n pct bolsas 170 57 mixto 94 31 suelto 36 12 La mayor parte de las personas toma t√© en bolsas. Sin embargo, el tipo de t√© (en t√©rminos de precio o marca) que compran es muy distinto dependiendo de la presentaci√≥n: tipo &lt;- tipo %&gt;% select(presentacion, prop_presentacion = pct) tabla_cruzada &lt;- te %&gt;% group_by(presentacion, precio) %&gt;% tally() %&gt;% # porcentajes por presentaci√≥n group_by(presentacion) %&gt;% mutate(prop = round(100 * n / sum(n))) %&gt;% select(-n) tabla_cruzada %&gt;% pivot_wider(names_from = presentacion, values_from = prop, values_fill = list(prop = 0)) %&gt;% formatear_tabla() precio bolsas mixto suelto marca 41 21 14 barato 3 1 3 marca_propia 9 4 3 desconocido 6 1 0 fino 8 20 56 variable 32 52 25 Estos datos podemos examinarlos un rato y llegar a conclusiones. Notemos que el uso de tablas no permite mostrar claramente patrones. Tampoco por medio de gr√°ficas como la siguiente: ggplot(tabla_cruzada %&gt;% ungroup %&gt;% mutate(price = fct_reorder(precio, prop)), aes(x = precio, y = prop, group = presentacion, colour = presentacion)) + geom_point() + coord_flip() + geom_line() En lugar de eso, calcularemos perfiles columna. Esto es, comparamos cada una de las columnas con la columna marginal (en la tabla de tipo de estilo de t√©): num_grupos &lt;- n_distinct(te %&gt;% select(presentacion)) tabla &lt;- te %&gt;% group_by(presentacion, precio) %&gt;% tally() %&gt;% group_by(presentacion) %&gt;% mutate(prop_precio = (100 * n / sum(n))) %&gt;% group_by(precio) %&gt;% mutate(prom_prop = sum(prop_precio)/num_grupos) %&gt;% mutate(perfil = 100 * (prop_precio / prom_prop - 1)) tabla ## [90m# A tibble: 17 x 6[39m ## [90m# Groups: precio [6][39m ## presentacion precio n prop_precio prom_prop perfil ## [3m[90m&lt;fct&gt;[39m[23m [3m[90m&lt;fct&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m bolsas marca 70 41.2 25.4 61.8 ## [90m 2[39m bolsas barato 5 2.94 2.26 30.1 ## [90m 3[39m bolsas marca_propia 16 9.41 5.48 71.7 ## [90m 4[39m bolsas desconocido 11 6.47 2.51 158. ## [90m 5[39m bolsas fino 14 8.24 28.0 -[31m70[39m[31m.[39m[31m6[39m ## [90m 6[39m bolsas variable 54 31.8 36.3 -[31m12[39m[31m.[39m[31m5[39m ## [90m 7[39m mixto marca 20 21.3 25.4 -[31m16[39m[31m.[39m[31m4[39m ## [90m 8[39m mixto barato 1 1.06 2.26 -[31m52[39m[31m.[39m[31m9[39m ## [90m 9[39m mixto marca_propia 4 4.26 5.48 -[31m22[39m[31m.[39m[31m4[39m ## [90m10[39m mixto desconocido 1 1.06 2.51 -[31m57[39m[31m.[39m[31m6[39m ## [90m11[39m mixto fino 19 20.2 28.0 -[31m27[39m[31m.[39m[31m8[39m ## [90m12[39m mixto variable 49 52.1 36.3 43.6 ## [90m13[39m suelto marca 5 13.9 25.4 -[31m45[39m[31m.[39m[31m4[39m ## [90m14[39m suelto barato 1 2.78 2.26 22.9 ## [90m15[39m suelto marca_propia 1 2.78 5.48 -[31m49[39m[31m.[39m[31m3[39m ## [90m16[39m suelto fino 20 55.6 28.0 98.4 ## [90m17[39m suelto variable 9 25 36.3 -[31m31[39m[31m.[39m[31m1[39m tabla_perfil &lt;- tabla %&gt;% select(presentacion, precio, perfil, pct = prom_prop) %&gt;% pivot_wider(names_from = presentacion, values_from = perfil, values_fill = list(perfil = -100.0)) if_profile &lt;- function(x){ any(x &lt; 0) &amp; any(x &gt; 0) } marcar &lt;- marcar_tabla_fun(25, &quot;red&quot;, &quot;black&quot;) tab_out &lt;- tabla_perfil %&gt;% arrange(desc(bolsas)) %&gt;% select(-pct, everything()) %&gt;% mutate_if(if_profile, marcar) %&gt;% knitr::kable(format_table_salida(), escape = FALSE, digits = 0, booktabs = T) %&gt;% kableExtra::kable_styling(latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;), bootstrap_options = c( &quot;hover&quot;, &quot;condensed&quot;), full_width = FALSE) if (knitr::is_latex_output()) { gsub(&quot;marca_propia&quot;, &quot;marca-propia&quot;, tab_out) } else { tab_out } precio bolsas mixto suelto pct desconocido 157.641196013289 -57.641196013289 -100 3 marca_propia 71.6967570081604 -22.3711470973743 -49.325609910786 5 marca 61.8106471150781 -16.389635229291 -45.4210118857871 25 barato 30.0871348026653 -52.9472065607381 22.8600717580728 2 variable -12.4877880581576 43.6124360668927 -31.1246480087351 36 fino -70.5895012167464 -27.8146572417104 98.4041584584568 28 Leemos esta tabla como sigue: por ejemplo, los compradores de t√© suelto compran t√© fino a una tasa casi el doble (98%) que el promedio. Tambi√©n podemos graficar como: tabla_graf &lt;- tabla_perfil %&gt;% ungroup %&gt;% mutate(precio = fct_reorder(precio, bolsas)) %&gt;% select(-pct) %&gt;% pivot_longer(cols = -precio, names_to = &quot;presentacion&quot;, values_to = &quot;perfil&quot;) g_perfil &lt;- ggplot(tabla_graf, aes(x = precio, xend = precio, y = perfil, yend = 0, group = presentacion)) + geom_point() + geom_segment() + facet_wrap(~presentacion) + geom_hline(yintercept = 0 , colour = &quot;gray&quot;)+ coord_flip() g_perfil Observaci√≥n: hay dos maneras de construir la columna promedio: tomando los porcentajes sobre todos los datos, o promediando los porcentajes de las columnas. Si los grupos de las columnas est√°n desbalanceados, estos promedios son diferentes. Cuando usamos porcentajes sobre la poblaci√≥n, perfiles columna y rengl√≥n dan el mismo resultado Sin embargo, cuando hay un grupo considerablemente m√°s grande que otros, las comparaciones se vuelven vs este grupo particular. No siempre queremos hacer esto. Interpretaci√≥n En el √∫ltimo ejemplo de tomadores de t√© utilizamos una muestra de personas, no toda la poblaci√≥n de tomadores de t√©. Eso quiere decir que tenemos cierta incertidumbre de c√≥mo se generalizan o no los resultados que obtuvimos en nuestro an√°lisis a la poblaci√≥n general. Nuestra respuesta depende de c√≥mo se extrajo la muestra que estamos considerando. Si el mecanismo de extracci√≥n incluye alg√∫n proceso probabil√≠stico, entonces es posible en principio entender qu√© tan bien generalizan los resultados de nuestro an√°lisis a la poblaci√≥n general, y entender esto depende de entender qu√© tanta variaci√≥n hay de muestra a muestra, de todas las posibles muestras que pudimos haber extraido. En las siguiente secciones discutiremos estos aspectos, en los cuales pasamos del trabajo de ‚Äúdetective‚Äù al trabajo de ‚Äújuez‚Äù en nuestro trabajo anal√≠tico. 2.1 Loess Las gr√°ficas de dispersi√≥n son la herramienta b√°sica para describir la relaci√≥n entre dos variables cuantitativas, y como vimos en ejemplo anteriores, muchas veces podemos apreciar mejor la relaci√≥n entre ellas si agregamos una curva loess que suavice. Los siguientes datos muestran los premios ofrecidos y las ventas totales de una loter√≠a a lo largo de 53 sorteos (las unidades son cantidades de dinero indexadas). Graficamos en escalas logar√≠tmicas y agregamos una curva loess. # cargamos los datos load(here::here(&quot;data&quot;, &quot;ventas_sorteo.Rdata&quot;)) ggplot(ventas.sorteo, aes(x = log(premio), y = log(ventas.tot.1))) + geom_point() + geom_smooth(method = &quot;loess&quot;, alpha = 0.5, degree = 1, se = FALSE) ## Warning: Ignoring unknown parameters: degree ## `geom_smooth()` using formula &#39;y ~ x&#39; El patr√≥n no era dif√≠cil de ver en los datos originales, sin embargo, la curva lo hace m√°s claro, el logaritmo de las ventas tiene una relaci√≥n no lineal con el logaritmo del premio: para premios no muy grandes no parece haber gran diferencia, pero cuando los premios empiezan a crecer por encima de 20,000 (aproximadamente \\(e^{10}\\)), las ventas crecen m√°s r√°pidamente que para premios menores. Este efecto se conoce como bola de nieve, y es frecuente en este tipo de loter√≠as. Antes de adentrarnos a loess comenzamos explicando c√≥mo se ajustan familias param√©tricas de curvas a conjuntos de datos dados. Ajustando familias param√©tricas. Supongamos que tenemos la familia \\(f_{a,b}=ax+b\\) y datos bivariados \\((x_1,y_1), ..., (x_n, y_n)\\). Buscamos encontrar \\(a\\) y \\(b\\) tales que \\(f_{a,b}\\) de un ajuste √≥ptimo a los datos. El criterio de m√≠nimos cuadrados consiste en encontrar \\(a\\), \\(b\\) que minimicen la suma de cuadrados: \\[\\sum_{i=1}^n(y_i-ax_i-b)^2\\] En este caso, las constantes \\(a\\) y \\(b\\) se pueden encontrar diferenciando esta funci√≥n objetivo. En este caso, estamos ajustando una recta a los datos, pero podemos repetir el argumento con otras familias de funciones (por ejemplo cuadr√°ticas). ggplot(ventas.sorteo, aes(x = log(premio), y = log(ventas.tot.1))) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Donde los par√°metros \\(a\\) y \\(b\\) est√°n dados por: mod_lineal &lt;- lm(log(ventas.tot.1) ~ log(premio), data = ventas.sorteo) round(coef(mod_lineal), 2) ## (Intercept) log(premio) ## 4.56 0.47 De modo que la curva ajustada es \\(\\log(V) = 4.6 + 0.47 \\log(P)\\), o en las unidades originales \\(V = 100 P^{0.47}\\), donde \\(V\\) son las ventas y \\(P\\) el premio. Si observamos la gr√°fica notamos que este modelo lineal (en los logaritmos) no es adecuado para estos datos. Podr√≠amos experimentar con otras familias (por ejemplo, una cuadr√°tica o c√∫bica, potencias, exponenciales, etc.); sin embargo, en la etapa exploratoria es mejor tomar una ruta de ajuste m√°s flexibles (a√∫n cuando esta no sea con funciones algebr√°icas), que al mismo tiempo sea robusto. Ajustando curvas loess La idea es producir ajustes locales de rectas o funciones cuadr√°ticas. En estas familias es necesario especificar dos par√°metros: Par√°metro de suavizamiento \\(\\alpha\\): cuando \\(\\alpha\\) es m√°s grande, la curva ajustada es m√°s suave. Grado de los polinomios locales que ajustamos \\(\\lambda\\): generalmente se toma \\(\\lambda=1,2\\). Entonces, supongamos que los datos est√°n dados por \\((x_1,y_1), ..., (x_n, y_n)\\), y sean \\(\\alpha\\) un par√°metro de suavizamiento fijo, y \\(\\lambda=1\\). Denotamos como \\(\\hat{g}(x)\\) la curva loess ajustada, y como \\(w_i(x)\\) a una funci√≥n de peso (que depende de x) para la observaci√≥n \\((x_i, y_i)\\). Para poder calcular \\(w_i(x)\\) debemos comenzar calculando \\(q=\\lfloor{n\\alpha}\\rfloor\\) que suponemos mayor que uno. Ahora definimos la funci√≥n tricubo: \\[ \\begin{equation} T(u)=\\begin{cases} (1-|u|^3)^3, &amp; \\text{para $|u| &lt; 1$}.\\\\ 0, &amp; \\text{en otro caso}. \\end{cases} \\end{equation} \\] entonces, para el punto \\(x\\) definimos el peso correspondiente al dato \\((x_i,y_i)\\), denotado por \\(w_i(x)\\) como: \\[w_i(x)=T\\bigg(\\frac{|x-x_i|}{d_q(x)}\\bigg)\\] donde \\(d_q(x)\\) es el valor de la \\(q-√©sima\\) distancia m√°s chica (la m√°s grande entre las \\(q\\) m√°s chicas) entre los valores \\(|x-x_j|\\), \\(j=1,2,...,n\\). De esta forma, las observaciones \\(x_i\\) reciben m√°s peso cuanto m√°s cerca est√©n de \\(x\\). En palabras, de \\(x_1,...,x_n\\) tomamos los \\(q\\) datos m√°s cercanos a \\(x\\), que denotamos \\(x_{i_1}(x) \\leq x_{i_2}(x) \\leq \\cdots x_{i_q}(x) \\leq\\). Los re-escalamos a \\([0,1]\\) haciendo corresponder \\(x\\) a \\(0\\) y el punto m√°s alejado de \\(x\\) (que es \\(x_{i_q}\\)) a 1. Aplicamos el tricubo (gr√°fica de abajo), para encontrar los pesos de cada punto. Los puntos que est√°n a una distancia mayor a \\(d_q(x)\\) reciben un peso de cero, y los m√°s cercanos un peso que depende de que tan cercanos est√°n a \\(x\\). tricubo &lt;- function(x) { ifelse(abs(x) &lt; 1, (1 - abs(x) ^ 3) ^ 3, 0) } curve(tricubo, from = -1.5, to = 1.5) Finalmente ajustamos una recta de m√≠nimos cuadrados ponderados por los pesos \\(w_i(x)\\), es decir, minimizamos \\[\\sum_{i=1}^nw_i(x)(y_i-ax_i-b)^2\\] Hacemos esto para cada valor de \\(x\\) que est√° en el rango de los datos \\(x_1,...,x_n\\). Observaciones: Cualquier funci√≥n con la forma de flan del tricubo (se desvanece fuera de \\((-1,1)\\), es creciente en \\((-1,0)\\) y decreciente en \\((0, 1)\\), adem√°s de ser continua y quiz√°s diferenciable) es un buen candidato para usar en lugar del tricubo. La raz√≥n por la que escogemos precisamente esta forma algebr√°ica no tiene que ver con el an√°lisis exploratorio, sino con las ventajas te√≥ricas adicionales que tiene en la inferencia. El caso \\(\\lambda=2\\) es similar. La √∫nica diferencia es en el paso de ajuste, donde usamos funciones cuadr√°ticas, y obtendr√≠amos entonces tres par√°metros \\(a(x), b(x), c(x)\\). Escogiendo de los par√°metros. Los par√°metros \\(\\alpha\\) y \\(\\lambda\\) se encuentran por ensayo y error. La idea general es que debemos encontrar una curva que explique patrones importantes en los datos (que ajuste los datos) pero que no muestre variaciones a escalas m√°s chicas dif√≠ciles de explicar (que pueden ser el resultado de influencias de otras variables, variaci√≥n muestral, ruido o errores de redondeo, por ejemplo). En el proceso de prueba y error iteramos el ajuste y en cada paso hacemos an√°lisis de residuales, con el fin de seleccionar un suavizamiento adecuado. Ejemplo de distintas selecciones de \\(\\lambda\\), en este ejemplo consideramos la ventas semanales de un producto a lo largo de 5 a√±os. Series de tiempo Podemos usar el suavizamiento loess para entender y describir el comportamiento de series de tiempo, en las cu√°les intentamos entender la dependencia de una serie de mediciones indexadas por el tiempo. T√≠picamente es necesario utilizar distintas componentes para describir exitosamente una serie de tiempo, y para esto usamos distintos tipos de suavizamientos. Veremos que distintas componentes var√≠an en distintas escalas de tiempo (unas muy lentas, cono la tendencia, otras m√°s rapidamente, como variaci√≥n quincenal, etc.). Caso de estudio: nacimientos en M√©xico Este caso de estudio esta basado en un an√°lisis propuesto por A. Vehtari y A. Gelman, junto con un an√°lisis de serie de tiempo de Cleveland (1993). En nuestro caso, usaremos los datos de nacimientos registrados por d√≠a en M√©xico desde 1999. Los usaremos para contestar las preguntas: ¬øcu√°les son los cumplea√±os m√°s frecuentes? y ¬øen qu√© mes del a√±o hay m√°s nacimientos? Podr√≠amos utilizaar una gr√°fica popular (ver por ejemplo esta visualizaci√≥n) como: Sin embargo, ¬øc√≥mo criticar√≠as este an√°lisis desde el punto de vista de los tres primeros principios del dise√±o anal√≠tico? ¬øLas comparaciones son √∫tiles? ¬øHay aspectos multivariados? ¬øQu√© tan bien explica o sugiere estructura, mecanismos o causalidad? Datos de natalidad para M√©xico library(lubridate) library(ggthemes) theme_set(theme_minimal(base_size = 14)) natalidad &lt;- readRDS(&quot;./data/nacimientos/natalidad.rds&quot;) %&gt;% mutate(dia_semana = weekdays(fecha)) %&gt;% mutate(dia_a√±o = yday(fecha)) %&gt;% mutate(a√±o = year(fecha)) %&gt;% mutate(mes = month(fecha)) %&gt;% ungroup %&gt;% mutate(dia_semana = recode(dia_semana, Monday = &quot;Lunes&quot;, Tuesday = &quot;Martes&quot;, Wednesday = &quot;Mi√©rcoles&quot;, Thursday = &quot;Jueves&quot;, Friday = &quot;Viernes&quot;, Saturday = &quot;S√°bado&quot;, Sunday = &quot;Domingo&quot;)) %&gt;% mutate(dia_semana = fct_relevel(dia_semana, c(&quot;Lunes&quot;, &quot;Martes&quot;, &quot;Mi√©rcoles&quot;, &quot;Jueves&quot;, &quot;Viernes&quot;, &quot;S√°bado&quot;, &quot;Domingo&quot;))) Consideremos los datos agregados del n√∫mero de nacimientos (registrados) por d√≠a desde 1999 hasta 2016. Un primer intento podr√≠a ser hacer una gr√°fica de la serie de tiempo. Sin embargo, vemos que no es muy √∫til: Hay varias caracter√≠sticas que notamos. Primero, parece haber una tendencia ligeramente decreciente del n√∫mero de nacimientos a lo largo de los a√±os. Segundo, la gr√°fica sugiere un patr√≥n anual. Y por √∫ltimo, encontramos que hay dispersi√≥n producida por los d√≠as de la semana. S√≥lo estas caracter√≠sticas hacen que la comparaci√≥n entre d√≠as sea dif√≠cil de realizar. Supongamos que comparamos el n√∫mero de nacimientos de dos mi√©rcoles dados. Esa comparaci√≥n ser√° diferente dependiendo: del a√±o donde ocurrieron, el mes donde ocurrieron, si semana santa ocurri√≥ en algunos de los mi√©rcoles, y as√≠ sucesivamente. Como en nuestros ejemplos anteriores, la idea del siguiente an√°lisis es aislar las componentes que observamos en la serie de tiempo: extraemos componentes ajustadas, y luego examinamos los residuales. En este caso particular, asumiremos una descomposici√≥n aditiva de la serie de tiempo (Cleveland 1993). En el estudio de series de tiempo una estructura com√∫n es considerar el efecto de diversos factores como tendencia, estacionalidad, ciclicidad e irregularidades de manera aditiva. Esto es, consideramos la descomposici√≥n \\[\\begin{align} y(t) = f_{t}(t) + f_{e}(t) + f_{c}(t) + \\varepsilon. \\end{align}\\] Una estrategia de ajuste, como veremos m√°s adelante, es proceder de manera modular. Es decir, se ajustan los componentes de manera secuencial considerando los residuales de los anteriores. Tendencia Comenzamos por extraer la tendencia, haciendo promedios loess (Cleveland 1979) con vecindades relativamente grandes. Quiz√° preferir√≠amos suavizar menos para capturar m√°s variaci√≥n lenta, pero si hacemos esto en este punto empezamos a absorber parte de la componente anual: mod_1 &lt;- loess(n ~ as.numeric(fecha), data = natalidad, span = 0.2, degree = 1) datos_dia &lt;- natalidad %&gt;% mutate(ajuste_1 = fitted(mod_1)) %&gt;% mutate(res_1 = n - ajuste_1) Notemos que a principios de 2000 el suavizador est√° en niveles de alrededor de 7000 nacimientos diarios, hacia 2015 ese n√∫mero es m√°s cercano a unos 6000. El modelo de regresion lineal ajusta una recta a un conjunto de datos. Por ejemplo, consideremos la familia \\(f_{a,b} = a x + b\\) para un conjunto de datos bivariados \\(\\{ (x_1, y_1), \\ldots, (x_N, y_N)\\}\\). La recta se ajusta al minimizar la suma de errores cuadr√°ticos \\[\\begin{align} \\frac1N \\sum_{n = 1}^N ( y_n - a x_n - b)^2. \\end{align}\\] Sin embargo, este modelo es muy inflexible. Una manera de mejorar su ajuste es considerar rectas de manera local. Esto lo logramos al minimizar una suma de errores cuadr√°ticos ponderados por los pesos \\(w_n(x)\\). Para esto minimizamos las \\(N\\) funciones de error \\[\\begin{align} \\sum_{n = 1}^N w_n(x_k) ( y_n - a x_n - b)^2, \\end{align}\\] donde \\(x_k \\in \\{x_1, \\ldots, x_N\\}\\). Notemos que el ajuste tradicional por medio de m√≠nimos cuadrados considera un ajuste con pesos \\[\\begin{align} w_n(x) = \\frac1N. \\end{align}\\] En la pr√°ctica, los pesos \\(w_n(\\cdot)\\) contienen a su vez un conjunto de par√°metros que dictan el comportamiento del ajuste local. Si por ejemplo tomamos en cuenta pesos Gaussianos \\[\\begin{align} w_n(x) = \\text{exp}\\left( -\\frac{ (x - x_n)^2 }{2\\sigma^2} \\right), \\end{align}\\] el par√°metro \\(\\sigma\\) dicta el efecto local de los puntos ancla \\(x_n\\). Dicho de otro modo, dicta la vecindad de incidencia de los datos \\(x_n.\\) Una generalizaci√≥n sencilla es considerar polinomios para el ajuste \\[\\begin{align} f_p(x) = b + a_1 x + \\cdots + a_p x ^p, \\end{align}\\] donde \\(p\\) es el m√°ximo grado del polinomio. De esta forma, \\(p\\) controla el suavizamiento de la curva. El caso de recta lineal claramente es para \\(p=1.\\) Componente anual Al obtener la tendencia podemos aislar el efecto a largo plazo y proceder a realizar mejores comparaciones (por ejemplo, comparar un d√≠a de 2000 y de 2015 tendria m√°s sentido). Ahora, ajustamos los residuales del suavizado anterior, pero con menos suavizamiento. As√≠ evitamos capturar tendencia: mod_anual &lt;- loess(res_1 ~ as.numeric(fecha), data = datos_dia, degree = 2, span = 0.005) datos_dia &lt;- datos_dia %&gt;% mutate(ajuste_2 = fitted(mod_anual)) %&gt;% mutate(res_2 = res_1 - ajuste_2) D√≠a de la semana Hasta ahora, hemos aislado los efectos por plazos largos de tiempo (tendencia) y hemos incorporado las variaciones estacionales (componente anual) de nuestra serie de tiempo. Ahora, veremos c√≥mo capturar el efecto por d√≠a de la semana. En este caso, podemos hacer suavizamiento loess para cada serie de manera independiente datos_dia &lt;- datos_dia %&gt;% group_by(dia_semana) %&gt;% nest() %&gt;% mutate(ajuste_mod = map(data, ~ loess(res_2 ~ as.numeric(fecha), data = .x, span = 0.1, degree = 1))) %&gt;% mutate(ajuste_3 = map(ajuste_mod, fitted)) %&gt;% select(-ajuste_mod) %&gt;% unnest(cols = c(data, ajuste_3)) %&gt;% mutate(res_3 = res_2 - ajuste_3) %&gt;% ungroup Residuales Por √∫ltimo, examinamos los residuales finales quitando los efectos ajustados: ## `geom_smooth()` using formula &#39;y ~ x&#39; Observaci√≥n: n√≥tese que la distribuci√≥n de estos residuales presenta irregularidades interesantes. La distribuci√≥n es de colas largas, y no se debe a unos cuantos datos at√≠picos. Esto generalmente es indicaci√≥n que hay factores importantes que hay que examinar mas a detalle en los residuales: Reestimaci√≥n Cuando hacemos este proceso secuencial de llevar el ajuste a los residual, a veces conviene iterarlo. La raz√≥n es que un una segunda o tercera pasada podemos hacer mejores estimaciones de cada componente, y es posible suavizar menos sin capturar componentes de m√°s alta frecuencia. As√≠ que podemos regresar a la serie original para hacer mejores estimaciones, m√°s suavizadas: # Quitamos componente anual y efecto de d√≠a de la semana datos_dia &lt;- datos_dia %&gt;% mutate(n_1 = n - ajuste_2 - ajuste_3) # Reajustamos mod_1 &lt;- loess(n_1 ~ as.numeric(fecha), data = datos_dia, span = 0.02, degree = 2, family = &quot;symmetric&quot;) Y ahora repetimos con la componente de d√≠a de la semana: An√°lisis de componentes Ahora comparamos las componentes estimadas y los residuales en una misma gr√°fica. Por definici√≥n, la suma de todas estas componentes da los datos originales. Este √∫ltimo paso nos permite diversas comparaciones que explican la variaci√≥n que vimos en los datos. Una gran parte de los residuales est√° entre \\(\\pm 250\\) nacimientos por d√≠a. Sin embargo, vemos que las colas tienen una dispersi√≥n mucho mayor: quantile(datos_dia$res_6, c(00, .01,0.05, 0.10, 0.90, 0.95, 0.99, 1)) %&gt;% round ## 0% 1% 5% 10% 90% 95% 99% 100% ## -2238 -1134 -315 -202 188 268 516 2521 ¬øA qu√© se deben estas colas tan largas? Viernes 13? Podemos empezar con una curosidad: en viernes o martes 13, ¬ønacen menos ni√±os? N√≥tese que fue √∫til agregar el indicador de Semana santa por el Viernes 13 de Semana Santa que se ve como un at√≠pico en el panel de los viernes 13. Residuales: antes y despu√©s de 2006 Veamos primero una agregaci√≥n sobre los a√±os de los residuales. Lo primero es observar un cambio que sucedi√≥ repentinamente en 2006: La raz√≥n es un cambio en la ley acerca de cu√°ndo pueden entrar los ni√±os a la primaria. Antes era por edad y hab√≠a poco margen. Ese exceso de nacimientos son reportes falsos para que los ni√±os no tuvieran que esperar un a√±o completo por haber nacido unos cuantos d√≠as antes de la fecha l√≠mite. Otras caracter√≠sticas que debemos investigar: Efectos de A√±o Nuevo, Navidad, Septiembre 16 y otros d√≠as feriados como Febrero 14. Semana santa: como la fecha cambia, vemos que los residuales negativos tienden a ocurrir dispersos alrededor del d√≠a 100 del a√±o. Otros d√≠as especiales: m√°s de residuales Ahora promediamos residuales (es posible agregar barras para indicar dispersi√≥n a lo largo de los a√±os) para cada d√≠a del a√±o. Podemos identificar ahora los residuales m√°s grandes: se deben, por ejemplo, a d√≠as feriados, con consecuencias adicionales que tienen en d√≠as ajuntos (excesos de nacimientos): ## `summarise()` regrouping output by &#39;dia_a√±o_366&#39;, &#39;antes_2006&#39; (override with `.groups` argument) Semana santa Para Semana Santa tenemos que hacer unos c√°lculos. Si alineamos los datos por d√≠as antes de Domingo de Pascua, obtenemos un patr√≥n de ca√≠da fuerte de nacimientos el Viernes de Semana Santa, y la caracter√≠stica forma de ‚Äúvalle con hombros‚Äù en d√≠as anteriores y posteriores estos Viernes. ¬øPor qu√© ocurre este patr√≥n? ## `geom_smooth()` using formula &#39;y ~ x&#39; N√≥tese un defecto de nuestro modelo: el patr√≥n de ‚Äúhombros‚Äù alrededor del Viernes Santo no es suficientemente fuerte para equilibrar los nacimientos faltantes. ¬øC√≥mo podr√≠amos mejorar nuestra descomposici√≥n? Referencias "],
["tareas.html", "Tareas 1. An√°slisis Exploratorio", " Tareas Las tareas se env√≠an por correo a teresa.ortiz.mancera@gmail.com con t√≠tulo: fundamentos-tareaXX (donde XX corresponde al n√∫mero de tarea, 01..). Las tareas deben incluir c√≥digo y resultados (si conocen Rmarkdown es muy conveniente para este prop√≥sito). 1. An√°slisis Exploratorio Realicen los ejercicios del script 01_exploratorio.R que vimos la clase pasada (RStudio.cloud proyecto 01-exploratorio). Escriban las respuestas en un reporte que incluya c√≥digo y resultados (puede ser word, html, pdf,‚Ä¶). "],
["referencias.html", "Referencias", " Referencias "]
]
