[
["index.html", "EST-46111: Fundamentos de Estadística con Remuestreo Información del curso Temario Evaluación", " EST-46111: Fundamentos de Estadística con Remuestreo Teresa Ortiz (001), Alfredo Garbuno (002), Felipe González Información del curso Notas del curso Fundamentos de Estadística con Remuestreo del programa de maestría en Ciencia de Datos del ITAM. En caso de encontrar errores o tener sugerencias del material se agradece la propuesta de correcciones mediante pull requests. Ligas Notas: https://tereom.github.io/fundamentos Correos: teresa.ortiz.mancera@gmail.com, alfredo.garbuno@itam.mx. GitHub: https://github.com/tereom/fundamentos Foros de discusion: - Grupo Teresa: slack - Grupo Alfredo: canvas Este trabajo está bajo una Licencia Creative Commons Atribución 4.0 Internacional. Temario Datos y análisis exploratorio Referencias: (Cleveland 1994), (Chihara and Hesterberg 2018) Visualización y análisis exploratorio Tipos de datos o estudios Muestras diseñadas y muestras naturales Experimentos y datos observacionales Introducción a Pruebas de Hipótesis Referencias: (Chihara and Hesterberg 2018) Introducción a pruebas de hipótesis. Pruebas de permutaciones Muestras pareadas y otros ejemplos Estimación y distribución de muestreo Referencias: (Chihara and Hesterberg 2018), (Hesterberg 2015) Estimadores y su distribución de muestreo Repaso de probabilidad y Teorema del límite central Introducción a estimación por intervalos Referencias: (Chihara and Hesterberg 2018), (Efron and Tibshirani 1993), (Hesterberg 2015) El método plugin y el boostrap Bootstrap e Intervalos de confianza. Ejemplos. Estimación Referencias: (Chihara and Hesterberg 2018), (Wasserman 2013) Estimación por máxima verosimilitud Ejemplos de estimación por máxima verosimilitud y Bootstrap paramétrico Propiedades de estimadores de máxima verosimilitud Más de pruebas de hipótesis Referencias: (Chihara and Hesterberg 2018), (Wasserman 2013) Pruebas de hipótesis para medias y proporciones: una y dos poblaciones. Introducción a inferencia bayesiana Referencias: (Kruschke 2015) Introducción a inferencia bayesiana Ejemplos de distribuciones conjugadas Introducción a métodos computacionales básicos: Muestreadores Metrópolis y Gibbs Evaluación Tareas semanales 20% Parcial teórico + parcial a casa 40% Final a casa 40% Referencias "],
["principios-de-visualización.html", "Sección 1 Principios de visualización El cuarteto de Ascombe Introducción Visualización popular de datos Teoría de visualización de datos Ejemplo: gráfica de Minard", " Sección 1 Principios de visualización El cuarteto de Ascombe En 1971 un estadístico llamado Frank Anscombe (fundador del departamento de Estadística de la Universidad de Yale) publicó cuatro conjuntos de dato. Cada uno consiste de 11 observaciones. La peculariedad de estos conjuntos es que tienen las mismas propiedades estadísticas. Sin embargo, cuando analizamos los datos de manera gráfica en un histograma encontramos rápidamente que los conjuntos de datos son muy distintos. Media de \\(x\\): 9 Varianza muestral de \\(x\\): 11 Media de \\(y\\): 7.50 Varianza muestral de \\(y\\): 4.12 Correlación entre \\(x\\) y \\(y\\): 0.816 Línea de regresión lineal: \\(y = 3.00 + 0.500x\\) En la gráfica del primer conjunto de datos, se ve clara una relación lineal simple con un modelo que cumple los supuestos de normalidad. La segunda gráfica (arriba a la derecha) muestra unos datos que tienen una asociación pero definitivamente no es lineal. En la tercera gráfica (abajo a la izquierda) están puntos alineados perfectamente en una línea recta, excepto por uno de ellos. En la última gráfica podemos ver un ejemplo en el cual basta tener una observación atípica para que se produzca un coeficiente de correlación alto aún cuando en realidad no existe una asociación lineal entre las dos variables. El cuarteto de Ascombe inspiró una técnica reciente para crear datos que comparten las mismas propiedades estadísticas al igual que en el cuarteto, pero que producen gráficas muy distintas (Matejka, Fitzmaurice). Introducción La visualización de datos no trata de hacer gráficas “bonitas” o “divertidas”, ni de simplificar lo complejo o ayudar a una persona “que no entiende mucho” a entender ideas complejas. Más bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. El siguiente ejemplo de (Tufte 2006), ilustra claramente la diferencia entre estos dos enfoques. A la izquierda están gráficas (más o menos típicas de Powerpoint) basadas en la filosofía de simplificar, de intentar no “ahogar” al lector con datos. El resultado es una colección incoherente, de bajo contenido, que no tiene mucho qué decir y que es, “indeferente al contenido y la evidencia”. A la derecha está una variación del rediseño de Tufte en forma de tabla, que en este caso particular es una manera eficiente de mostrar claramente los patrones que hay en este conjunto simple de datos. ¿Qué principios son los que soportan la efectividad de esta tabla sobre la gráfica de la derecha? Veremos que hay dos conjuntos de principios importantes: unos relacionados con el diseño y otros con la naturaleza del análisis de datos, independientemente del método de visualización. Visualización popular de datos Publicaciones populares (periódicos, revistas, sitios internet) muchas veces incluyen visualización de datos como parte de sus artículos o reportajes. En general siguen el mismo patrón que en la visión tradicionalista de la estadística: sirven más para divertir que para explicar, tienden a explicar ideas simples y conjuntos chicos de datos, y se consideran como una “ayuda” para los “lectores menos sofisticados”. Casi siempre se trata de gráficas triviales (muchas veces con errores graves) que no aportan mucho a artículos que tienen un nivel de complejidad mucho mayor (es la filosofía: lo escrito para el adulto, lo graficado para el niño). Teoría de visualización de datos Existe teoría fundamentada acerca de la visualización. Después del trabajo pionero de Tukey, los principios e indicadores de Tufte se basan en un estudio de la historia de la graficación y ejercicios de muestreo de la práctica gráfica a lo largo de varias disciplinas (¿cuáles son las mejores gráficas? ¿por qué? El trabajo de Cleveland es orientado a la práctica del análisis de datos (¿cuáles gráficas nos han ayudado a mostrar claramente los resultados del análisis?), por una parte, y a algunos estudios de percepción visual. En resumen, hablaremos de las siguientes guías: Principios generales del diseño analítico Aplicables a una presentación o análisis completos, y como guía para construir nuevas visualizaciones (Tufte 2006). Principio 1. Muestra comparaciones, contrastes, diferencias. Principio 2. Muestra causalidad, mecanismo, explicación, estructura sistemática. Principio 3. Muestra datos multivariados, es decir, más de una o dos variables. Principio 4. Integra palabras, números, imágenes y diagramas. Principio 5. Describe la totalidad de la evidencia. Muestra fuentes usadas y problemas relevantes. Principio 6. Las presentaciones analíticas, a fin de cuentas, se sostienen o caen dependiendo de la calidad, relevancia e integridad de su contenido. Técnicas de visualización Esta categoría incluye técnicas específicas que dependen de la forma de nuestros datos y el tipo de pregunta que queremos investigar (Tukey (1977), Cleveland (1993), Cleveland (1994), Tufte (2006)). Tipos de gráficas: cuantiles, histogramas, caja y brazos, gráficas de dispersión, puntos/barras/ líneas, series de tiempo. Técnicas para mejorar gráficas: Transformación de datos, transparencia, vibración, banking 45, suavizamiento y bandas de confianza. Pequeños múltiplos Indicadores de calidad gráfica Aplicables a cualquier gráfica en particular. Estas son guías concretas y relativamente objetivas para evaluar la calidad de una gráfica (Tufte 1986). Integridad Gráfica. El factor de engaño, es decir, la distorsión gráfica de las cantidades representadas, debe ser mínimo. Chartjunk. Minimizar el uso de decoración gráfica que interfiera con la interpretación de los datos: 3D, rejillas, rellenos con patrones. Tinta de datos. Maximizar la proporción de tinta de datos vs. tinta total de la gráfica. For non-data- ink, less is more. For data-ink, less is a bore. Densidad de datos. Las mejores gráficas tienen mayor densidad de datos, que es la razón entre el tamaño del conjunto de datos y el área de la gráfica. Las gráficas se pueden encoger mucho. Percepción visual. Algunas tareas son más fáciles para el ojo humano que otras (Cleveland 1994). Factor de engaño y Chartjunk El factor de engaño es el cociente entre el efecto mostrado en una gráfica y el efecto correspondiente en los datos. Idealmente, el factor de engaño debe ser 1 (ninguna distorsión). El chartjunk son aquellos elementos gráficos que no corresponden a variación de datos, o que entorpecen la interpretación de una gráfica. Estos son los indicadores de calidad más fáciles de entender y aplicar, y afortunadamente cada vez son menos comunes. Un diseño popular que califica como chartjunk y además introduce factores de engaño es el pie de 3D. En la gráfica de la derecha, podemos ver como la rebanada C se ve más grande que la rebanada A, aunque claramente ese no es el caso (factor de engaño). La razón es la variación en la perspectiva que no corresponde a variación en los datos (chartjunk). Crítica gráfica: Gráfica de pie Todavía elementos que pueden mejorar la comprensión de nuestra gráfica de pie: se trata de la decodificiación que hay que hacer categoría - color - cuantificación. Podemos agregar las etiquetas como se muestra en la serie de la derecha, pero entonces: ¿por qué no mostrar simplemente la tabla de datos? ¿qué agrega el pie a la interpretación? La deficiencias en el pie se pueden ver claramente al intentar graficar más categorías (13) . En el primer pie no podemos distinguir realmente cuáles son las categorías grandes y cuáles las chicas, y es muy difícil tener una imagen mental clara de estos datos. Agregar los porcentajes ayuda, pero entonces, otra vez, preguntamos cuál es el propósito del pie. La tabla de la izquierda hace todo el trabajo (una vez que ordenamos las categrías de la más grande a la más chica). Es posible hacer una gráfica de barras como la de abajo a la izquierda. Hay otros tipos de chartjunk comunes: uno es la textura de barras, por ejemplo. El efecto es la producción de un efecto moiré que es desagradable y quita la atención de los datos, como en la gráfica de barras de abajo. Otro común son las rejillas, como mostramos en las gráficas de la izquierda. Nótese como en estos casos hay efectos ópticos no planeados que degradan la percepción de los patrones en los datos. Pequeños múltiplos y densidad gráfica La densidad de una gráfica es el tamaño del conjunto de datos que se grafica comparado con el área total de la gráfica. En el siguiente ejemplo, graficamos en logaritmo-10 de cabezas de ganado en Francia (cerdos, res, ovejas y caballos). La gráfica de la izquierda es pobre en densidad pues sólo representa 4 datos. La manera más fácil de mejorar la densidad es hacer más chica la gráfica: La razón de este encogimiento es una que tiene qué ver con las oportunidades perdidas de una gráfica grande. Si repetimos este mismo patrón (misma escala, mismos tipos de ganado) para distintos países obtenemos la siguiente gráfica: Esta es una gráfica de puntos. Es útil como sustituto de una gráfica de barras, y es superior en el sentido de que una mayor proporción de la tinta que se usa es tinta de datos. Otra vez, mayor proporción de tinta de datos representa más oportunidades que se pueden capitalizar, como muestra la gráfica de punto y líneas que mostramos al principio (rendimiento en campos de cebada). Más pequeños múltiplos Los pequeños múltiplos presentan oportunidades para mostrar más acerca de nuestro problema de interés. Consideramos por ejemplo la relación de radiación solar y niveles de ozono: ggplot(airquality, aes(x=Solar.R, y=Ozone)) + geom_point() + geom_smooth(method = &quot;loess&quot;, span = 1) En el ejemplo anterior incluyendo una variable adicional (velocidad del viento) podemos entender más acerca de la relación de radiación solar y niveles de ozono: airquality$Wind_cat &lt;- cut(airquality$Wind, breaks = quantile(airquality$Wind, c(0, 1/3, 2/3, 1)), include.lowest = TRUE) ggplot(airquality, aes(x=Solar.R, y=Ozone)) + geom_point() + facet_wrap(~Wind_cat) + geom_smooth(method = &quot;loess&quot;, span = 0.8, se = FALSE, method.args = list(degree = 1, family=&quot;symmetric&quot;)) Tinta de datos Maximizar la proporción de tinta de datos en nuestras gráficas tiene beneficios inmediatos. La regla es: si hay tinta que no representa variación en los datos, o la eliminación de esa tinta no representa pérdidas de significado, esa tinta debe ser eliminada. El ejemplo más claro es el de las rejillas en gráficas y tablas: ¿Por qué usar grises en lugar de negros? La respuesta tiene qué ver con el principio de tinta de datos: si marcamos las diferencias sutil pero claramente, tenemos más oportunidades abiertas para hacer énfasis en lo que nos interesa: a una gráfica o tabla saturada no se le puede hacer más - es difícil agregar elementos adicionales que ayuden a la comprensión. Si comenzamos marcando con sutileza, entonces se puede hacer más. Los mapas geográficos son un buen ejemplo de este principio. El espacio en blanco es suficientemente bueno para indicar las fronteras en una tabla, y facilita la lectura: Para un ejemplo del proceso de rediseño de una tabla, ver aquí. Finalmente, podemos ver un ejemplo que intenta incorporar los elementos del diseño analítico, incluyendo pequeños múltiplos: Decoración Percepción de escala Entre la percepción visual y la interpretación de una gráfica están implícitas tareas visuales específicas que las personas debemos realizar para ver correctamente la gráfica. En la década de los ochenta, William S. Cleveland y Robert McGill realizaron algunos experimentos identificando y clasificando estas tareas para diferentes tipos de gráficos (Cleveland and McGill 1984). En estos, se le pregunta a la persona que compare dos valores dentro de una gráfica, por ejemplo, en dos barras en una gráfica de barras, o dos rebanadas de una gráfica de pie. Los resultados de Cleveland y McGill fueron replicados por Heer y Bostock en 2010 y los resultados se muestran en las gráficas de la derecha: Ejemplo: gráfica de Minard Concluimos esta sección con una gráfica que, aunque poco común, ejemplifica los principios de una buena gráfica, y es reconocida como una de las mejores visualizaciones de la historia. Una gráfica excelente, presenta datos interesantes de forma bien diseñada: es una cuestión de fondo, de diseño, y estadística… [Se] compone de ideas complejas comunicadas con claridad, precisión y eficiencia. … [Es] lo que da al espectador la mayor cantidad de ideas, en el menor tiempo, con la menor cantidad de tinta, y en el espacio más pequeño. … Es casi siempre multivariado. … Una excelente gráfica debe decir la verdad acerca de los datos. (Tufte, 1983) La famosa visualización de Charles Joseph Minard de la marcha de Napoleón sobre Moscú, ilustra los principios de una buena gráfica. Tufte señala que esta imagen “bien podría ser el mejor gráfico estadístico jamás dibujado”, y sostiene que “cuenta una historia rica y coherente con sus datos multivariados, mucho más esclarecedora que un solo número que rebota en el tiempo”. Se representan seis variables: el tamaño del ejército, su ubicación en una superficie bidimensional, la dirección del movimiento del ejército y la temperatura en varias fechas durante la retirada de Moscú\". Hoy en día Minard es reconocido como uno de los principales contribuyentes a la teoría de análisis de datos y creación de infografías con un fundamento estadístico. Se grafican 6 variables: el número de tropas de Napoleón, la distancia, la temperatura, la latitud y la longitud, la dirección en que viajaban las tropas y la localización relativa a fechas específicas. La gráfica de Minard, como la describe E.J. Marey, parece “desafiar la pluma del historiador con su brutal elocuencia”, la combinación de datos del mapa, y la serie de tiempo, dibujados en 1869, “retratan una secuencia de pérdidas devastadoras que sufrieron las tropas de Napoleón en 1812”. Comienza en la izquierda, en la frontera de Polonia y Rusia, cerca del río Niemen. La línea gruesa dorada muestra el tamaño de la Gran Armada (422,000) en el momento en que invadía Rusia en junio de 1812. El ancho de esta banda indica el tamaño de la armada en cada punto del mapa. En septiembre, la armada llegó a Moscú, que ya había sido saqueada y dejada desértica, con sólo 100,000 hombres. El camino del retiro de Napoleón desde Moscú está representado por la línea oscura (gris) que está en la parte inferior, que está relacionada a su vez con la temperatura y las fechas en el diagrama de abajo. Fue un invierno muy frío, y muchos se congelaron en su salida de Rusia. Como se muestra en el mapa, cruzar el río Berezina fue un desastre, y el ejército de Napoleón logró regresar a Polonia con tan sólo 10,000 hombres. También se muestran los movimientos de las tropas auxiliaries, que buscaban proteger por atrás y por la delantera mientras la armada avanzaba hacia Moscú. La gráfica de Minard cuenta una historia rica y cohesiva, coherente con datos multivariados y con los hechos históricos, y que puede ser más ilustrativa que tan sólo representar un número rebotando a lo largo del tiempo. Referencias "],
["análisis-exploratorio.html", "Sección 2 Análisis exploratorio El papel de la exploración en el análisis de datos Algunos conceptos básicos Ejemplos Loess", " Sección 2 Análisis exploratorio “Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone –as the first step.” — John Tukey “The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey El papel de la exploración en el análisis de datos El estándar científico para contestar preguntas o tomar decisiones es uno que se basa en el análisis de datos. Es decir, en primer lugar se deben reunir todos los datos disponibles que puedan contener o sugerir alguna guía para entender mejor la pregunta o la decisión a la que nos enfrentamos. Esta recopilación de datos —que pueden ser cualitativos, cuantitativos, o una mezcla de los dos— debe entonces ser analizada para extraer información relevante para nuestro problema. En análisis de datos existen dos distintos tipos de trabajo: El trabajo exploratorio o de detective: ¿cuáles son los aspectos importantes de estos datos? ¿qué indicaciones generales muestran los datos? ¿qué tareas de análisis debemos empezar haciendo? ¿cuáles son los caminos generales para formular con precisión y contestar algunas preguntas que nos interesen? El trabajo inferencial, confirmatorio, o de juez: ¿cómo evaluar el peso de la evidencia de los descubrimientos del paso anterior? ¿qué tan bien soportadas están las respuestas y conclusiones por nuestro conjunto de datos? Algunos conceptos básicos Empezamos explicando algunas ideas que no serán útiles más adelante. Por ejemplo, los siguientes datos fueron registrados en un restaurante durante cuatro días consecutivos: library(tidyverse) library(patchwork) source(&quot;R/funciones_auxiliares.R&quot;) # usamos los datos tips del paquete reshape2 tips &lt;- reshape2::tips # renombramos variables y niveles propinas &lt;- tips %&gt;% rename(cuenta_total = total_bill, propina = tip, sexo = sex, fumador = smoker, dia = day, momento = time, num_personas = size) %&gt;% mutate(sexo = recode(sexo, Female = &quot;Mujer&quot;, Male = &quot;Hombre&quot;), fumador = recode(fumador, No = &quot;No&quot;, Yes = &quot;Si&quot;), dia = recode(dia, Sun = &quot;Dom&quot;, Sat = &quot;Sab&quot;, Thur = &quot;Jue&quot;, Fri = &quot;Vie&quot;), momento = recode(momento, Dinner = &quot;Cena&quot;, Lunch = &quot;Comida&quot;)) %&gt;% select(-sexo) %&gt;% mutate(dia = fct_relevel(dia, c(&quot;Jue&quot;, &quot;Vie&quot;, &quot;Sab&quot;, &quot;Dom&quot;))) Y vemos una muestra sample_n(propinas, 10) %&gt;% formatear_tabla() ## Warning in kableExtra::kable_styling(., latex_options = c(&quot;striped&quot;), ## bootstrap_options = c(&quot;striped&quot;, : Please specify format in kable. kableExtra ## can customize either HTML or LaTeX outputs. See https://haozhu233.github.io/ ## kableExtra/ for details. cuenta_total propina fumador dia momento num_personas 22.67 2.00 Si Sab Cena 2 15.69 3.00 Si Sab Cena 3 18.26 3.25 No Jue Comida 2 18.15 3.50 Si Dom Cena 3 25.21 4.29 Si Sab Cena 2 30.06 2.00 Si Sab Cena 3 24.71 5.85 No Jue Comida 2 15.98 3.00 No Vie Comida 3 7.56 1.44 No Jue Comida 2 13.42 3.48 Si Vie Comida 2 Aquí la unidad de observación es una cuenta particular. Tenemos tres mediciones numéricas de cada cuenta: cúanto fue la cuenta total, la propina, y el número de personas asociadas a la cuenta. Los datos están separados según se fumó o no en la mesa, y temporalmente en dos partes: el día (Jueves, Viernes, Sábado o Domingo), cada uno separado por Cena y Comida. Denotamos por \\(x\\) el valor de medición de una unidad de observación. Usualmente utilizamos sub-índices para identificar entre diferentes puntos de datos (observaciones), por ejemplo, \\(x_n\\) para la \\(n-\\)ésima observación. De tal forma que una colección de \\(N\\) observaciones la escribimos como \\[\\begin{align} \\{x_1, \\ldots, x_N\\}. \\end{align}\\] El primer tipo de comparaciones que nos interesa hacer es para una medición: ¿Varían mucho o poco los datos de un tipo de medición? ¿Cuáles son valores típicos o centrales? ¿Existen valores atípicos? Supongamos entonces que consideramos simplemente la variable de cuenta_total. Podemos comenzar por ordenar los datos, y ver cuáles datos están en los extremos y cuáles están en los lugares centrales: En general la colección de datos no está ordenada por sus valores. Esto es debido a que las observaciones en general se recopilan de manera aleatoria. Utilizamos la notación de \\(\\sigma(n)\\) para denotar un reordenamiento de los datos de tal forma \\[\\begin{align} \\{x_{\\sigma(1)}, \\ldots, x_{\\sigma(N)}\\}, \\end{align}\\] y que satisface la siguiente serie de desigualdades \\[\\begin{align} x_{\\sigma(1)} \\leq \\ldots \\leq x_{\\sigma(N)}. \\end{align}\\] propinas &lt;- propinas %&gt;% mutate(orden_cuenta = rank(cuenta_total, ties.method = &quot;first&quot;), f = (orden_cuenta - 0.5) / n()) cuenta &lt;- propinas %&gt;% select(orden_cuenta, f, cuenta_total) %&gt;% arrange(f) bind_rows(head(cuenta), tail(cuenta)) %&gt;% formatear_tabla() orden_cuenta f cuenta_total 1 0.0020492 3.07 2 0.0061475 5.75 3 0.0102459 7.25 4 0.0143443 7.25 5 0.0184426 7.51 6 0.0225410 7.56 239 0.9774590 44.30 240 0.9815574 45.35 241 0.9856557 48.17 242 0.9897541 48.27 243 0.9938525 48.33 244 0.9979508 50.81 También podemos graficar los datos en orden, interpolando valores consecutivos. g_orden &lt;- ggplot(cuenta, aes(y = orden_cuenta, x = cuenta_total)) + geom_point(colour = &quot;red&quot;, alpha = 0.5) + labs(subtitle = &quot;Cuenta total&quot;) g_cuantiles &lt;- ggplot(cuenta, aes(y = f, x = cuenta_total)) + geom_point(colour = &quot;red&quot;, alpha = 0.5) + geom_line(alpha = 0.5) + labs(subtitle = &quot;&quot;) g_orden + g_cuantiles A esta función le llamamos la función de cuantiles para la variable cuenta_total. Nos sirve para comparar directamente los distintos valores que observamos los datos según el orden que ocupan. La función de cuantiles muestral esta definida por \\[\\begin{align} \\hat{F}(x) = \\frac1N \\sum_{n = 1}^N \\mathbb{1}\\{x_n \\leq x\\}, \\end{align}\\] donde la funcion indicadora está definida por \\[\\begin{align} 1\\{ x \\leq t\\} = \\begin{cases} 1, \\text{ si } x \\leq t \\\\ 0, \\text{ en otro caso} \\end{cases}. \\end{align}\\] Observación: la función de cuantiles definida arriba también es conocida como la función de acumulación empírica. Se puede encontrar la siguiente notación en la literatura \\[\\begin{align} \\hat F(x) = F_N(x) = \\text{Pr}_N(X \\leq x), \\end{align}\\] así como \\[\\begin{align} \\text{Pr}_N(X \\geq x) = 1 - \\hat F(x). \\end{align}\\] Para una medición de interés \\(x\\) con posibles valores en el intervalo \\([a, b]\\). Comprueba que \\(\\hat F(a) = 0\\) y \\(\\hat F(b) = 1\\) para cualquier colección de datos de tamaño \\(N.\\) La gráfica anterior, también nos sirve para poder estudiar la dispersión y valores centrales de los datos observados. Por ejemplo, podemos notar que: El rango de datos va de unos 3 dólares hasta 50 dólares Los valores centrales —del cuantil 0.25 al 0.75, por decir un ejemplo— están entre unos 13 y 25 dólares El cuantil 0.5 (o también conocido como mediana) está alrededor de 18 dólares. ¿Cómo definirías la mediana en términos de la función de cuantiles? Pista: Considera los casos por separado para \\(N\\) impar o par. Éste último puede ser utilizado para dar un valor central de la distribución de valores para cuenta_total. Asimismo podemos dar resúmenes más refinados si es necesario. Por ejemplo, podemos reportar que: El cuantil 0.95 es de unos 35 dólares — sólo 5% de las cuentas son de más de 35 dólares El cuantil 0.05 es de unos 8 dólares — sólo 5% de las cuentas son de 8 dólares o menos. Finalmente, la forma de la gráfica se interpreta usando su pendiente (tasa de cambio) haciendo comparaciones en diferentes partes de la gráfica: La distribución de valores tiene asimetría: el 10% de las cuentas más altas tiene considerablemente más dispersión que el 10% de las cuentas más bajas. Entre los cuantiles 0.2 y 0.5 es donde existe mayor densidad de datos: la pendiente (tasa de cambio) es alta, lo que significa que al avanzar en los valores observados, los cuantiles (el porcentaje de casos) aumenta rápidamente. Cuando la pendiente es casi plana, quiere decir que los datos tienen más dispersión local o están más separados. En algunos casos, es más natural hacer un histograma, donde dividimos el rango de la variable en cubetas o intervalos (en este caso de igual longitud), y graficamos por medio de barras cuántos datos caen en cada cubeta: Es una gráfica más popular, pero perdemos cierto nivel de detalle, y distintas particiones resaltan distintos aspectos de los datos. ¿Cómo se ve la gráfica de cuantiles de las propinas? ¿Cómo crees que esta gráfica se compara con distintos histogramas? g_1 &lt;- ggplot(propinas, aes(sample = propina)) + geom_qq(distribution = stats::qunif) + xlab(&quot;f&quot;) + ylab(&quot;propina&quot;) g_1 Observación. Cuando hay datos repetidos, los cuantiles tienen que interpretarse como sigue: el cuantil-\\(f\\) con valor \\(q\\) satisface que existe una proporción aproximada \\(f\\) de los datos que están en el valor \\(q\\) o por debajo de éste, pero no necesariamente exactamente una proporción \\(f\\) de los datos estan en \\(q\\) o por debajo. Observación. La definición de cuantiles muestrales no es única y distintos programas utilizan diferentes acercamientos (incluso puede variar entre paquetes o funciones de un mismo programa), ver Hyndman y Fan 2012. Finalmente, una gráfica más compacta que resume la gráfica de cuantiles o el histograma es el diagrama de caja y brazos. Mostramos dos versiones, la clásica de Tukey (T) y otra versión menos común de Spear/Tufte (ST): library(ggthemes) cuartiles &lt;- quantile(cuenta$cuenta_total) t(cuartiles) %&gt;% formatear_tabla() 0% 25% 50% 75% 100% 3.07 13.3475 17.795 24.1275 50.81 g_1 &lt;- ggplot(cuenta, aes(x = f, y = cuenta_total)) + labs(subtitle = &quot;Gráfica de cuantiles: Cuenta total&quot;) + geom_hline(yintercept = cuartiles[2], colour = &quot;gray&quot;) + geom_hline(yintercept = cuartiles[3], colour = &quot;gray&quot;) + geom_hline(yintercept = cuartiles[4], colour = &quot;gray&quot;) + geom_point(alpha = 0.5) + geom_line() g_2 &lt;- ggplot(cuenta, aes(x = factor(&quot;ST&quot;, levels =c(&quot;ST&quot;)), y = cuenta_total)) + geom_tufteboxplot() + labs(subtitle = &quot; &quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) g_3 &lt;- ggplot(cuenta, aes(x = factor(&quot;T&quot;), y = cuenta_total)) + geom_boxplot() + labs(subtitle = &quot; &quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) g_4 &lt;- ggplot(cuenta, aes(x = factor(&quot;P&quot;), y = cuenta_total)) + geom_jitter(height = 0, width =0.2, alpha = 0.5) + labs(subtitle = &quot; &quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) g_5 &lt;- ggplot(cuenta, aes(x = factor(&quot;V&quot;), y = cuenta_total)) + geom_violin() + labs(subtitle = &quot; &quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) g_1 + g_2 + g_3 + g_4 + plot_layout(widths = c(8, 2, 2, 2)) El diagrama de la derecha explica los elementos de la versión típica del diagrama de caja y brazos (boxplot). RIC se refiere al *Rango Intercuantílico**, definido por la diferencia entre los cuantiles 25% y 75%. Figura: Jumanbar / CC BY-SA Hasta ahora hemos utilizado la definición general de cuantiles. Donde consideramos el cuantil \\(q\\), para buscar \\(x\\) tal que \\(\\hat F(x) = q.\\) Hay valores típicos de interés que corresponden a \\(q\\) igual a 25%, 50% y 75%. Éstos valores se denominan cuartiles. Ventajas en el análisis inicial En un principio del análisis, estos resúmenes (cuantiles) pueden ser más útiles que utilizar medias y varianzas, por ejemplo. La razón es que los cuantiles: Son cantidades más fácilmente interpretables Los cuantiles centrales son más resistentes a valores atípicos que medias o varianzas Sin embargo, permite identificar valores extremos Es fácil comparar cuantiles de distintos bonches de datos Media y desviación estándar Las medidas más comunes de localización y dispersión para un conjunto de datos son la media muestral y la desviación estándar muestral. En general, no son muy apropiadas para iniciar el análisis exploratorio, pues: Son medidas más difíciles de interpretar y explicar que los cuantiles. En este sentido, son medidas especializadas. Por ejemplo, intenta explicar intuitivamente qué es la media. No son resistentes a valores atípicos o erróneos. Su falta de resistencia los vuelve poco útiles en las primeras etapas de limpieza y descripción. La media, o promedio, se denota por \\(\\bar x\\) y se define como \\[\\begin{align} \\bar x = \\frac1N \\sum_{n = 1}^N x_n. \\end{align}\\] La desviación estándar muestral se define como \\[\\begin{align} \\text{std}(x) = \\sqrt{\\frac1{N-1} \\sum_{n = 1}^N (x_n - \\bar x)^2}. \\end{align}\\] Sin embargo, La media y desviación estándar son computacionalmente convenientes. Para el trabajo de modelado estas medidas de resumen tienen ventajas claras (bajo ciertos supuestos teóricos). En muchas ocasiones conviene usar estas medidas pues permite hacer comparaciones históricas o tradicionales —pues análisis anteriores pudieran estar basados en éstas. Medias recortadas. Una medida intermedia entre la mediana y la media es la media recortada. Si denotamos \\(G\\) al conjunto de datos original, y \\(p\\) un valor entre \\(0\\) y \\(1\\), entonces \\(G_p\\) es el coonjunto de datos que resulta de \\(G\\) cuando se excluye de \\(G\\) la proporción \\(p\\) de los datos más bajos y la proporción \\(p\\) de datos más altos. La media recortada-\\(p\\) es el promedio de los valores en \\(G_p\\). Considera el caso de tener \\(N\\) observaciones y asume que ya tienes calculado el promedio para dichas observaciones. Este promedio lo denotaremos por \\(\\bar x_N\\). Ahora, considera que has obtenido \\(M\\) observaciones más. Escribe una fórmula recursiva para la media del conjunto total de datos \\(\\bar x_{N+M}\\) en función de lo que ya tenías precalculado \\(\\bar x_N.\\) ¿En qué situaciones esta propiedad puede ser conveniente? Ejemplos Precios de casas En este ejemplo consideremos los datos de precios de ventas de la ciudad de Ames, Iowa. En particular nos interesa entender la variación del precio de las casas. Por este motivo calculamos los cuantiles que corresponden al 25%, 50% y 75% (cuartiles), así como el mínimo y máximo de los precios de las casas: quantile(casas %&gt;% pull(precio_miles)) ## 0% 25% 50% 75% 100% ## 37.9 132.0 165.0 215.0 755.0 Comprueba que el mínimo y máximo están asociados a los cuantiles 0% y 100%, respectivamente. Una posible comparación es considerar los precios y sus variación en función de zona de la ciudad en que se encuentra una vivienda. Podemos usar diagramas de caja y brazos para hacer una comparación burda de los precios en distintas zonas de la ciudad: ggplot(casas, aes(x = nombre_zona, y = precio_miles)) + geom_boxplot() + coord_flip() La primera pregunta que nos hacemos es cómo pueden variar las características de las casas dentro de cada zona. Para esto, podemos considerar el área de las casas. En lugar de graficar el precio, graficamos el precio por metro cuadrado, por ejemplo: ggplot(casas, aes(x = nombre_zona, y = precio_m2)) + geom_boxplot() + coord_flip() Podemos cuantificar la variación que observamos de zona a zona y la variación que hay dentro de cada una de las zonas. Una primera aproximación es observar las variación del precio al calcular la mediana dentro de cada zona, y después cuantificar por medio de cuantiles cómo varía la mediana entre zonas: casas %&gt;% group_by(nombre_zona) %&gt;% summarise(mediana_zona = median(precio_m2), .groups = &quot;drop&quot;) %&gt;% pull(mediana_zona) %&gt;% quantile() %&gt;% round() ## 0% 25% 50% 75% 100% ## 963 1219 1298 1420 1725 Por otro lado, las variaciones con respecto a las medianas dentro de cada zona, por grupo, se resume como: quantile(casas %&gt;% group_by(nombre_zona) %&gt;% mutate(residual = precio_m2 - median(precio_m2)) %&gt;% pull(residual)) %&gt;% round() ## 0% 25% 50% 75% 100% ## -765 -166 0 172 1314 Nótese que este último paso tiene sentido pues la variación dentro de las zonas, en términos de precio por metro cuadrado, es similar. Esto no lo podríamos haber hecho de manera efectiva si se hubiera utilizado el precio de las casas sin ajustar por su tamaño. Podemos resumir este primer análisis de varianza con un par de gráficas de cuantiles (Cleveland 1993): mediana &lt;- median(casas$precio_m2) resumen &lt;- casas %&gt;% group_by(nombre_zona) %&gt;% mutate(mediana_zona = median(precio_m2)) %&gt;% mutate(residual = precio_m2 - mediana_zona) %&gt;% ungroup() %&gt;% mutate(mediana_zona = mediana_zona - mediana) %&gt;% select(nombre_zona, mediana_zona, residual) %&gt;% pivot_longer(mediana_zona:residual, names_to = &quot;tipo&quot;, values_to = &quot;valor&quot;) ggplot(resumen, aes(sample = valor)) + geom_qq(distribution = stats::qunif) + facet_wrap(~ tipo) + ylab(&quot;Precio por m2&quot;) + xlab(&quot;f&quot;) + labs(subtitle = &quot;Precio por m2 por zona&quot;, caption = paste0(&quot;Mediana total de &quot;, round(mediana))) Vemos que la mayor parte de la variación del precio por metro cuadrado ocurre dentro de cada zona, una vez que controlamos por el tamaño de las casas. La variación dentro de cada zona es aproximadamente simétrica, aunque la cola derecha es ligeramente más larga con algunos valores extremos. Podemos seguir con otro indicador importante: la calificación de calidad de los terminados de las casas. Como primer intento podríamos hacer: Lo que indica que las calificaciones de calidad están distribuidas de manera muy distinta a lo largo de las zonas, y que probablemente no va ser simple desentrañar qué variación del precio se debe a la zona y cuál se debe a la calidad. Prueba Enlace Consideremos la prueba Enlace (2011) de matemáticas para primarias. Una primera pregunta que alguien podría hacerse es: ¿cuáles escuelas son mejores en este rubro, las privadas o las públicas? enlace_tbl &lt;- enlace %&gt;% group_by(tipo) %&gt;% summarise(n_escuelas = n(), cuantiles = list(cuantil(mate_6, c(0.05, 0.25, 0.5, 0.75, 0.95)))) %&gt;% unnest(cols = cuantiles) %&gt;% mutate(valor = round(valor)) enlace_tbl %&gt;% spread(cuantil, valor) %&gt;% formatear_tabla() tipo n_escuelas 0.05 0.25 0.5 0.75 0.95 Indígena/Conafe 13599 304 358 412 478 588 General 60166 380 454 502 548 631 Particular 6816 479 551 593 634 703 Para un análisis exploratorio podemos utilizar distintas gráficas. Por ejemplo, podemos utilizar nuevamente las gráficas de caja y brazos, así como graficar los percentiles. Nótese que en la gráfica 1 se utilizan los cuantiles 0.05, 0.25, 0.5, 0.75 y 0.95: Se puede discutir qué tan apropiada es cada gráfica con el objetivo de realizar comparaciones. Sin duda, graficar más cuantiles es más útil para hacer comparaciones. Por ejemplo, en la Gráfica 1 podemos ver que la mediana de las escuelas generales está cercana al cuantil 5% de las escuelas particulares. Por otro lado, el diagrama de caja y brazos muestra también valores “atípicos”. Es importante notar que una comparación más robusta se puede lograr por medio de pruebas de hipótesis, las cuales veremos mas adelante en el curso. Regresando a nuestro análisis exploratorio, notemos que la diferencia es considerable entre tipos de escuela. Antes de contestar prematuramente la pregunta: ¿cuáles son las mejores escuelas? busquemos mejorar la interpretabilidad de nuestras comparaciones usando los principios 2 y 3. Podemos comenzar por agregar, por ejemplo, el nivel del marginación del municipio donde se encuentra la escuela. Para este objetivo, podemos usar páneles (pequeños múltiplos útiles para hacer comparaciones) y graficar: Esta gráfica pone en contexto la pregunta inicial, y permite evidenciar la dificultad de contestarla. En particular: Señala que la pregunta no sólo debe concentarse en el tipo de “sistema”: pública, privada, etc. Por ejemplo, las escuelas públicas en zonas de marginación baja no tienen una distribución de calificaciones muy distinta a las privadas en zonas de marginación alta. El contexto de la escuela es importante. Debemos de pensar qué factores –por ejemplo, el entorno familiar de los estudiantes– puede resultar en comparaciones que favorecen a las escuelas privadas. Un ejemplo de esto es considerar si los estudiantes tienen que trabajar o no. A su vez, esto puede o no ser reflejo de la calidad del sistema educativo. Si esto es cierto, entonces la pregunta inicial es demasiado vaga y mal planteada. Quizá deberíamos intentar entender cuánto “aporta” cada escuela a cada estudiante, como medida de qué tan buena es cada escuela. Estados y calificaciones en SAT ¿Cómo se relaciona el gasto por alumno, a nivel estatal, con sus resultados académicos? Hay trabajo considerable en definir estos términos, pero supongamos que tenemos el siguiente conjunto de datos (Guber 1999), que son datos oficiales agregados por estado de Estados Unidos. Consideremos el subconjunto de variables sat, que es la calificación promedio de los alumnos en cada estado (para 1997) y expend, que es el gasto en miles de dólares por estudiante en (1994-1995). sat &lt;- read_csv(&quot;data/sat.csv&quot;) sat_tbl &lt;- sat %&gt;% select(state, expend, sat) %&gt;% gather(variable, valor, expend:sat) %&gt;% group_by(variable) %&gt;% summarise(cuantiles = list(cuantil(valor))) %&gt;% unnest(cols = c(cuantiles)) %&gt;% mutate(valor = round(valor, 1)) %&gt;% spread(cuantil, valor) sat_tbl %&gt;% formatear_tabla variable 0 0.25 0.5 0.75 1 expend 3.7 4.9 5.8 6.4 9.8 sat 844.0 897.2 945.5 1032.0 1107.0 Esta variación es considerable para promedios del SAT: el percentil 75 es alrededor de 1050 puntos, mientras que el percentil 25 corresponde a alrededor de 800. Igualmente, hay diferencias considerables de gasto por alumno (miles de dólares) a lo largo de los estados. Ahora hacemos nuestro primer ejercico de comparación: ¿Cómo se ven las calificaciones para estados en distintos niveles de gasto? Podemos usar una gráfica de dispersión: library(ggrepel) ggplot(sat, aes(x = expend, y = sat, label = state)) + geom_point(colour = &quot;red&quot;, size = 2) + geom_text_repel(colour = &quot;gray50&quot;) + xlab(&quot;Gasto por alumno (miles de dólares)&quot;) + ylab(&quot;Calificación promedio en SAT&quot;) Estas comparaciones no son de alta calidad, solo estamos usando 2 variables —que son muy pocas— y no hay mucho que podamos decir en cuanto explicación. Sin duda nos hace falta una imagen más completa. Necesitaríamos entender la correlación que existe entre las demás características de nuestras unidades de estudio. Las unidades que estamos comparando pueden diferir fuertemente en otras propiedades importantes (aka, dimensiones), lo cual no permite interpretar la gráfica de manera sencilla. Sabemos que es posible que el IQ difiera en los estados. Pero no sabemos cómo producir diferencias de este tipo. Sin embargo, ¡descubrimos que existe una variable adicional! Ésta es el porcentaje de alumnos de cada estado que toma el SAT. Podemos agregar como sigue: ggplot(sat, aes(x = expend, y = math, label=state, colour = frac)) + geom_point() + geom_text_repel() + xlab(&quot;Gasto por alumno (miles de dólares)&quot;) + ylab(&quot;Calificación en matemáticas&quot;) Esto nos permite entender por qué nuestra comparación inicial es relativamente pobre. Los estados con mejores resultados promedio en el SAT son aquellos donde una fracción relativamente baja de los estudiantes toma el examen. La diferencia es considerable. En este punto podemos hacer varias cosas. Una primera idea es intentar comparar estados más similares en cuanto a la población de alumnos que asiste. Podríamos hacer grupos como sigue: set.seed(991) k_medias_sat &lt;- kmeans(sat %&gt;% select(frac), centers = 4, nstart = 100, iter.max = 100) sat$clase &lt;- k_medias_sat$cluster sat &lt;- sat %&gt;% group_by(clase) %&gt;% mutate(clase_media = round(mean(frac))) %&gt;% ungroup %&gt;% mutate(clase_media = factor(clase_media)) sat &lt;- sat %&gt;% mutate(rank_p = rank(frac, ties= &quot;first&quot;) / length(frac)) ggplot(sat, aes(x = rank_p, y = frac, label = state, colour = clase_media)) + geom_point(size = 2) Estos resultados indican que es más probable que buenos alumnos decidan hacer el SAT. Lo interesante es que esto ocurre de manera diferente en cada estado. Por ejemplo, en algunos estados era más común otro examen: el ACT. Si hacemos clusters de estados según el % de alumnos, empezamos a ver otra historia. Para esto, ajustemos rectas de mínimos cuadrados como referencia: Sin embargo, el resultado puede variar considerablemente si categorizamos de distintas maneras. Tablas de conteos Consideremos los siguientes datos de tomadores de té (del paquete FactoMineR (Lê et al. 2008)): tea &lt;- read_csv(&quot;data/tea.csv&quot;) # nombres y códigos te &lt;- tea %&gt;% select(how, price, sugar) %&gt;% rename(presentacion = how, precio = price, azucar = sugar) %&gt;% mutate( presentacion = fct_recode(presentacion, suelto = &quot;unpackaged&quot;, bolsas = &quot;tea bag&quot;, mixto = &quot;tea bag+unpackaged&quot;), precio = fct_recode(precio, marca = &quot;p_branded&quot;, variable = &quot;p_variable&quot;, barato = &quot;p_cheap&quot;, marca_propia = &quot;p_private label&quot;, desconocido = &quot;p_unknown&quot;, fino = &quot;p_upscale&quot;), azucar = fct_recode(azucar, sin_azúcar = &quot;No.sugar&quot;, con_azúcar = &quot;sugar&quot;)) sample_n(te, 10) ## # A tibble: 10 x 3 ## presentacion precio azucar ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 mixto variable sin_azúcar ## 2 suelto fino con_azúcar ## 3 bolsas fino con_azúcar ## 4 mixto variable sin_azúcar ## 5 bolsas variable sin_azúcar ## 6 suelto variable con_azúcar ## 7 bolsas variable con_azúcar ## 8 mixto fino sin_azúcar ## 9 bolsas marca con_azúcar ## 10 mixto marca sin_azúcar Nos interesa ver qué personas compran té suelto, y de qué tipo. Empezamos por ver las proporciones que compran té según su empaque (en bolsita o suelto): precio &lt;- te %&gt;% count(precio) %&gt;% mutate(prop = round(100 * n / sum(n))) %&gt;% select(-n) tipo &lt;- te %&gt;% group_by(presentacion) %&gt;% tally() %&gt;% mutate(pct = round(100 * n / sum(n))) tipo %&gt;% formatear_tabla presentacion n pct bolsas 170 57 mixto 94 31 suelto 36 12 La mayor parte de las personas toma té en bolsas. Sin embargo, el tipo de té (en términos de precio o marca) que compran es muy distinto dependiendo de la presentación: tipo &lt;- tipo %&gt;% select(presentacion, prop_presentacion = pct) tabla_cruzada &lt;- te %&gt;% count(presentacion, precio) %&gt;% # porcentajes por presentación group_by(presentacion) %&gt;% mutate(prop = round(100 * n / sum(n))) %&gt;% select(-n) tabla_cruzada %&gt;% pivot_wider(names_from = presentacion, values_from = prop, values_fill = list(prop = 0)) %&gt;% formatear_tabla() precio bolsas mixto suelto marca 41 21 14 barato 3 1 3 marca_propia 9 4 3 desconocido 6 1 0 fino 8 20 56 variable 32 52 25 Estos datos podemos examinarlos un rato y llegar a conclusiones. Notemos que el uso de tablas no permite mostrar claramente patrones. Tampoco por medio de gráficas como la siguiente: ggplot(tabla_cruzada %&gt;% ungroup %&gt;% mutate(price = fct_reorder(precio, prop)), aes(x = precio, y = prop, group = presentacion, colour = presentacion)) + geom_point() + coord_flip() + geom_line() En lugar de eso, calcularemos perfiles columna. Esto es, comparamos cada una de las columnas con la columna marginal (en la tabla de tipo de estilo de té): num_grupos &lt;- n_distinct(te %&gt;% select(presentacion)) tabla &lt;- te %&gt;% count(presentacion, precio) %&gt;% group_by(presentacion) %&gt;% mutate(prop_precio = (100 * n / sum(n))) %&gt;% group_by(precio) %&gt;% mutate(prom_prop = sum(prop_precio)/num_grupos) %&gt;% mutate(perfil = 100 * (prop_precio / prom_prop - 1)) tabla ## # A tibble: 17 x 6 ## # Groups: precio [6] ## presentacion precio n prop_precio prom_prop perfil ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bolsas marca 70 41.2 25.4 61.8 ## 2 bolsas barato 5 2.94 2.26 30.1 ## 3 bolsas marca_propia 16 9.41 5.48 71.7 ## 4 bolsas desconocido 11 6.47 2.51 158. ## 5 bolsas fino 14 8.24 28.0 -70.6 ## 6 bolsas variable 54 31.8 36.3 -12.5 ## 7 mixto marca 20 21.3 25.4 -16.4 ## 8 mixto barato 1 1.06 2.26 -52.9 ## 9 mixto marca_propia 4 4.26 5.48 -22.4 ## 10 mixto desconocido 1 1.06 2.51 -57.6 ## 11 mixto fino 19 20.2 28.0 -27.8 ## 12 mixto variable 49 52.1 36.3 43.6 ## 13 suelto marca 5 13.9 25.4 -45.4 ## 14 suelto barato 1 2.78 2.26 22.9 ## 15 suelto marca_propia 1 2.78 5.48 -49.3 ## 16 suelto fino 20 55.6 28.0 98.4 ## 17 suelto variable 9 25 36.3 -31.1 tabla_perfil &lt;- tabla %&gt;% select(presentacion, precio, perfil, pct = prom_prop) %&gt;% pivot_wider(names_from = presentacion, values_from = perfil, values_fill = list(perfil = -100.0)) if_profile &lt;- function(x){ any(x &lt; 0) &amp; any(x &gt; 0) } marcar &lt;- marcar_tabla_fun(25, &quot;red&quot;, &quot;black&quot;) tab_out &lt;- tabla_perfil %&gt;% arrange(desc(bolsas)) %&gt;% select(-pct, everything()) %&gt;% mutate(across(where(is.numeric), round)) %&gt;% mutate(across(where(if_profile), marcar)) %&gt;% knitr::kable(format_table_salida(), escape = FALSE, booktabs = T) %&gt;% kableExtra::kable_styling(latex_options = c(&quot;striped&quot;, &quot;scale_down&quot;), bootstrap_options = c( &quot;hover&quot;, &quot;condensed&quot;), full_width = FALSE) if (knitr::is_latex_output()) { gsub(&quot;marca_propia&quot;, &quot;marca-propia&quot;, tab_out) } else { tab_out } precio bolsas mixto suelto pct desconocido 158 -58 -100 3 marca_propia 72 -22 -49 5 marca 62 -16 -45 25 barato 30 -53 23 2 variable -12 44 -31 36 fino -71 -28 98 28 Leemos esta tabla como sigue: por ejemplo, los compradores de té suelto compran té fino a una tasa casi el doble (98%) que el promedio. También podemos graficar como: tabla_graf &lt;- tabla_perfil %&gt;% ungroup %&gt;% mutate(precio = fct_reorder(precio, bolsas)) %&gt;% select(-pct) %&gt;% pivot_longer(cols = -precio, names_to = &quot;presentacion&quot;, values_to = &quot;perfil&quot;) g_perfil &lt;- ggplot(tabla_graf, aes(x = precio, xend = precio, y = perfil, yend = 0, group = presentacion)) + geom_point() + geom_segment() + facet_wrap(~presentacion) + geom_hline(yintercept = 0 , colour = &quot;gray&quot;)+ coord_flip() g_perfil Observación: hay dos maneras de construir la columna promedio: tomando los porcentajes sobre todos los datos, o promediando los porcentajes de las columnas. Si los grupos de las columnas están desbalanceados, estos promedios son diferentes. Cuando usamos porcentajes sobre la población, perfiles columna y renglón dan el mismo resultado Sin embargo, cuando hay un grupo considerablemente más grande que otros, las comparaciones se vuelven vs este grupo particular. No siempre queremos hacer esto. Interpretación En el último ejemplo de tomadores de té utilizamos una muestra de personas, no toda la población de tomadores de té. Eso quiere decir que tenemos cierta incertidumbre de cómo se generalizan o no los resultados que obtuvimos en nuestro análisis a la población general. Nuestra respuesta depende de cómo se extrajo la muestra que estamos considerando. Si el mecanismo de extracción incluye algún proceso probabilístico, entonces es posible en principio entender qué tan bien generalizan los resultados de nuestro análisis a la población general, y entender esto depende de entender qué tanta variación hay de muestra a muestra, de todas las posibles muestras que pudimos haber extraido. En las siguiente secciones discutiremos estos aspectos, en los cuales pasamos del trabajo de “detective” al trabajo de “juez” en nuestro trabajo analítico. Loess Las gráficas de dispersión son la herramienta básica para describir la relación entre dos variables cuantitativas, y como vimos en ejemplo anteriores, muchas veces podemos apreciar mejor la relación entre ellas si agregamos una curva loess que suavice. Los siguientes datos muestran los premios ofrecidos y las ventas totales de una lotería a lo largo de 53 sorteos (las unidades son cantidades de dinero indexadas). Graficamos en escalas logarítmicas y agregamos una curva loess. # cargamos los datos load(here::here(&quot;data&quot;, &quot;ventas_sorteo.Rdata&quot;)) ggplot(ventas.sorteo, aes(x = log(premio), y = log(ventas.tot.1))) + geom_point() + geom_smooth(method = &quot;loess&quot;, alpha = 0.5, method.args = list(degree = 1), se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; El patrón no era difícil de ver en los datos originales, sin embargo, la curva lo hace más claro, el logaritmo de las ventas tiene una relación no lineal con el logaritmo del premio: para premios no muy grandes no parece haber gran diferencia, pero cuando los premios empiezan a crecer por encima de 20,000 (aproximadamente \\(e^{10}\\)), las ventas crecen más rápidamente que para premios menores. Este efecto se conoce como bola de nieve, y es frecuente en este tipo de loterías. Antes de adentrarnos a loess comenzamos explicando cómo se ajustan familias paramétricas de curvas a conjuntos de datos dados. Ajustando familias paramétricas. Supongamos que tenemos la familia \\(f_{a,b}=ax+b\\) y datos bivariados \\((x_1,y_1), ..., (x_n, y_n)\\). Buscamos encontrar \\(a\\) y \\(b\\) tales que \\(f_{a,b}\\) de un ajuste óptimo a los datos. El criterio de mínimos cuadrados consiste en encontrar \\(a\\), \\(b\\) que minimicen la suma de cuadrados: \\[\\sum_{i=1}^n(y_i-ax_i-b)^2\\] En este caso, las constantes \\(a\\) y \\(b\\) se pueden encontrar diferenciando esta función objetivo. Más aún, estamos ajustando una recta a los datos, pero podemos repetir el argumento con otras familias de funciones (por ejemplo cuadráticas). ggplot(ventas.sorteo, aes(x = log(premio), y = log(ventas.tot.1))) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Donde los parámetros \\(a\\) y \\(b\\) están dados por: mod_lineal &lt;- lm(log(ventas.tot.1) ~ log(premio), data = ventas.sorteo) round(coef(mod_lineal), 2) ## (Intercept) log(premio) ## 4.56 0.47 De modo que la curva ajustada es \\(\\log(V) = 4.6 + 0.47 \\log(P)\\), o en las unidades originales \\(V = 100 P^{0.47}\\), donde \\(V\\) son las ventas y \\(P\\) el premio. Si observamos la gráfica notamos que este modelo lineal (en los logaritmos) no es adecuado para estos datos. Podríamos experimentar con otras familias (por ejemplo, una cuadrática o cúbica, potencias, exponenciales, etc.); sin embargo, en la etapa exploratoria es mejor tomar una ruta de ajuste más flexibles (aún cuando esta no sea con funciones algebráicas), que al mismo tiempo sea robusto. Observación: Los modelos de regresión lineal, cuando se pueden ajustar de manera razonable, son altamente deseables por su simplicidad: los datos se describen con pocos parámetros y tenemos incrementos marginales constantes en todo el rango de la variable que juega como factor, de modo que la interpretación es simple. Por esta razón, muchas veces vale la pena transformar los datos con el fin de enderezar la relación de dos variables y poder ajustar una función lineal. Ajustando curvas loess La idea es producir ajustes locales de rectas o funciones cuadráticas. En estas familias es necesario especificar dos parámetros: Parámetro de suavizamiento \\(\\alpha\\): cuando \\(\\alpha\\) es más grande, la curva ajustada es más suave. Grado de los polinomios locales que ajustamos \\(\\lambda\\): generalmente se toma \\(\\lambda=1,2\\). Entonces, supongamos que los datos están dados por \\((x_1,y_1), ..., (x_n, y_n)\\), y sean \\(\\alpha\\) un parámetro de suavizamiento fijo, y \\(\\lambda=1\\). Denotamos como \\(\\hat{g}(x)\\) la curva loess ajustada, y como \\(w_i(x)\\) a una función de peso (que depende de x) para la observación \\((x_i, y_i)\\). Para poder calcular \\(w_i(x)\\) debemos comenzar calculando \\(q=\\lfloor{n\\alpha}\\rfloor\\) que suponemos mayor que uno. Ahora definimos la función tricubo: \\[ \\begin{equation} T(u)=\\begin{cases} (1-|u|^3)^3, &amp; \\text{para $|u| &lt; 1$}.\\\\ 0, &amp; \\text{en otro caso}. \\end{cases} \\end{equation} \\] entonces, para el punto \\(x\\) definimos el peso correspondiente al dato \\((x_i,y_i)\\), denotado por \\(w_i(x)\\) como: \\[w_i(x)=T\\bigg(\\frac{|x-x_i|}{d_q(x)}\\bigg)\\] donde \\(d_q(x)\\) es el valor de la \\(q-ésima\\) distancia más chica (la más grande entre las \\(q\\) más chicas) entre los valores \\(|x-x_j|\\), \\(j=1,2,...,n\\). De esta forma, las observaciones \\(x_i\\) reciben más peso cuanto más cerca estén de \\(x\\). En palabras, de \\(x_1,...,x_n\\) tomamos los \\(q\\) datos más cercanos a \\(x\\), que denotamos \\(x_{i_1}(x) \\leq x_{i_2}(x) \\leq \\cdots x_{i_q}(x) \\leq\\). Los re-escalamos a \\([0,1]\\) haciendo corresponder \\(x\\) a \\(0\\) y el punto más alejado de \\(x\\) (que es \\(x_{i_q}\\)) a 1. Aplicamos el tricubo (gráfica de abajo), para encontrar los pesos de cada punto. Los puntos que están a una distancia mayor a \\(d_q(x)\\) reciben un peso de cero, y los más cercanos un peso que depende de que tan cercanos están a \\(x\\). tricubo &lt;- function(x) { ifelse(abs(x) &lt; 1, (1 - abs(x) ^ 3) ^ 3, 0) } curve(tricubo, from = -1.5, to = 1.5) Finalmente ajustamos una recta de mínimos cuadrados ponderados por los pesos \\(w_i(x)\\), es decir, minimizamos \\[\\sum_{i=1}^nw_i(x)(y_i-ax_i-b)^2\\] Hacemos esto para cada valor de \\(x\\) que está en el rango de los datos \\(x_1,...,x_n\\). Observaciones: Cualquier función con la forma de flan del tricubo (se desvanece fuera de \\((-1,1)\\), es creciente en \\((-1,0)\\) y decreciente en \\((0, 1)\\), además de ser continua y quizás diferenciable) es un buen candidato para usar en lugar del tricubo. La razón por la que escogemos precisamente esta forma algebráica no tiene que ver con el análisis exploratorio, sino con las ventajas teóricas adicionales que tiene en la inferencia. El caso \\(\\lambda=2\\) es similar. La única diferencia es en el paso de ajuste, donde usamos funciones cuadráticas, y obtendríamos entonces tres parámetros \\(a(x), b(x), c(x)\\). Escogiendo de los parámetros. Los parámetros \\(\\alpha\\) y \\(\\lambda\\) se encuentran por ensayo y error. La idea general es que debemos encontrar una curva que explique patrones importantes en los datos (que ajuste los datos) pero que no muestre variaciones a escalas más chicas difíciles de explicar (que pueden ser el resultado de influencias de otras variables, variación muestral, ruido o errores de redondeo, por ejemplo). En el proceso de prueba y error iteramos el ajuste y en cada paso hacemos análisis de residuales, con el fin de seleccionar un suavizamiento adecuado. Ejemplo de distintas selecciones de \\(\\lambda\\), en este ejemplo consideramos la ventas semanales de un producto a lo largo de 5 años. Series de tiempo Podemos usar el suavizamiento loess para entender y describir el comportamiento de series de tiempo, en las cuáles intentamos entender la dependencia de una serie de mediciones indexadas por el tiempo. Típicamente es necesario utilizar distintas componentes para describir exitosamente una serie de tiempo, y para esto usamos distintos tipos de suavizamientos. Veremos que distintas componentes varían en distintas escalas de tiempo (unas muy lentas, cono la tendencia, otras más rapidamente, como variación quincenal, etc.). Caso de estudio: nacimientos en México Este caso de estudio esta basado en un análisis propuesto por A. Vehtari y A. Gelman, junto con un análisis de serie de tiempo de Cleveland (1993). En nuestro caso, usaremos los datos de nacimientos registrados por día en México desde 1999. Los usaremos para contestar las preguntas: ¿cuáles son los cumpleaños más frecuentes? y ¿en qué mes del año hay más nacimientos? Podríamos utilizaar una gráfica popular (ver por ejemplo esta visualización) como: Sin embargo, ¿cómo criticarías este análisis desde el punto de vista de los tres primeros principios del diseño analítico? ¿Las comparaciones son útiles? ¿Hay aspectos multivariados? ¿Qué tan bien explica o sugiere estructura, mecanismos o causalidad? Datos de natalidad para México library(lubridate) library(ggthemes) theme_set(theme_minimal(base_size = 14)) natalidad &lt;- readRDS(&quot;./data/nacimientos/natalidad.rds&quot;) %&gt;% mutate(dia_semana = weekdays(fecha)) %&gt;% mutate(dia_año = yday(fecha)) %&gt;% mutate(año = year(fecha)) %&gt;% mutate(mes = month(fecha)) %&gt;% ungroup %&gt;% mutate(dia_semana = recode(dia_semana, Monday = &quot;Lunes&quot;, Tuesday = &quot;Martes&quot;, Wednesday = &quot;Miércoles&quot;, Thursday = &quot;Jueves&quot;, Friday = &quot;Viernes&quot;, Saturday = &quot;Sábado&quot;, Sunday = &quot;Domingo&quot;)) %&gt;% mutate(dia_semana = fct_relevel(dia_semana, c(&quot;Lunes&quot;, &quot;Martes&quot;, &quot;Miércoles&quot;, &quot;Jueves&quot;, &quot;Viernes&quot;, &quot;Sábado&quot;, &quot;Domingo&quot;))) Consideremos los datos agregados del número de nacimientos (registrados) por día desde 1999 hasta 2016. Un primer intento podría ser hacer una gráfica de la serie de tiempo. Sin embargo, vemos que no es muy útil: Hay varias características que notamos. Primero, parece haber una tendencia ligeramente decreciente del número de nacimientos a lo largo de los años. Segundo, la gráfica sugiere un patrón anual. Y por último, encontramos que hay dispersión producida por los días de la semana. Sólo estas características hacen que la comparación entre días sea difícil de realizar. Supongamos que comparamos el número de nacimientos de dos miércoles dados. Esa comparación será diferente dependiendo: del año donde ocurrieron, el mes donde ocurrieron, si semana santa ocurrió en algunos de los miércoles, y así sucesivamente. Como en nuestros ejemplos anteriores, la idea del siguiente análisis es aislar las componentes que observamos en la serie de tiempo: extraemos componentes ajustadas, y luego examinamos los residuales. En este caso particular, asumiremos una descomposición aditiva de la serie de tiempo (Cleveland 1993). En el estudio de series de tiempo una estructura común es considerar el efecto de diversos factores como tendencia, estacionalidad, ciclicidad e irregularidades de manera aditiva. Esto es, consideramos la descomposición \\[\\begin{align} y(t) = f_{t}(t) + f_{e}(t) + f_{c}(t) + \\varepsilon. \\end{align}\\] Una estrategia de ajuste, como veremos más adelante, es proceder de manera modular. Es decir, se ajustan los componentes de manera secuencial considerando los residuales de los anteriores. Tendencia Comenzamos por extraer la tendencia, haciendo promedios loess (Cleveland 1979) con vecindades relativamente grandes. Quizá preferiríamos suavizar menos para capturar más variación lenta, pero si hacemos esto en este punto empezamos a absorber parte de la componente anual: mod_1 &lt;- loess(n ~ as.numeric(fecha), data = natalidad, span = 0.2, degree = 1) datos_dia &lt;- natalidad %&gt;% mutate(ajuste_1 = fitted(mod_1)) %&gt;% mutate(res_1 = n - ajuste_1) Notemos que a principios de 2000 el suavizador está en niveles de alrededor de 7000 nacimientos diarios, hacia 2015 ese número es más cercano a unos 6000. Componente anual Al obtener la tendencia podemos aislar el efecto a largo plazo y proceder a realizar mejores comparaciones (por ejemplo, comparar un día de 2000 y de 2015 tendria más sentido). Ahora, ajustamos los residuales del suavizado anterior, pero con menos suavizamiento. Así evitamos capturar tendencia: mod_anual &lt;- loess(res_1 ~ as.numeric(fecha), data = datos_dia, degree = 2, span = 0.005) datos_dia &lt;- datos_dia %&gt;% mutate(ajuste_2 = fitted(mod_anual)) %&gt;% mutate(res_2 = res_1 - ajuste_2) Día de la semana Hasta ahora, hemos aislado los efectos por plazos largos de tiempo (tendencia) y hemos incorporado las variaciones estacionales (componente anual) de nuestra serie de tiempo. Ahora, veremos cómo capturar el efecto por día de la semana. En este caso, podemos hacer suavizamiento loess para cada serie de manera independiente datos_dia &lt;- datos_dia %&gt;% group_by(dia_semana) %&gt;% nest() %&gt;% mutate(ajuste_mod = map(data, ~ loess(res_2 ~ as.numeric(fecha), data = .x, span = 0.1, degree = 1))) %&gt;% mutate(ajuste_3 = map(ajuste_mod, fitted)) %&gt;% select(-ajuste_mod) %&gt;% unnest(cols = c(data, ajuste_3)) %&gt;% mutate(res_3 = res_2 - ajuste_3) %&gt;% ungroup Residuales Por último, examinamos los residuales finales quitando los efectos ajustados: ## `geom_smooth()` using formula &#39;y ~ x&#39; Observación: nótese que la distribución de estos residuales presenta irregularidades interesantes. La distribución es de colas largas, y no se debe a unos cuantos datos atípicos. Esto generalmente es indicación que hay factores importantes que hay que examinar mas a detalle en los residuales: Reestimación Cuando hacemos este proceso secuencial de llevar el ajuste a los residual, a veces conviene iterarlo. La razón es que un una segunda o tercera pasada podemos hacer mejores estimaciones de cada componente, y es posible suavizar menos sin capturar componentes de más alta frecuencia. Así que podemos regresar a la serie original para hacer mejores estimaciones, más suavizadas: # Quitamos componente anual y efecto de día de la semana datos_dia &lt;- datos_dia %&gt;% mutate(n_1 = n - ajuste_2 - ajuste_3) # Reajustamos mod_1 &lt;- loess(n_1 ~ as.numeric(fecha), data = datos_dia, span = 0.02, degree = 2, family = &quot;symmetric&quot;) Y ahora repetimos con la componente de día de la semana: Análisis de componentes Ahora comparamos las componentes estimadas y los residuales en una misma gráfica. Por definición, la suma de todas estas componentes da los datos originales. Este último paso nos permite diversas comparaciones que explican la variación que vimos en los datos. Una gran parte de los residuales está entre \\(\\pm 250\\) nacimientos por día. Sin embargo, vemos que las colas tienen una dispersión mucho mayor: quantile(datos_dia$res_6, c(00, .01,0.05, 0.10, 0.90, 0.95, 0.99, 1)) %&gt;% round ## 0% 1% 5% 10% 90% 95% 99% 100% ## -2238 -1134 -315 -202 188 268 516 2521 ¿A qué se deben estas colas tan largas? Viernes 13? Podemos empezar con una curosidad: en viernes o martes 13, ¿nacen menos niños? Nótese que fue útil agregar el indicador de Semana santa por el Viernes 13 de Semana Santa que se ve como un atípico en el panel de los viernes 13. Residuales: antes y después de 2006 Veamos primero una agregación sobre los años de los residuales. Lo primero es observar un cambio que sucedió repentinamente en 2006: La razón es un cambio en la ley acerca de cuándo pueden entrar los niños a la primaria. Antes era por edad y había poco margen. Ese exceso de nacimientos son reportes falsos para que los niños no tuvieran que esperar un año completo por haber nacido unos cuantos días antes de la fecha límite. Otras características que debemos investigar: Efectos de Año Nuevo, Navidad, Septiembre 16 y otros días feriados como Febrero 14. Semana santa: como la fecha cambia, vemos que los residuales negativos tienden a ocurrir dispersos alrededor del día 100 del año. Otros días especiales: más de residuales Ahora promediamos residuales (es posible agregar barras para indicar dispersión a lo largo de los años) para cada día del año. Podemos identificar ahora los residuales más grandes: se deben, por ejemplo, a días feriados, con consecuencias adicionales que tienen en días ajuntos (excesos de nacimientos): ## `summarise()` regrouping output by &#39;dia_año_366&#39;, &#39;antes_2006&#39; (override with `.groups` argument) Semana santa Para Semana Santa tenemos que hacer unos cálculos. Si alineamos los datos por días antes de Domingo de Pascua, obtenemos un patrón de caída fuerte de nacimientos el Viernes de Semana Santa, y la característica forma de “valle con hombros” en días anteriores y posteriores estos Viernes. ¿Por qué ocurre este patrón? ## `geom_smooth()` using formula &#39;y ~ x&#39; Nótese un defecto de nuestro modelo: el patrón de “hombros” alrededor del Viernes Santo no es suficientemente fuerte para equilibrar los nacimientos faltantes. ¿Cómo podríamos mejorar nuestra descomposición? Referencias "],
["tipos-de-estudio-y-experimentos.html", "Sección 3 Tipos de estudio y experimentos Muestreo aleatorio Pero si no podemos hacer muestreo aleatorio? El estimador estándar Experimentos tradicionales Bloqueo Variables desconocidas Aleatorizando el tratamiento Resumen: selección de unidades y tratamiento Asignación natural del tratamiento", " Sección 3 Tipos de estudio y experimentos Motivación Pregunta de entrevista de Google (Chihara and Hesterberg 2018) Imagina que eres consultor y te preguntan lo siguiente (ver siguiente figura): Estoy haciendo una comparación de antes y después donde la hipótesis alternativa es pre.media.error &gt; post.media.error. La distribución de ambas muestras es sesgada a la derecha. ¿Qué prueba me recomiendas para ésta situación? Figure 3.1: Error CPR, gráfica de densidad. La siguiente imagen Roger Peng representa una situación común a la que se enfrenta el analista de datos, y se desarrolló en el contexto de preguntas vagas. En el esquema hay tres caminos: uno es uno ideal que pocas veces sucede, otro produce respuestas poco útiles pero es fácil, y otro es tortuoso pero que caracteriza el mejor trabajo de análisis de datos: Figure 3.2: Adaptado de R. Peng: Tukey, design thinking and better questions. Ejemplos: Alguien nos pregunta cuáles son las tiendas que mas venden de una cadena. Podríamos consultar bases de datos, hacer extracciones, definir periodos, etc. y dar una respuesta que probablemente es poco útil. Nos damos cuenta, por ejemplo, porque la peor tienda es una que abrió hace relativamente poco, y la mejor es una de las tiendas más grandes que está en una zona de tráfico de alto costo. Una pregunta más interesante es, ¿qué equipos de ventas tienen mejor desempeño? ¿Cuánto aporta tener una cafetería dentro de la tienda en términos de ventas?, etc. Proceso Generador de Datos Entre las preguntas que se debe hacer el analista de datos una fundamental es entender el proceso generador de datos, pues esto determinará que otras preguntas son relevantes, tanto en términos prácticos como estadísticos. La inferencia estadística busca hacer afirmaciones, cuantificadas de manera probabilista, acerca de datos que no tenemos, usando regularidades y conocimiento de datos que sí tenemos disponibles y métodos cuantitativos. Para hacer afirmaciones inferenciales eficientes y bien calibradas (con garantías estadísticas de calibración) a preguntas donde queremos generalizar de muestra a población, se requiere conocer con precisión el proceso que genera los datos muestrales. Esto incluye saber con detalle cómo se seleccionaron los datos a partir de los que se quiere hacer inferencia. En este caso, eficiente quiere decir que aprovechamos toda la información que está en los datos observados de manera que nuestros rangos de incertidumbre son lo más chico posibles (además de estar correctamente calibrados). Por su parte, probabilísticamente bien calibrados se refiere a que, lo que decimos que puede ocurrir con 10% de probabilidad ocurre efectivamente 1 de cada 10 veces, si decimos 20% entonces ocurre 2 de 20, etc. Veremos que para muestras dadas naturalmente, a veces es muy difiícil entender a fondo el proceso generación de la muestra. Ejemplo: Prevalencia de anemia Supongamos que nos interesa conocer el porcentaje de menores en edad escolar, (entre 6 y 15 años), con anemia en México. La fuente de datos disponible corresponde a registros de del IMSS de hospitalizaciones de menores, ya sea por anemia o que por otra causa (infecciones gastrointestinales, apendicitis, tratamiento de leucemia, …), se registró si el menor tenía anemia. En nuestra muestra el 47% de los niños tiene anemia. head(paciente) #&gt; # A tibble: 6 x 4 #&gt; edad padecimiento sexo anemia #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 8 picadura alacrán mujer 0 #&gt; 2 10 infección intestinal hombre 1 #&gt; 3 7 mordedura de perro hombre 1 #&gt; 4 8 asma hombre 1 #&gt; 5 13 infección intestinal mujer 0 #&gt; 6 7 picadura alacrán hombre 0 ¿Qué nos dice esta cantidad acerca de la anemia en la población? ¿Podemos hacer inferencia estadística? ¿Cómo calculamos intervalos de confianza? # Si calculo el error estándar de la p estimada como sigue, es correcto? p &lt;- mean(paciente$anemia) sqrt(p * (1 - p) / 5000) #&gt; [1] 0.007060751 Muestreo aleatorio En la situación ideal diseñaríamos una muestra aleatoria de menores de edad, por ejemplo, utilizando el registro en educación primaria de la SEP, y mediríamos la prevalencia de anemia en la muestra, usaríamos esta muestra para estimar la prevalencia en la población y tendríamos además las herramientas para medir la incertidumbre de nuestra estimación (reportar intervalos, o errores estándar). Pero si no podemos hacer muestreo aleatorio? En el caso de prevalencia de anemia, discutiendo con médicos e investigadores nos informan que la anemia se presenta en tasas más altas en niños más chicos. paciente %&gt;% count(edad) %&gt;% mutate(prop = round(100 * n / sum(n))) #&gt; # A tibble: 10 x 3 #&gt; edad n prop #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 6 1001 20 #&gt; 2 7 931 19 #&gt; 3 8 980 20 #&gt; 4 9 445 9 #&gt; 5 10 484 10 #&gt; 6 11 489 10 #&gt; 7 12 246 5 #&gt; 8 13 239 5 #&gt; 9 14 90 2 #&gt; 10 15 95 2 Y consultando con las proyecciones de población notamos que los niños chicos están sobrerepresentados en la muestra. Lo que nos hace considerar que debemos buscar una manera de ponderar nuestras observaciones para que reflejen a la población. Más aún, investigamos que algunas enfermedades están asociadas a mayor prevalencia de anemia: paciente %&gt;% count(padecimiento) %&gt;% arrange(-n) #&gt; # A tibble: 7 x 2 #&gt; padecimiento n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 infección respiratoria 745 #&gt; 2 mordedura de perro 723 #&gt; 3 úlcera 723 #&gt; 4 asma 713 #&gt; 5 apendcitis 704 #&gt; 6 picadura alacrán 701 #&gt; 7 infección intestinal 691 Utilizamos esta información para modelar y corregir nuestra estimación original. Por ejemplo con modelos de regresión. Sin embargo, debemos preguntarnos: ¿Hay más variables qué nos falta considerar? Nuestras estimaciones están bien calibradas? Ejemplo: Policías y tráfico Supongamos que nos preguntan en cuánto reduce un policía el tráfico en un crucero grande de la ciudad. La cultura popular ha establecido que los policías en cruceros hacen más tráfico porque no saben mover los semáforos. Nosotros decidimos buscar unos datos para entender esto. Escogemos entonces un grupo de cruceros problemáticos, registramos el tráfico cuando visitamos, y si había un policía o no. Después de este esfuerzo, obtenemos los siguientes datos: #&gt; # A tibble: 10 x 2 #&gt; # Groups: policia [2] #&gt; policia tiempo_espera_min #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 0 2.27 #&gt; 2 0 2.65 #&gt; 3 0 3.4 #&gt; 4 0 0.39 #&gt; 5 0 1.1 #&gt; 6 1 10.8 #&gt; 7 1 4.67 #&gt; 8 1 7.77 #&gt; 9 1 6.3 #&gt; 10 1 6.99 Lo que sabemos ahora es que la presencia de un policía es indicador de tráfico alto. El análisis prosiguiría calculando medias y medidas de error (escogimos una muestra aleatoria): Si somos ingenuos, entonces podríamos concluir que los policías efectivamente empeoran la situación cuando manipulan los semáforos, y confirmaríamos la sabiduría popular. Para juzgar este argumento desde el punto de vista causal, nos preguntamos primero: ¿Cuáles son los contrafactuales (los contrafactuales explican que pasaría si hubiéramos hecho otra cosa que la que efectivamente hicimos) de las observaciones? El estimador estándar A la comparación anterior - la diferencia de medias de tratados y no tratados - le llamamos usualmente el estimador estándar del efecto causal. Muchas veces este es un estimador malo del efecto causal. En nuestro ejemplo, para llegar a la conclusión errónea que confirma la sabiduría popular, hicimos un supuesto importante: En nuestra muestra, los casos con policía actúan como contrafactuales de los casos sin policía. Asi que asumimos que los casos con policía y sin policía son similares, excepto por la existencia o no de policía. En nuestro ejemplo, quizá un analista más astuto nota que tienen categorías históricas de qué tan complicado es cada crucero. Junta a sus datos, y obtiene: #&gt; # A tibble: 10 x 3 #&gt; # Groups: policia [2] #&gt; policia tiempo_espera_min categoria #&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 0 2.27 Fluido #&gt; 2 0 2.65 Fluido #&gt; 3 0 3.4 Típico #&gt; 4 0 0.39 Fluido #&gt; 5 0 1.1 Fluido #&gt; 6 1 10.8 Complicado #&gt; 7 1 4.67 Típico #&gt; 8 1 7.77 Complicado #&gt; 9 1 6.3 Complicado #&gt; 10 1 6.99 Típico El analista argumenta entonces qu los policías se enviaron principalmente a cruceros que se consideran Complicados según datos históricos. Esto resta credibilidad a la comparación que hicimos inicialmente: La comparación del estimador estándar no es de peras con peras: estamos comparando qué efecto tienen los policías en cruceros difíciles con cruceros no difíciles donde no hay policía. La razón de esto es que el proceso generador de los datos incluye el hecho de que no se envían policías a lugares donde no hay tráfico. ¿Cómo producir contrafactuales hacer la comparación correcta? Experimentos tradicionales Idealmente, quisiéramos observar un mismo crucero en las dos condiciones: con y sin policías. Esto no es posible. En un experimento “tradicional”, como nos lo explicaron en la escuela, nos aproximamos a esto preparando dos condiciones idénticas, y luego alteramos cada una de ellas con nuestra intervención. Si el experimento está bien hecho, esto nos da observaciones en pares, y cada quien tiene su contrafactual. La idea del experimiento tradicional es controlar todos los factores que intervienen en los resultados, y sólo mover el tratamiento para producir los contrafactuales. Más en general, esta estrategia consiste en hacer bloques de condiciones, donde las condiciones son prácticamente idénticas dentro e cada bloque. Comparamos entonces unidades tratadas y no tratadas dentro de cada bloque. Por ejemplo, si queremos saber si el tiempo de caída libre es diferente para un objeto más pesado que otro, prepararíamos dos pesos con el mismo tamaño pero de peso distinto. Soltaríamos los dos al mismo tiempo y compararíamos el tiempo de caída de cada uno. En nuestro caso, como es usual en problemas de negocio o sociales, hacer esto es considerablemente más difícil. No podemos “preparar” cruceros con condiciones idénticas. Sin embargo, podríamos intentar bloquear los cruceros según información que tenemos acerca de ellos, para hacer más comparaciones e peras con peras. Bloqueo Podemos acercanos en lo posible a este ideal de experimentación usando información existente. En lugar de hacer comparaciones directas entre unidades que recibieron el tratamiento y las que no (que pueden ser diferentes en otros aspectos, como vimos arriba), podemos refinar nuestras comparaciones bloquéandolas con variables conocidas. En el ejemplo de los policías, podemos hacer lo siguiente: dentro de cada categoría de cruceros (fluido, típico o complicado), tomaremos una muestra de cruceros, algunos con policía y otros sin. Haremos comparaciones dentro de cada categoría. Obtenemos un muestra con estas características (6 casos en cada categoría de crucero, 3 con policía y 3 sin policía): categoria policia n Fluido 0 3 Fluido 1 3 Típico 0 3 Típico 1 3 Complicado 0 3 Complicado 1 3 Y ahora hacemos comparaciones dentro de cada bloque creado por categoría: #&gt; # A tibble: 3 x 3 #&gt; # Groups: categoria [3] #&gt; categoria `policia =0` `policia =1` #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Fluido 2.1 0.8 #&gt; 2 Típico 5.6 4.2 #&gt; 3 Complicado 10.4 8.6 Y empezamos a ver otra imagen en estos datos: comparando tipos e cruceros similares, los que tienen policía tienen tiempos de espera ligeramente más cortos. ¿Hemos termniado? ¿Podemos concluir que el efecto de un policía es beneficiosos pero considerablemente chico? ¿Qué problemas puede haber con este análisis? Variables desconocidas El problema con el análisis anterior es que controlamos por una variable que conocemos, pero muchas otras variables pueden estar ligadas con el proceso de selección de cruceros para enviar policías. Por ejemplo, envían o policías a cruceros Típicos solo cuando reportan mucho tráfico. No envían a un polícia a un crucero Complicado si no presenta demasiado tráfico. Existen otras variables desconocidas que los tomadores de decisiones usan para enviar a los policías. En este caso, por ejemplo, los expertos hipotéticos nos señalan que hay algunos cruceros que aunque problemáticos a veces, su tráfico se resuelve rápidamente, mientras que otros tienen tráfico más persistente, y prefieren enviar policías a los de tráfico persistente. La lista de cruceros persistentes están en una hoja de excel que se comparte de manera informal. En resumen, no tenemos conocimiento detallado del proceso generador de datos en cuanto a cómo se asignan los policías a los cruceros. Igual que en la sección anterior, podemos cortar esta complejidad usando aleatorización. Nótese que los expertos no están haciendo nada malo: en su trabajo están haciendo el mejor uso de los recursos que tienen. El problema es que por esa misma razón no podemos saber el resultado de sus esfuerzos, y si hay maneras de optimizar la asignación que hacen actualmente. Aleatorizando el tratamiento Tomamos la decisión entonces de hacer un experimento que incluya aletorización. En un dia particular, escogeremos algunos cruceros. Dicidimos usar solamente cruceros de la categoría Complicada y Típica, pues esos son los más interesantes para hacer intervenciones. Usaremos un poco de código para entener el detalle: en estos datos, tenemos para cada caso los dos posibles resultados hipotéticos \\(y_0\\) y \\(y_1\\) (con policia y sin policia). En el experimento asignamos el tratamiento al azar: muestra_exp &lt;- trafico_tbl %&gt;% filter(categoria != &quot;Fluido&quot;) %&gt;% sample_n(200) %&gt;% # asignar tratamiento al azar, esta es nuestra intervención: mutate(tratamiento_policia = rbernoulli(length(y_0), 0.5)) %&gt;% # observar resultado mutate(tiempo_espera_exp = ifelse(tratamiento_policia ==1, y_1, y_0)) Nótese la diferencia si tomamos la asignación natural del tratamiento (policía o no): set.seed(134) muestra_natural &lt;- trafico_tbl %&gt;% filter(categoria != &quot;Fluido&quot;) %&gt;% sample_n(200) %&gt;% # usamos el tratamiento que se asignó # policia indica si hubo o no policía en ese crucero # observar resultado mutate(tiempo_espera_obs = ifelse(policia ==1, y_1, y_0)) Resumimos nuestros resultados del experimento son: #&gt; # A tibble: 2 x 3 #&gt; # Groups: categoria [2] #&gt; categoria `policia=0` `policia=1` #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Típico 6.24 4.97 #&gt; 2 Complicado 15.8 8.47 Sin embargo, la muestra natural da: #&gt; # A tibble: 2 x 3 #&gt; # Groups: categoria [2] #&gt; categoria `policia=0` `policia=1` #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Típico 5.49 4.35 #&gt; 2 Complicado 10.8 8.93 ¿Cuál de los dos análisis da la respuesta correcta a la pregunta: ayudan o no los policías a reducir el tráfico en los cruceros problemáticos? El experimento establece que un policía en promedio reduce a la mitad el tiempo de espera en un crucero complicado Resumen: selección de unidades y tratamiento Vimos dos tipos de inferencia que requieren distintos diseños de estudio, en particual debemos considerar el mecanismo de aleatorización para entender las inferencias que podemos hacer: casual o a poblaciones. El punto crucial para entender las medidas de incertidumbre estadística es visualizar de manera hipotética, replicaciones del estudio y las condiciones que llevaron a la selección de la muestra. Esto es, entender el proceso generador de datos e imaginar replicarlo. Inferencia estadística de acuerdo al tipo del diseño (Ramsey and Schafer 2012). El cuadro arriba a la izquierda es donde el análisis es más simple y los resultados son más fáciles de interpretar. Es posible hacer análisis fuera de este cuadro, pero el proceso es más complicado, requieren más supuestos, conocimiento del dominio y habilidades de análisis. En general resultan conclusiones menos sólidas. Muchas veces no nos queda otra más que trabajar fuera del cuadro ideal. Ubica los siguientes tipos de análisis: Pruebas clínicas para medicinas Analizar cómo afecta tener seguro médico a los ingresos, usando datos del ENIGH. Estimación de retorno sobre inversión en modelos de marketing mix. Asignación natural del tratamiento Cuando consideramos un sistema donde se “asignan” tratamientos, generalmente los tratamientos se asignan bajo un criterio de optimización o conveniencia. La cara buena de este hecho es que de alguna forma los resultados están intentando optimizarse, y la gente está haciendo su trabajo. La cara mala de este hecho es que no podemos evaluar de manera simple la efectividad de los tratamientos. Y esto hace difícil optimizar de forma cuantificable los procesos, o entender qué funciona y qué no. Referencias "],
["pruebas-de-hipótesis.html", "Sección 4 Pruebas de hipótesis Comparación con poblaciones de referencia Comparando distribuciones Permutaciones y el lineup Comparaciones con lineup 2 Prueba de permutaciones para proporciones Pruebas de hipótesis tradicionales Tomadores de té 2 Pruebas de permutación: Implementación Ejemplo: tiempos de fusión Ejemplo: tiempos de fusión 2 Separación de grupos La “crisis de replicabilidad” El jardín de los senderos que se bifurcan Ejemplo: decisiones de análisis y valores p Alternativas o soluciones", " Sección 4 Pruebas de hipótesis Las primeras técnicas inferenciales que veremos intentan contestar la siguiente pregunta: Si observamos cierto patrón en los datos, ¿cómo podemos cuantificar la evidencia de que es un patrón notable y no sólo debido a fluctuaciones en los datos particulares que tenemos? ¿Cómo sabemos que no estamos sobreinterpretando esas fluctuaciones? Por ejemplo: Un sistema tiene cierto comportamiento “usual” para el cual tenemos datos históricos. El sistema presenta fluctuaciones en el tiempo. Observamos la última salida de nuestro sistema. Naturalmente, tiene fluctuaciones. ¿Esas fluctuaciones son consistentes con la operación usual del sistema? ¿Existe evidencia para pensar que algo en el sistema cambió? Comparación con poblaciones de referencia En las prueba de hipótesis, tratamos de construir distribuciones de referencia para comparar resultados que obtengamos con un “estándar” de variación, y juzgar si nuestros resultados son consistentes con la referencia o no (Box et al. (1978)). En algunos casos, ese estándar de variación puede construirse con datos históricos. Ejemplo Supongamos que estamos considerando cambios rápidos en una serie de tiempo de alta frecuencia. Hemos observado la serie en su estado “normal” durante un tiempo considerable, y cuando observamos nuevos datos quisiéramos juzgar si hay indicaciones o evidencia en contra de que el sistema sigue funcionando de manera similar. Digamos que monitoreamos ventanas de tiempo de tamaño 20 y necesitamos tomar una decisión. Abajo mostramos cinco ejemplos donde el sistema opera normalmente, que muestra la variabilidad en el tiempo en ventanas cortas del sistema. Ahora suponemos que obtenemos una nueva ventana de datos. ¿Hay evidencia en contra de que el sistema sigue funcionando de manera similar? Nuestra primera inclinación debe ser comparar: en este caso, compararamos ventanas históricas con nuestra nueva serie: # usamos datos simulados para este ejemplo set.seed(8812) historicos &lt;- simular_serie(2000) ¿Vemos algo diferente en los datos nuevos (el panel de color diferente)? Indpendientemente de la respuesta, vemos que hacer este análisis de manera tan simple no es siempre útil: seguramente podemos encontrar maneras en que la nueva muestra (4) es diferente a muestras históricas. Por ejemplo, ninguna de muestras tiene un “forma de montaña” tan clara. Nos preguntamos si no estamos sobreinterpretando variaciones que son parte normal del proceso. Podemos hacer un mejor análisis si extraemos varias muestras del comportamiento usual del sistema, graficamos junto a la nueva muestra, y revolvemos las gráficas para que no sepamos cuál es cuál. Entonces la pregunta es: ¿Podemos detectar donde están los datos nuevos? Esta se llama una prueba de lineup, o una prueba de ronda de sospechosos (Wickham et al. (2010)). En la siguiente gráfica, en uno de los páneles están los datos recientemente observados. ¿Hay algo en los datos que distinga al patrón nuevo? # nuevos datos obs &lt;- simular_serie(500, x_inicial = last(obs$obs)) # muestrear datos históricos prueba_tbl &lt;- muestrear_ventanas(historicos, obs[1:20, ], n_ventana = 20) # gráfica de pequeños múltiplos ggplot(prueba_tbl$lineup, aes(x = t_0, y = obs)) + geom_line() + facet_wrap(~rep, nrow = 4) + scale_y_log10() ¿Cuáles son los datos nuevos (solo hay un panel con los nuevos datos)? ¿Qué implica que la gráfica que escogamos como “más diferente” no sean los datos nuevos? ¿Qué implica que le “atinemos” a la gráfica de los datos nuevos? Ahora observamos al sistema en otro momento y repetimos la comparación. En el siguiente caso obtenemos: Aunque es imposible estar seguros de que ha ocurrido un cambio, la diferencia de una de las series es muy considerable. Si identificamos los datos correctos, la probabilidad de que hayamos señalado la nueva serie “sobreinterpretando” fluctuaciones en un proceso que sigue comportándose normalente es 0.05 - relativamente baja. Detectar los datos diferentes es evidencia en contra de que el sistema sigue funcionando de la misma manera que antes. Observaciones y terminología: Llamamos hipótesis nula a la hipótesis de que los nuevos datos son producidos bajo las mismas condiciones que los datos de control o de referencia. Si no escogemos la gráfica de los nuevos datos, nuestra conclusión es que la prueba no aporta evidencia en contra de la hipótesis nula. Si escogemos la gráfica correcta, nuestra conclusión es que la prueba aporta evidencia en contra de la hipótesis nula. ¿Qué tan fuerte es la evidencia, en caso de que descubrimos los datos no nulos? Cuando el número de paneles es más grande y detectamos los datos, la evidencia es más alta en contra de la nula. Decimos que el nivel de significancia de la prueba es la probabilidad de seleccionar a los datos correctos cuando la hipótesis nula es cierta (el sistema no ha cambiado). En el caso de 20 paneles, la significancia es de 1/20 = 0.05. Cuando detectamos los datos nuevos, niveles de significancia más bajos implican más evidencia en contra de la nula. Si acertamos, y la diferencia es más notoria y fue muy fácil detectar la gráfica diferente (pues sus diferencias son más extremas), esto también sugiere más evidencia en contra de la hipótesis nula. Finalmente, esta prueba rara vez (o nunca) nos da seguridad completa acerca de ninguna conclusión, aún cuando hiciéramos muchos páneles. Comparando distribuciones Ahora intentamos un ejemplo más típico. Supongamos tenemos muestras para tres grupos a, b y c, que quiere decir que dentro de cada grupo, el proceso e selección de los elementos se hace de manera al azar y de manera simétrica (por ejemplo cada elemento tiene a misma probabiidad de ser seleccionado, y las extracciones se hacen de manera independiente.) Queremos comparar las distribuciones de los datos obtenidos para cada grupo. Quizá la pregunta detrás de esta comparación es: el grupo de clientes b recibió una promoción especial. ¿Están gastando más? La medición que comparamos es el gasto de los clientes. En la muestra observamos diferencias entre los grupos. Pero notamos adicionalmente que hay mucha variación dentro de cada grupo. Nos podríamos preguntar entonces si las diferencias que observamos se deben variación muestral, por ejemplo. Podemos construir ahora una hipótesis nula, que establece que las observaciones provienen de una población similar: Las tres poblaciones (a, b, c) son prácticamente indistiguibles. En este caso, la variación que observamos se debería a que tenemos información incompleta. Como en el ejemplo anterior necesitamos construir o obtener una distribución de referencia para comparar qué tan extremos o diferentes son los datos que observamos. Esa distribución de referencia debería estar basada en el supuesto de que los grupos producen datos de distribuciones similares. Si tuvieramos mediciones similares históricas de estos tres grupos, quizá podríamos extraer datos de referencia y comparar, como hicimos en el ejempo anterior. Pero esto es menos común en este tipo de ejemplos. Permutaciones y el lineup Para abordar este problema podemos pensar en usar permutaciones de los grupos de la siguiente forma (Box et al. (1978), T. C. Hesterberg (2015)): Si los grupos producen datos bajo procesos idénticos, entonces los grupos a, b, c solo son etiquetas que no contienen información. Podríamos permutar al azar las etiquetas y observar nuevamente la gráfica de caja y brazos por grupos. Si la hipótesis nula es cierta (grupos idénticos), esta es una muestra tan verosímil como la que obtuvimos. Así que podemos construir datos de referencia permutando las etiquetas de los grupos al azar, y observando la variación que ocurre. Si la hipótesis nula es cercana a ser cierta, no deberíamos de poder distinguir fácilmente los datos observados de los producidos con las permutaciones al azar. Vamos a intentar esto, por ejemplo usando una gráfica de cuantiles simplificada. Hacemos un lineup, o una rueda de sospechosos (usamos el paquete Wickham, Chowdhury, and Cook (2012), ver Wickham et al. (2010)), donde 19 de los acusados son generados mediante permutaciones al azar de la variable del grupo, y el culpable (los verdaderos datos) están en una posición escogida al azar. ¿Podemos identificar los datos verdaderos? Para evitar sesgarnos, también ocultamos la etiqueta verdadera Usamos una gráfica que muestra los cuantes 0.10, 0.50, 0.90: set.seed(88) reps &lt;- lineup(null_permute(&quot;grupo&quot;), muestra_tab, n = 20) reps_mezcla &lt;- reps %&gt;% mutate(grupo_1 = factor(digest::digest2int(grupo) %% 177)) grafica_cuantiles(reps_mezcla, grupo_1, x) + facet_wrap(~.sample, ncol = 5) + ylab(&quot;x&quot;) + labs(caption = &quot;Mediana y percentiles 10% y 90%&quot;)+ geom_point(aes(colour = grupo_1)) Y la pregunta que hacemos es podemos distinguir nuestra muestra entre todas las replicaciones producidas con permutaciones? ¿Dónde están los datos observados? Según tu elección, ¿qué tan diferentes son los datos observados de los datos nulos? En este ejemplo, es difícil indicar cuáles son los datos. Los grupos tienen distribuciones similares y es factible que las diferencias que observamos se deban a variación muestral. Si la persona escoge los verdaderos datos, encontramos evidencia en contra de la hipótesis nula (los tres grupos son equivalentes). En algunos contextos, se dice que los datos son significativamente diferentes al nivel 0.05. Esto es evidencia en contra de que los datos se producen de manera homogénea, independientemente del grupo. Si la persona escoge uno de los datos permutados, no encontramos evidencia en contra de que los tres grupos producen datos con distribuciones similares. Comparaciones con lineup 2 Repitimos el ejemplo para otra muestra (en este ejemplo el proceso generador de datos es diferente para el grupo b): Hacemos primero la prueba del lineup: set.seed(121) reps &lt;- lineup(null_permute(&quot;grupo&quot;), muestra_tab, n = 20) grafica_cuantiles(reps %&gt;% mutate(grupo_escondido = factor(digest::digest2int(grupo) %% 177)), grupo_escondido, x) + facet_wrap(~.sample) + ylab(&quot;x&quot;) + coord_flip() + geom_point(aes(colour = grupo_escondido)) Podemos distinguir más o menos claramente que está localizada en valores más altos y tiene mayor dispersión. En este caso, como en general podemos identificar los datos, obtenemos evidencia en contra de que los tres grupos tienen distribuciones iguales. Estos ejemplos siguen la idea de inferencia visual propuestas en Wickham et al. (2010), Hofmann et al. (2012) e implementadas en R en el paquete lineup. Son pruebas muy flexibles y estadísticamente rigurosas. Prueba de permutaciones para proporciones Veremos otro ejemplo donde podemos hacer más concreta la idea de distribución nula o de referencia usando pruebas de permutaciones. Supongamos que con nuestra muestra de tomadores de té, queremos probar la siguiente hipótesis nula: Los tomadores de té en bolsas exclusivamente usan azúcar más a tasas simillares que los tomadores de té suelto (que pueden o no también tomar té en bolsita). Los datos que obtuvimos en nuestra encuesta, en conteos, son: te_azucar &lt;- tea %&gt;% select(how, sugar) %&gt;% mutate(how = ifelse(how == &quot;tea bag&quot;, &quot;bolsa_exclusivo&quot;, &quot;suelto o bolsa&quot;)) te_azucar %&gt;% count(how, sugar) %&gt;% pivot_wider(names_from = how, values_from = n) %&gt;% formatear_tabla() sugar bolsa_exclusivo suelto o bolsa No.sugar 81 74 sugar 89 56 Y en proporciones tenemos que: how prop_azucar n bolsa_exclusivo 0.52 170 suelto o bolsa 0.43 130 Pero distintas muestras podrían haber dado distintos resultados. Nos preguntamos qué tan fuerte es la evidencia en contra de que en realidad los dos grupos de personas usan azúcar en proporciones similares, y la diferencia que vemos se puede atribuir a variación muestral. En este ejemplo, podemos usar una estadística de prueba numérica, por ejemplo, la diferencia entre las dos proporciones: \\[p_1 - p_2\\]. (tomadores de en bolsa solamente vs. suelto y bolsa). El proceso sería entonces: La hipótesis nula es que los dos grupos tienen distribuciones iguales, que este caso quiere decir que en la población, tomadores de té solo en bolsa usan azúcar a las mismas tasas que tomadores de suelto o bolsas. Bajo nuestra hipótesis nula (proporciones iguales), producimos una cantidad grande (por ejemplo 10 mil o más) de muestras permutando las etiquetas de los grupos. Evaluamos nuestra estadística de prueba en cada una de las muestras permutadas. El conjunto de valores obtenidos nos da nuestra distribución de referencia (ya no estamos limitados a 20 replicaciones como en las pruebas gráficas). Y la pregunta clave es: ¿el valor de la estadística en nuestra muestra es extrema en comparación a la distribución de referencia? dif_obs &lt;- te_azucar %&gt;% mutate(usa_azucar = as.numeric(sugar == &quot;sugar&quot;)) %&gt;% group_by(how) %&gt;% summarise(prop_azucar = mean(usa_azucar)) %&gt;% pivot_wider(names_from = how, values_from = prop_azucar) %&gt;% mutate(diferencia_prop = bolsa_exclusivo - `suelto o bolsa`) %&gt;% pull(diferencia_prop) La diferencia observada es: dif_obs %&gt;% round(3) ## [1] 0.093 Ahora construimos nuestra distribución nula o de referencia: reps &lt;- lineup(null_permute(&quot;how&quot;), te_azucar, n = 10000) valores_ref &lt;- reps %&gt;% mutate(usa_azucar = as.numeric(sugar == &quot;sugar&quot;)) %&gt;% group_by(.sample, how) %&gt;% summarise(prop_azucar = mean(usa_azucar)) %&gt;% pivot_wider(names_from = how, values_from = prop_azucar) %&gt;% mutate(diferencia = bolsa_exclusivo - `suelto o bolsa`) Y graficamos nuestros resultados (con un histograma y una gráfica de cuantiles, por ejemplo). la estadística evaluada un cada una de nuestras muestras permutadas: g_1 &lt;- ggplot(valores_ref, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif) + xlab(&quot;f&quot;) + ylab(&quot;diferencia&quot;) + labs(subtitle = &quot;Distribución nula o de referencia&quot;) g_2 &lt;- ggplot(valores_ref, aes(x = diferencia)) + geom_histogram(binwidth = 0.04) + coord_flip() + xlab(&quot;&quot;) + labs(subtitle = &quot; &quot;) g_1 + g_2 Este es el rango de fluctuación usual para nuestra estadística *bajo la hipótesis de que los dos grupos de tomadores de té consumen té a la misma tasa. El valor que obtuvimos en nuestros datos es 0.09, que no es un valor extremo en la distribución de referencia que vimos arriba: esta muestra no aporta mucha evidencia en contra de que los grupos tienen distribuciones similares. Podemos graficar otra vez marcando el valor de referencia: # Función de distribución acumulada (inverso de función de cuantiles) dist_perm &lt;- ecdf(valores_ref$diferencia) # Calculamos el percentil del valor observado percentil_obs &lt;- dist_perm(dif_obs) g_1 &lt;- ggplot(valores_ref, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif) + xlab(&quot;f&quot;) + ylab(&quot;diferencia&quot;) + labs(subtitle = &quot;Distribución nula o de referencia&quot;) + geom_hline(yintercept = dif_obs, colour = &quot;red&quot;) + annotate(&quot;text&quot;, x = 0.3, y = dif_obs - 0.05, label = &quot;diferencia observada&quot;, colour = &quot;red&quot;) g_2 &lt;- ggplot(valores_ref, aes(x = diferencia)) + geom_histogram(binwidth = 0.04) + coord_flip() + xlab(&quot;&quot;) + labs(subtitle = &quot; &quot;) + geom_vline(xintercept = dif_obs, colour = &quot;red&quot;) + annotate(&quot;text&quot;, x = dif_obs, y = 2000, label = percentil_obs,vjust = -0.2, colour = &quot;red&quot;) g_1 + g_2 Y vemos que es un valor algo (pero no muy) extremo en la distribución de referencia que vimos arriba: esta muestra no aporta una gran cantidad de evidencia en contra de que los grupos tienen distribuciones similares, que en este caso significa que los dos grupos usan azúcar a tasas similares. Pruebas de hipótesis tradicionales Comencemos recordando la definición de parámetro y estadística. Definición. Un parámetro es una característica (numérica) de una población o de una distribución de probabilidad. Una estadística es una característica (numérica) de los datos. Cualquier función de un parámetro es también un parámetro, y cualquier función de una estadística es también una estadística. Cuando la estadística se calcula de una muestra aleatoria, es por consiguiente aleatoria y es por tanto una variable aleatoria. Por ejemplo \\(\\mu\\) y \\(\\sigma\\) son parámetros de la distribución normal con función de densidad \\(f(x) = (1/\\sqrt{2\\pi}\\sigma)e^{(x-\\mu)^2/(2\\sigma^2)}\\). La varianza \\(\\sigma^2\\), y el cociente (señal a ruido) \\(\\mu/\\sigma\\) también son parámetros. Si \\(X_1,X_2,...,X_n\\) son una muestra aleatoria, entinces la media \\(\\bar{X}=1/n\\sum X_i\\) es una estadística. Ahora podemos pasar a las definiciones correspondientes a pruebas de hipótesis (o pruebas de significancia). Definición. Denotamos por \\(H_0\\) a la hipótesis nula la cual usualmente tratamos como la afirmación del status quo. La hipótesis alternativa la denotamos por \\(H_1\\) y representa el supuesto que está a prueba y para el cual buscamos evidencia en los datos. Definición. La hipótesis normalmente se plantea en términos de un parámetro (\\(\\theta\\in\\mathbb{R}\\)) o conjunto de parámetros (\\(\\theta\\in\\mathbb{R}^p\\)) de la distribución de interés (por ejemplo media, moda, varianza). Para una hipótesis nula del estilo \\(H_0: \\theta = \\theta_0,\\) la hipótesis a contrastar se puede denominar como: Hipótesis alternativa de una cola \\(H_1: \\theta \\gt \\theta_0\\) Hipótesis alternativa de dos colas \\(H_1: \\theta \\neq \\theta_0\\) En el ejemplo anterior planteamos hipótesis nula (proporciones iguales) e hipótesis alternativa que la proporción de tomadores de te suelto que usan azúcar en menor, esto corresponde a una hipótesis alternativa a dos colas: \\(H_0: p_1 = p_2\\), y \\(H_1:p_1 &gt; p_2\\). Definición. Una estadística de prueba es una función numérica de los datos cuyo valor determina el resultado de la prueba. La función usualmente es denotada por \\(T(\\bf X)\\) donde \\(\\bf X\\) representa los datos como variable aleatoria. Por ejemplo, \\(T = T(X_1, \\ldots, X_n)\\) si sólo tenemos una muestra, o por \\(T = T(X_1, \\ldots, X_n, Y_1, \\ldots, Y_m)\\) en el caso de tener dos muestras. Al evaluar la prueba para un conjunto de datos dado, \\(x\\), ésta se denomina estadística de prueba observada, \\(t = T(x).\\) La estadística de prueba correspondiente al ejemplo es \\(T = p_1 - p_2\\) Definición. El valor p es la probabilidad de que bajo la hipótesis nula los datos generen un valor tan extremo como la estadística de prueba observada. Por ejemplo, si consideramos la hipótesis nula admite valores grandes, el valor p se calcula como \\(P(T \\geq t).\\) En el ejemplo de tomadores de té lo calculamos usando el percentil donde nuestra observación cae en la distribución generada por las permutación (valor p de una cola). . Podemos calcular, por ejemplo: Valor p de dos colas: Si la hipótesis nula es cierta, ¿cuál es la probabilidad de observar una diferencia tan extrema o más extrema de lo que observamos? Considerando en este caso interpretamos extrema como que cae lejos de donde a mayoría de la distribución se concentra, podemos calcular el valor p como sigue. A partir de el valor observado, consideramos cuál dato es menor: la probabilidad bajo lo hipótesis nula de observar una diferencia mayor de a que observamos, o la probabilidad de observar una diferencia menor a la que observamos. Tomamos el mínimo y multiplicamos por dos (T. C. Hesterberg (2015)): 2 * min(dist_perm(dif_obs), (1 - dist_perm(dif_obs))) ## [1] 0.085 Este valor p se considera como evidencia “moderada” en contra de la hipótesis nula. Valores p más chicos (observaciones más extremas en comparación con la referencia) aportan más evidencia en contra de la hipótesis de que los grupos de tomadores de té , y valores más grandes aportan menos evidencia. Definición. Un resultado es estadisticamente significativo si tiene muy baja probabilidad de suceder al azar. Entre más pequeño requiramos un valor p oara declarar un resultado estadísticamente significativo, somos más conservadores. Las pruebas de hipótesis con frecuencia inician contestando una pregunta más general que los valores p: ¿Cuál es la distribución de la estadística de prueba cuando no hay efecto real? Definición. La distribución nula es la disttibución de la estadística de prueba si la hipótesis nula es cierta. En ocasiones también nos referimos a ella como la distribución de referencia pues estamos comparando la estadística de prueba observada a su referencia para determinar que tan inusual es. En el ejemplo de tomadores de te aproximamos la distribución nula (y los valores p) con simulación; sin embargo, para algunas estadisticas hay métodos exactos. En particular, usamos el método de pruebas de permutación, el algoritmo para dos grupos sería como sigue. Prueba de permutación para dos muestras Supongamos que tenemos m observaciones de una población y n de otra. Combina los m+n valores. Repite: Saca una remuestra de tamaño m sin reemplazo. Usa las n observaciones restantes para obtener la otra muestra. Calcula la estadística de prueba (que compara las muestras). Calcula el valor p como la fracción de las veces que la estadística sobrepasó la estadística observada, multiplica por 2 para una prueba de dos lados. La distribución de la estadística a lo largo de las remuestras de permutación es la distribución de permutación. Ésta puede ser exacta, si se calcula exhaustivamente (cuando tenemos pocas observaciones es posible) o aproximada. Tomadores de té 2 Ahora hacemos una prueba de permutaciones otro par de proporciones con el mismo método. La hipótesis nula ahora es: Los tomadores de té Earl Gray usan azúcar a una tasa similar a los tomadores de té negro Los datos que obtuvimos en nuestra encuesta, en conteos, son: sugar black Earl Grey No.sugar 51 84 sugar 23 109 Y en porcentajes tenemos que: prop_azucar &lt;- te_azucar %&gt;% count(Tea, sugar) %&gt;% group_by(Tea) %&gt;% mutate(prop = 100 * n / sum(n), n = sum(n)) %&gt;% filter(sugar == &quot;sugar&quot;) %&gt;% select(Tea, prop_azucar = prop, n) %&gt;% mutate(&#39;% usa azúcar&#39; = round(prop_azucar)) %&gt;% select(-prop_azucar) prop_azucar %&gt;% formatear_tabla Tea n % usa azúcar black 74 31 Earl Grey 193 56 Pero distintas muestras podrían haber dado distintos resultados. Nos preguntamos que tan fuerte es la evidencia en contra de que en realidad los dos grupos de personas usan azúcar en proporciones similares, y la diferencia que vemos se puede atribuir a variación muestral. Escribimos la función que calcula diferencias para cada muestra: calc_diferencia_2 &lt;- function(datos){ datos %&gt;% mutate(usa_azucar = as.numeric(sugar == &quot;sugar&quot;)) %&gt;% group_by(Tea) %&gt;% summarise(prop_azucar = mean(usa_azucar)) %&gt;% pivot_wider(names_from = Tea, values_from = prop_azucar) %&gt;% mutate(diferencia_prop = `Earl Grey` - black) %&gt;% pull(diferencia_prop) } La diferencia observada es: ## [1] 0.254 Ahora construimos nuestra distribución nula o de referencia: set.seed(2) reps &lt;- lineup(null_permute(&quot;Tea&quot;), te_azucar, n = 10000) valores_ref &lt;- reps %&gt;% group_by(.sample) %&gt;% nest() %&gt;% mutate(diferencia = lapply(data, calc_diferencia_2)) %&gt;% unnest(diferencia) Y podemos graficar la distribución de referencia otra vez marcando el valor observado En este caso, la evidencia es muy fuerte en contra de la hipótesis nula, pues el resultado que obtuvimos es muy extremo en relación a la distribución de referencia. El valor p es cercano a 0. Haz una prueba de permutaciones para diferencia de medias para comparar la propina en cena vs en comidas. * Grafica la distribución de referencia. * Calcula el valor p (dos colas). Pruebas de permutación: Implementación Hasta ahora nos hemos centrado en ejemplos de diferencias en medias. Podemos extender las pruebas de permutación a \\(\\bar{X}\\) (la media de la primera muestra), \\(m\\bar{X}\\) (la suma de las observaciones en la primera muestra), y más. Teorema. En pruebas de permutación, si dos estadísticas de prueba \\(T_1\\) y \\(T_2\\) están relacionadas por una función estríctamente creciente, \\(T_1(X^*)=f(T_2(X^*))\\) donde \\(X^*\\) es una remuestra de permutación de los datos originales, entonces los valores p serán los mismos en las pruebas de permutación. Agregar uno al numerador y denominador. Cuando se calcula el valor p en la implementación de muestreo, agregar uno al numerador y denominador. Esto corresponde a incluir los datos como una remuestra adicional y sirve para evitar reportar el valor p \\(0\\) que es imposible pues siempre hay una remuestra con un valor al menos tan extremo como los datos observados (los datos mismos). Muestras con reemplazo de la Distribución Nula. En la implementación de muestreo, no nos aseguramos que las remuestras sean únicas. Sería más acertado tomar muestras sin reemplazo, sin embargo, el costo computacional es demasiado alto. Entre más muestras más exactitud. Hemos usado 10,000 muestras, en general entre más remuestras tendremos una mejor estimación del valor p. Si el verdadero valor es \\(p\\) el estimado tendrá una varianza aproximadamente de \\(p(1-p)/N\\) donde \\(N\\) es el número de remuestras. Observación. Así como los \\(n\\) datos originales son una muestra de la población, también las \\(N\\) remuestras de la estadística son una muestra de una población, en este caso de la distribución nula. La pruebas de permutaciones son más útiles cuando nuestra hipótesis nula se refiere que la distribución de los grupos son muy similares, o la independencia entre observaciones y grupo. Esto también aplica cuando queremos probar por ejemplo, que una variable numérica Y es independiente de X. Hay algunas hipótesis que no se pueden probar con este método, como por ejemplo, las que se refieren a una sola muestra: ¿los datos son consistentes con que su media es igual a 5? Adicionalmente, en algunas ocasiones queremos probar aspectos más específicos de las diferencias: como ¿son iguales las medias o medianas de dos grupos de datos? ¿Tienen dispersión similar? Las pruebas de permutaciones no están tan perfectamente adaptadas a este problema, pues prueban todos los aspectos de las distribuciones que se comparan, aún cuando escogamos una estadística particular que pretende medir, por ejemplo, diferencia de medias. Eso quiere decir que podemos rechazar igualdad de medias, por ejemplo, cuando en realidad otra característica de las distribuciones es la que difiere mucho en las poblaciones En algunas referencias (ver Chihara and Hesterberg (2018), Efron and Tibshirani (1993)) se argumenta que de todas formas las pruebas de permutaciones son relativamente robustas a esta desadaptación. Un caso excepcional, por ejemplo, es cuando las poblaciones que comparamos resultan tener dispersión extremadamente distinta, y adicionalmente los tamaños de muestra de los grupos son muy desiguales (otra vez, ver ejemplos en Chihara and Hesterberg (2018)). Ejemplo: tiempos de fusión Veamos el siguiente ejemplo, que es un experimento donde se midió el tiempo que tardan distintas personas en fusionar un estereograma para ver una imagen 3D. (Cleveland (1993)). Existen dos condiciones: en una se dio indicaciones de qué figura tenían que buscar (VV) y en otra no se dio esa indicación. ¿Las instrucciones verbales ayudan a fusionar más rápido el estereograma? Una pregunta que podríamos hacer es: considerando que hay mucha variación en el tiempo de fusión dentro de cada tratamiento, necesitamos calificar la evidencia de nuestra conclusión (el tiempo de fusión se reduce con información verbal). Podemos usar una prueba de permutaciones, esta vez justificándola por el hecho de que los tratamientos se asignan al azar: si los tratamientos son indistinguibles, entonces las etiquetas de los grupos son solo etiquetas, y permutarlas daría muestras igualmente verosímiles. En este caso, compararemos gráficas de cuantiles de los datos con los producidos por permutaciones (transformamos los datos pues en este caso es más apropiado una comparación multiplicativa): ¿Podemos identificar los datos? En general, muy frecuentemente las personas identifican los datos correctamente, lo que muestra evidencia considerable de que la instrucción verbal altera los tiempos de respuesta de los partipantes, y en este caso ayuda a reducir el tiempo de fusión de los estereogramas. Ejemplo: tiempos de fusión 2 Podemos usar las pruebas de permutaciones para distintos de tipos de estadísticas: medianas, medias, comparar dispersión usando rangos intercuartiles o varianzas, etc. Regresamos a los tiempos de fusión. Podemos hacer una prueba de permutaciones para la diferencia de las medias o medianas, por ejemplo. En este ejemplo usaremos una medida de centralidad un poco diferente, como ilustración: el promedio de los cuartiles superior e inferior de las dos distribuciones. Usaremos el cociente de estas dos cantidades para medir su diferencia # esta función hace permutaciones y calcula la diferencia para cada una permutaciones_est &lt;- function(datos, variable, calc_diferencia, n = 1000){ # calcular estadística para cada grupo permutar &lt;- function(variable){ sample(variable, length(variable)) } tbl_perms &lt;- tibble(.sample = seq(1, n-1, 1)) %&gt;% mutate(diferencia = map_dbl(.sample, ~ datos %&gt;% mutate({{variable}}:= permutar({{variable}})) %&gt;% calc_diferencia)) bind_rows(tbl_perms, tibble(.sample = n, diferencia = calc_diferencia(datos))) } stat_fusion &lt;- function(x){ (quantile(x, 0.75) + quantile(x, 0.25))/2 } calc_fusion &lt;- function(stat_fusion){ fun &lt;- function(datos){ datos %&gt;% group_by(nv.vv) %&gt;% summarise(est = stat_fusion(time)) %&gt;% pivot_wider(names_from = nv.vv, values_from = est) %&gt;% mutate(dif = VV / NV ) %&gt;% pull(dif) } fun } calc_cociente &lt;- calc_fusion(stat_fusion) dif_obs &lt;- calc_cociente(fusion) # permutar valores_ref &lt;- permutaciones_est(fusion, nv.vv, calc_cociente, n = 10000) dist_perm_nv &lt;- ecdf(valores_ref$diferencia) cuantil_obs &lt;- dist_perm_nv(dif_obs) Y el valor p de dos colas es dist_perm_nv &lt;- ecdf(valores_ref$diferencia) 2 * min(dist_perm_nv(dif_obs), 1 - dist_perm_nv(dif_obs)) ## [1] 0.0354 Lo que muestra evidencia considerable, aunque no muy fuerte, de que la instrucción verbal ayuda a reducir el tiempo de fusión de los estereogramas: la caja del diagrama de caja y brazos para el grupo VV está encogida por un factor menor a 1. Separación de grupos Este ejemplo tomado de Chowdhury et al. (2015) (tanto la idea como el código). La pregunta que se aborda en ese estudio es: Existen métodos de clasificación (supervisados o no supervisados) para formar grupos en términos de variables que describen a los individuos Estos métodos (análisis discriminante, o k-means, por ejemplo), pretenden formar grupos compactos, bien separados entre ellos. Cuando aplicamos el método, obtenemos clasificadores basados en las variables de entrada. La pregunta es: ¿los grupos resultantes son producto de patrones que se generalizan a la población, o capitalizaron en variación aleatoria para formarse? Especialmente cuando tenemos muchas mediciones de los individuos, y una muestra relativamente chica, Es relativamente fácil encontrar combinaciones de variables que separan los grupos, aunque estas combinaciones y diferencias están basadas en ruido y no generalizan a la población. Como muestran en Chowdhury et al. (2015), el lineup es útil para juzgar si tenemos evidencia en contra de que los grupos en realidad son iguales, y usamos variación muestral para separarlos. Avispas (opcional) En el siguiente ejemplo, tenemos 4 grupos de avispas (50 individuos en total), y para cada individuo se miden expresiones de 42 genes distintos. La pregunta es: ¿Podemos separar a los grupos de avispas dependiendo de sus mediciones? En este se usó análisis discriminante para buscar proyecciones de los datos en dimensión baja de forma que los grupos sean lo más compactos y separados posibles. Para probar qué tan bien funciona este método, podemos hacer una prueba de permutación, aplicamos LDA y observamos los resultados. Y vemos que incluso permutando los grupos, es generalmente posible separarlos en grupos bien definidos: la búsqueda es suficientemente agresiva para encontrar combinaciones lineales que los separan. Que no podamos distinguir los datos verdaderos de las replicaciones nulas indica que este método difícilmente puede servir para separar los grupos claramente. Otro enfoque sería separar los datos en una muestra de entrenamiento y una de prueba (que discutiremos en la última sesión). Aplicamos el procedimiento a la muestra de entrenamiento y luego vemos qué pasa con los datos de prueba: set.seed(8) wasps_1 &lt;- wasps %&gt;% mutate(u = runif(nrow(wasps), 0, 1)) wasps_entrena &lt;- wasps_1 %&gt;% filter(u &lt;= 0.8) wasps_prueba &lt;- wasps_1 %&gt;% filter(u &gt; 0.8) wasp.lda &lt;- MASS::lda(Group ~ ., data=wasps_entrena[,-1]) wasp_ld_entrena &lt;- predict(wasp.lda, dimen=2)$x %&gt;% as_tibble(.name_repair = &quot;universal&quot;) %&gt;% mutate(tipo = &quot;entrenamiento&quot;) %&gt;% mutate(grupo = wasps_entrena$Group) wasp_ld_prueba &lt;- predict(wasp.lda, newdata = wasps_prueba, dimen=2)$x %&gt;% as_tibble(.name_repair = &quot;universal&quot;) %&gt;% mutate(tipo = &quot;prueba&quot;)%&gt;% mutate(grupo = wasps_prueba$Group) wasp_lda &lt;- bind_rows(wasp_ld_entrena, wasp_ld_prueba) ggplot(wasp_lda, aes(x = LD1, y = LD2, colour = grupo)) + geom_point(size = 3) + facet_wrap(~tipo) Aunque esta separación de datos es menos efectiva en este ejemplo por la muestra chica, podemos ver que la separación lograda en los datos de entrenamiento probablemente se debe a variación muestral. La “crisis de replicabilidad” Recientemente (Ioannidis (2005)) se ha reconocido en campos como la sicología la crisis de replicabilidad. Varios estudios que recibieron mucha publicidad inicialmente no han podido ser replicados posteriormente por otros investigadores. Por ejemplo: Hacer poses poderosas produce cambios fisiológicos que mejoran nuestro desempeño en ciertas tareas Mostrar palabras relacionadas con “viejo” hacen que las personas caminen más lento (efectos de priming) En todos estos casos, el argumento de la evidencia de estos efectos fue respaldada por una prueba de hipótesis nula con un valor p menor a 0.05. La razón es que ese es el estándar de publicación seguido por varias áreas y revistas. La tasa de no replicabilidad parece ser mucho más alta (al menos la mitad o más según algunas fuentes, como la señalada arriba) que lo sugeriría la tasa de falsos positivos (menos de 5%) Este problema de replicabilidad parece ser más frecuente cuando: Se trata de estudios de potencia baja: mediciones ruidosas y tamaños de muestra chicos. El plan de análisis no está claramente definido desde un principio (lo cual es difícil cuando se están investigando “fenómenos no estudiados antes”) ¿A qué se atribuye esta crisis de replicabilidad? El jardín de los senderos que se bifurcan Aunque haya algunos ejemplos de manipulaciones conscientes –e incluso, en menos casos, malintencionadas– para obtener resultados publicables o significativos (p-hacking), como vimos en ejemplos anteriores, hay varias decisiones, todas razonables, que podemos tomar cuando estamos buscando las comparaciones correctas. Algunas pueden ser: Transformar los datos (tomar o no logaritmos, u otra transformación) Editar datos atípicos (razonable si los equipos pueden fallar, o hay errores de captura, por ejemplo) Distintas maneras de interpretar los criterios de inclusión de un estudio (por ejemplo, algunos participantes mostraron tener gripa, o revelaron que durmieron muy poco la noche anterior, etc. ¿los dejamos o los quitamos?) Dado un conjunto de datos, las justificaciones de las decisiones que se toman en cada paso son razonables, pero con datos distintos las decisiones podrían ser diferentes. Este es el jardín de los senderos que se bifurcan Gelman, que invalida en parte el uso valores p como criterio de evidencia contra la hipótesis nula. Esto es exacerbado por: Tamaños de muestra chicos y efectos “inestables” que se quieren medir (por ejemplo en sicología) El hecho de que el criterio de publicación es obtener un valor p &lt; 0.05, y la presión fuerte sobre los investigadores para producir resultados publicables (p &lt; 0.05) El que estudios o resultados similares que no obtuvieron valores \\(p\\) por debajo del umbral no son publicados o reportados. Ver por ejemplo el comunicado de la ASA. Ojo: esas presiones de publicación no sólo ocurre para investigadores en sicología. Cuando trabajamos en problemas de análisis de datos en problemas que son de importancia, es común que existan intereses de algunas partes o personas involucradas por algunos resultados u otros (por ejemplo, nuestros clientes de consultoría o clientes internos). Eso puede dañar nuestro trabajo como analistas, y el avance de nuestro equipo. Aunque esas presiones son inevitables, se vuelven manejables cuando hay una relación de confianza entre las partes involucradas. Ejemplo: decisiones de análisis y valores p En el ejemplo de datos de fusión, decidimos probar, por ejemplo, el promedio de los cuartiles inferior y superior, lo cual no es una decisión típica pero usamos como ilustración. Ahora intentamos usar distintas mediciones de la diferencia entre los grupos, usando distintas medidas resumen y transformaciones (por ejemplo, con o sin logaritmo). Aquí hay unas 12 combinaciones distintas para hacer el análisis (multiplicadas por criterios de “aceptación de datos en la muestra”, que simulamos tomando una submuestra al azar): # otra manera de calcular las permutaciones calc_fusion &lt;- function(stat_fusion, trans, comparacion){ fun &lt;- function(datos){ datos %&gt;% group_by(nv.vv) %&gt;% summarise(est = stat_fusion({{ trans }}(time))) %&gt;% pivot_wider(names_from = nv.vv, values_from = est) %&gt;% mutate(dif = {{ comparacion }}) %&gt;% pull(dif) } fun } valor_p &lt;- function(datos, variable, calc_diferencia, n = 1000){ # calcular estadística para cada grupo permutar &lt;- function(variable){ sample(variable, length(variable)) } tbl_perms &lt;- tibble(.sample = seq(1, n-1, 1)) %&gt;% mutate(diferencia = map_dbl(.sample, ~ datos %&gt;% mutate({{variable}} := permutar({{variable}})) %&gt;% calc_diferencia)) perms &lt;- bind_rows(tbl_perms, tibble(.sample = n, diferencia = calc_diferencia(datos))) perms_ecdf &lt;- ecdf(perms$diferencia) dif &lt;- calc_diferencia(datos) 2 * min(perms_ecdf(dif), 1- perms_ecdf(dif)) } set.seed(7272) media_cuartiles &lt;- function(x){ (quantile(x, 0.75) + quantile(x, 0.25))/2 } # nota: usar n=10000 o más, esto solo es para demostración: ejemplo &lt;- list() calc_dif &lt;- calc_fusion(mean, identity, VV - NV) ejemplo$media_dif &lt;- valor_p(fusion %&gt;% sample_frac(0.95), nv.vv, calc_dif, n = 10000) calc_dif &lt;- calc_fusion(mean, log, VV - NV) ejemplo$media_dif_log &lt;- valor_p(fusion %&gt;% sample_frac(0.95), nv.vv, calc_dif, n = 10000) calc_dif &lt;- calc_fusion(median, identity, VV / NV) ejemplo$mediana_razon &lt;- valor_p(fusion %&gt;% sample_frac(0.95), nv.vv, calc_dif, n = 10000) calc_dif &lt;- calc_fusion(media_cuartiles, identity, VV / NV) ejemplo$cuartiles_razon &lt;- valor_p(fusion %&gt;% sample_frac(0.95), nv.vv, calc_dif, n = 10000) ejemplo &lt;- read_rds(&quot;cache/ejemplo_p_val.rds&quot;) ejemplo$media_dif ## [1] 0.0658 ejemplo$media_dif_log ## [1] 0.018 ejemplo$mediana_razon ## [1] 0.049 ejemplo$cuartiles_razon ## [1] 0.0464 Si existen grados de libertad - muchas veces necesarios para hacer un análisis exitoso-, entonces los valores p pueden tener poco significado. Alternativas o soluciones El primer punto importante es reconocer que la mayor parte de nuestro trabajo es exploratorio (recordemos el proceso complicado del análisis de datos de refinamiento de preguntas). En este tipo de trabajo, reportar valores p puede tener poco sentido, y mucho menos tiene sentido aceptar algo “verdadero” cuando pasa un umbral de significancia dado. Nuestro interés principal al hacer análisis es expresar correctamente y de manera útil la incertidumbre asociada a las conclusiones o patrones que mostramos (asociada a variación muestral, por ejemplo) para que el proceso de toma de decisiones sea informado. Un resumen de un número (valor p, o el que sea) no puede ser tomado como criterio para tomar una decisión que generalmente es compleja. En la siguiente sección veremos cómo podemos mostrar parte de esa incertidumbre de manera más útil. Por otra parte, los estudios confirmatorios (donde se reportan valores p) también tienen un lugar. En áreas como la sicología, existen ahora movimientos fuertes en favor de la repetición de estudios prometedores pero donde hay sospecha de grados de libertad del investigador. Este movimiento sugiere dar valor a los estudios exploratorios que no reportan valor p, y posteriormente, si el estudio es de interés, puede intentarse una replicación confirmatoria, con potencia más alta y con planes de análisis predefinidos. Referencias "],
["estimación-y-distribución-de-muestreo.html", "Sección 5 Estimación y distribución de muestreo Ejemplo: precios de casas Distribución de muestreo Más ejemplos El error estándar Calculando la distribución de muestreo Teorema central del límite", " Sección 5 Estimación y distribución de muestreo En esta sección discutiremos cuál el objetivo general del proceso de estimación. y cómo entender y manejar la variabilidad que se produce cuando aleatorizamos la selección de las muestras que utilizamos para hacer análisis. Ejemplo: precios de casas Supongamos que queremos conocer el valor total de las casas que se vendieron recientemente en una zona particular. Supondremos que tenemos un listado de las casas que se han vendido recientemente, pero en ese listado no se encuentra el precio de venta. Decidimos entonces tomar una muestra aleatoria de 100 de esas casas. Para esas casas hacemos trabajo de campo para averiguar el precio de venta. marco_casas &lt;- read_csv(&quot;data/casas.csv&quot;) set.seed(841) muestra_casas &lt;- sample_n(marco_casas, 100) %&gt;% select(id, nombre_zona, area_habitable_sup_m2, precio_miles) sprintf(&quot;Hay %0.0f casas en total, tomamos muestra de %0.0f&quot;, nrow(marco_casas), nrow(muestra_casas)) ## [1] &quot;Hay 1144 casas en total, tomamos muestra de 100&quot; head(muestra_casas) ## # A tibble: 6 x 4 ## id nombre_zona area_habitable_sup_m2 precio_miles ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 287 NAmes 161. 159 ## 2 755 NAmes 95.3 156 ## 3 1190 Gilbert 168. 189 ## 4 36 NridgHt 228. 309 ## 5 32 Sawyer 114. 149. ## 6 538 NAmes 80.3 111. Como tomamos una muestra aleatoria, intentamos estimar el valor total de las casas que se vendieron expandiendo el total muestral, es decir nuestro estimador \\(\\hat{t} = t(X_1,\\ldots X_{100})\\) del total poblacional \\(t\\) es \\[\\hat{t} = \\frac{N}{n} \\sum_{i=1}^{100} X_i = N\\bar{x}\\] Esta función implementa el estimador: n &lt;- nrow(muestra_casas) # tamaño muestra N &lt;- nrow(marco_casas) # tamaño población estimar_total &lt;- function(muestra_casas, N){ total_muestral &lt;- sum(muestra_casas$precio_miles) n &lt;- nrow(muestra_casas) # cada unidad de la muestra representa a N/n f_exp &lt;- N / n # estimador total es la expansión del total muestral estimador_total &lt;- f_exp * total_muestral res &lt;- tibble(total_muestra = total_muestral, factor_exp = f_exp, est_total_millones = estimador_total / 1000) res } estimar_total(muestra_casas, N) %&gt;% mutate(across(where(is.numeric), round, 2)) ## # A tibble: 1 x 3 ## total_muestra factor_exp est_total_millones ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18444. 11.4 211 Sin embargo, si hubiéramos obtenido otra muestra, hubiéramos obtenido otra estimación diferente. Por ejemplo: estimar_total(sample_n(marco_casas, 100), N) %&gt;% mutate(across(where(is.numeric), round, 2)) ## # A tibble: 1 x 3 ## total_muestra factor_exp est_total_millones ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 17916. 11.4 205. El valor poblacional que buscamos estimar (nótese que en la práctica este no lo conocemos) es: # multiplicar por 1000 para que sea en millones de dólares total_pob &lt;- sum(marco_casas %&gt;% pull(precio_miles)) / 1000 total_pob ## [1] 209.7633 Así que: Para algunas muestras esta estadística puede estar muy cercana al valor poblacional, pero para otras puede estar más lejana. Para entender qué tan buena es una estimación particular, entonces, tenemos que entender cuánta variabilidad hay de muestra a muestra debida a la aleatorización. Esto depende del diseño de la muestra y de la población de precios de casas (que no conocemos). Distribución de muestreo La distribución de muestreo de una estadística enumera los posibles resultados que puede tomar esa estadística sobre todas las muestras posibles. Este es el concepto básico para poder entender qué tan bien o mal estima un parámetro poblacional dado. En nuestro ejemplo anterior de precio de casas, no podemos calcular todas las posibles estimaciones bajo todas las posibles muestras, pero podemos aproximar repitiendo una gran cantidad de veces el proceso de muestreo, como hicimos al aproximar la distribución de permutaciones de estadísticas de prueba de las secciones anteriores. Empezamos repitiendo 10 veces y examinamos cómo varía nuestra estadística: replicar_muestreo &lt;- function(marco_casas, m = 500, n){ # n es el tamaño de muestra que se saca de marco_casas # m es el número de veces que repetimos el muestro de tamaño n resultados &lt;- map(1:m, function(id) { sample_n(marco_casas, n) %&gt;% estimar_total(N) %&gt;% mutate(id_muestra = id) %&gt;% select(id_muestra, everything()) }) resultados %&gt;% bind_rows } replicar_muestreo(marco_casas, m = 10, n = 100) %&gt;% mutate(across(where(is.numeric), round, 1)) %&gt;% formatear_tabla() id_muestra total_muestra factor_exp est_total_millones 1 17594.8 11.4 201.3 2 17423.9 11.4 199.3 3 18444.3 11.4 211.0 4 17696.6 11.4 202.4 5 17275.8 11.4 197.6 6 17867.6 11.4 204.4 7 18450.8 11.4 211.1 8 18187.2 11.4 208.1 9 18604.2 11.4 212.8 10 19144.4 11.4 219.0 Como vemos, hay variación considerable en nuestro estimador del total, pero la estimación que haríamos con cualquiera de estas muestras no es muy mala. Ahora examinamos un número más grande de simulaciones: replicaciones_1 &lt;- replicar_muestreo(marco_casas, m = 1500, n = 100) Y el siguiente histograma nos dice qué podemos esperar de la variación de nuestras estimadores, y donde es más probable que una estimación particular caiga: graf_1 &lt;- ggplot(replicaciones_1, aes(x = est_total_millones)) + geom_histogram() + geom_vline(xintercept = total_pob, colour = &quot;red&quot;) + xlab(&quot;Millones de dólares&quot;) + scale_x_continuous(breaks = seq(180, 240, 10), limits = c(180, 240)) graf_1 Con muy alta probabilidad el error no será de más de unos 30 millones de dólares (o no más de 20% del valor poblacional). Definición Sea \\(X_1, X_2, \\ldots X_n\\) una muestra, y \\(T = t(X_1, X_2, \\ldots, X_n)\\) una estadística. La distribución de muestreo de \\(T\\) es la función de distribución de \\(T\\). Esta distribución es sobre todas las posibles muestras que se pueden obtener. Cuando usamos \\(T\\) para estimar algún parámetro poblacional \\(\\theta\\), decimos informalmente que el estimador es preciso si su distribución de muestreo está muy concentrada alrededor del valor \\(\\theta\\) que queremos estimar. Si la distribución de muestreo está concentrada en un conjunto muy grande o muy disperso, quiere decir que con alta probabilidad cuando obtengamos nuestra muestra y calculemos nuestra estimación, el resultado estará lejano del valor poblacional que nos interesa estimar. Veamos qué pasa cuando hacemos la muestra más grande en nuestro ejemplo: replicaciones_2 &lt;- replicar_muestreo(marco_casas, m = 1500, n = 250) Graficamos las dos distribuciones de muestreo juntas, y vemos cómo con mayor muestra obtenemos un estimador más preciso, y sin considerar el costo, preferimos el estimador mejor concentrado alrededor del valor que buscamos estimar. library(patchwork) graf_2 &lt;- ggplot(replicaciones_2, aes(x = est_total_millones)) + geom_histogram() + geom_vline(xintercept = total_pob, colour = &quot;red&quot;) + xlab(&quot;Millones de dólares&quot;) + scale_x_continuous(breaks = seq(180, 240, 10), limits = c(180, 240)) graf_1 + graf_2 Observación: a veces este concepto se confunde la distribución poblacional de las \\(X_i\\). Esto es muy diferente. Por ejemplo, en nuestro caso, el histograma de la distribución de valores poblacionales es ggplot(marco_casas, aes(x = precio_miles)) + geom_histogram() que en general no tiene ver mucho en escala o forma con la distribución de muestreo de nuestro estimador del total. Más ejemplos Podemos también considerar muestrear de poblaciones sintéticas o modelos probabilísticos que usamos para modelar poblaciones reales. Por ejemplo, supongamos que tomamos una muestra de tamaño 15 de la distribución uniforme en \\([0,1]\\). Es decir, cada \\(X_i\\) es un valor uniformemente distribuido en \\([0,1]\\), y las \\(X_i\\) se extraen independientemente unas de otras. Consideramos dos estadísticas de interés: La media muestral \\(T_1(X) = \\frac{1}{15}\\sum_{i = 1}^{15} X_i\\) El cuantil 0.75 de la muestra \\(T_2(X) = q_{0.75}(X)\\) ¿Cómo crees que se vean las distribuciones muestrales de estas estadísticas? ¿Alrededor de qué valores crees que concentren? ¿Crees que tendrán mucha o poca dispersión? ¿Qué forma crees que tengan? Para el primer caso hacemos: # simular replicar_muestreo_unif &lt;- function(est = estimador_1, m, n = 15){ valores_est &lt;- map_dbl(1:m, ~est(runif(n))) tibble(id_muestra = 1:m, estimacion = valores_est) } sim_estimador_1 &lt;- replicar_muestreo_unif(mean, 4000, 15) # graficar aprox de distribución de muestreo ggplot(sim_estimador_1, aes(x = estimacion)) + geom_histogram(bins = 40) + xlim(c(0, 1)) # simular para el máximo cuantil_75 &lt;- function(x) quantile(x, 0.75) sim_estimador_2 &lt;- replicar_muestreo_unif(cuantil_75, 4000, 15) # graficar distribución de muestreo ggplot(sim_estimador_2, aes(x = estimacion)) + geom_histogram(breaks = seq(0, 1, 0.02)) + xlim(c(0, 1)) Supón que tenemos una muestra de 30 observaciones de una distribución uniforme \\([0,b]\\). ¿Qué tan buen estimador de \\(b/2\\) es la media muestral? ¿Cómo lo cuantificarías? ¿Qué tan buen estimador del cuantil 0.8 de la distribución uniforme es el cuantil 0.8 muestral? ¿Qué desventajas notas en este estimador? El error estándar Una primera medida útil de la dispersión de la distribución de muestreo es su desviación estándar: la razón específica tiene qué ver con un resultado importante, el teorema central del límite, que veremos más adelante. En este caso particular, a esta desviación estándar se le llama error estándar: Definición A la desviación estándar de una estadística \\(T\\) le llamamos su error estándar, y la denotamos por \\(\\text{ee}(T)\\). A cualquier estimador de este error estándar lo denotamos como \\(\\hat{\\text{ee}}(T)\\). Este error estándar mide qué tanto varía el estimador \\(T\\) de muestra a muestra. Observación: es importante no confundir el error estándar con la desviación estándar de una muestra (o de la población). En nuestro ejemplo de las uniformes, la desviación estándar de las muestras varía como: map_dbl(1:10000, ~ sd(runif(15))) %&gt;% quantile %&gt;% round(2) ## 0% 25% 50% 75% 100% ## 0.11 0.26 0.29 0.31 0.41 Mientras que el error estándar de la media es aproximadamente map_dbl(1:10000, ~ mean(runif(15))) %&gt;% sd ## [1] 0.07439575 y el error estándar del máximo es aproximadamente map_dbl(1:10000, ~ max(runif(15))) %&gt;% sd ## [1] 0.05928675 Como ejercicio para contrastar estos conceptos, puedes considerar: ¿Qué pasa con la desviación estándar de una muestra muy grande de uniformes? ¿Qué pasa con el error estándar de la media muestral de una muestra muy grande de uniformes? Ejemplo: valor de casas Consideramos el error estándar del estimador del total del inventario vendido, usando una muestra de 250 con el estimador del total que consideramos. Como aproximamos con simulación la distribución de muestreo, podemos hacer: ee_2 &lt;- replicaciones_2 %&gt;% pull(est_total_millones) %&gt;% sd round(ee_2, 1) ## [1] 5.2 que está en millones de pesos y cuantifica la dispersión de la distribución de muestreo del estimador del total. Para tamaño de muestra 100, obtenemos más dispersión: ee_1 &lt;- replicaciones_1 %&gt;% pull(est_total_millones) %&gt;% sd round(ee_1, 1) ## [1] 8.9 Nótese que esto es muy diferente, por ejemplo, a la desviación estándar poblacional o de una muestra. Estas dos cantidades miden la variabilidad del estimador del total. Calculando la distribución de muestreo En los ejemplos anteriores usamos simulación para obtener aproximaciones de la distribución de muestreo de algunos estimadores. También es posible: Hacer cálculos exactos a partir de modelos probabilísticos. Hacer aproximaciones asintóticas para muestras grandes (de las cuales la más importante es la que da el teorema central del límite). En los ejemplos de arriba, cuando muestreamos de la poblaciones, extrajimos las muestras de manera aproximadamente independiente. Cada observación \\(X_i\\) tiene la misma distribución y las \\(X_i\\)’s son independientes. Este tipo de diseños aleatorizados es de los más simples, y se llama muestreo aleatorio simple. En general, en esta parte haremos siempre este supuesto: Una muestra es iid (independiente e idénticamente distribuida) si es es un conjunto de observaciones \\(X_1,X_2, \\ldots X_n\\) independientes, y cada una con la misma distribución. En términos de poblaciones, esto lo logramos obteniendo cada observación de manera aleatoria con el mismo procedimiento. En términos de modelos probabilísticos, cada \\(X_i\\) se extrae de la misma distribución fija \\(F\\) (que pensamos como la “población”) de manera independiente. Esto lo denotamos por \\(X_i \\overset{iid}{\\sim} F.\\) Ejemplo Si \\(X_1, X_2, \\ldots X_n\\) es una muestra de uniformes independientes en \\([0,1]\\), ¿cómo calcularíamos la distribución de muestreo del máximo muestra \\(T_2 = \\max\\)? En este caso, es fácil calcular su función de distribución acumulada de manera exacta: \\[F_{\\max}(x) = P(\\max\\{X_1,X_2,\\ldots X_n\\} \\leq x)\\] El máximo es menor o igual a \\(x\\) si y sólo si todas las \\(X_i\\) son menores o iguales a \\(x\\), así que \\[F_\\max (x) = P(X_1\\leq x, X_2\\leq x, \\cdots, X_n\\leq x)\\] como las \\(X_i\\)’s son independientes entonces \\[F_\\max(x) = P(X_1\\leq x)P(X_2\\leq x)\\cdots P(X_n\\leq x) = x^n\\] para \\(x\\in [0,1]\\), pues para cada \\(X_i\\) tenemos \\(P(X_i\\leq x) = x\\) (demuéstralo). Así que no es necesario usar simulación para conocer esta distribución de muestreo. Derivando esta distribución acumulada obtenemos su densidad, que es \\[f(x) = nx^{n-1}\\] para \\(x\\in [0,1]\\), y es cero en otro caso. Si comparamos con nuestra simulación: teorica &lt;- tibble(x = seq(0, 1 ,0.001)) %&gt;% mutate(f_dens = 15 * x^14) sim_estimador_3 &lt;- replicar_muestreo_unif(max, 4000, 15) ggplot(sim_estimador_3) + geom_histogram(aes(x = estimacion), breaks = seq(0, 1, 0.02)) + xlim(c(0.5, 1)) + # el histograma es de ancho 0.02 y el número de simulaciones 4000 geom_line(data = teorica, aes(x = x, y = (4000 * 0.02) * f_dens), colour = &quot;red&quot;, size = 1.3) Y vemos que con la simulación obtuvimos una buena aproximación Nota: ¿cómo se relaciona un histograma con la función de densidad que genera los datos? Supón que \\(f(x)\\) es una función de densidad, y obtenemos un número \\(n\\) de simulaciones independientes. Si escogemos un histograma de ancho \\(\\Delta\\), ¿cuántas observaciones esperamos que caigan en un intervalo \\(I = [a - \\Delta/2, a + \\Delta/2]\\)?. La probabilidad de que una observación caiga en \\(I\\) es igual a \\[P(X\\in I) = \\int_I f(x)\\,dx = \\int_{a - \\Delta/2}^{a + \\Delta/2} f(x)\\,dx \\approx f(a) \\text{long}(I) = f(a) \\Delta\\] para \\(\\Delta\\) chica. Si nuestra muestra es de tamaño \\(n\\), el número esperado de observaciones que caen en \\(I\\) es entonces \\(nf(a)\\Delta\\). Eso explica el ajuste que hicimos en la gráfica de arriba. Otra manera de hacer es ajustando el histograma: si en un intervalo el histograma alcanza el valor \\(y\\), \\[f(a) = \\frac{y}{n\\Delta}\\] teorica &lt;- tibble(x = seq(0, 1 ,0.001)) %&gt;% mutate(f_dens = 15*x^{14}) ggplot(sim_estimador_3) + geom_histogram(aes(x = estimacion, y = ..density..), breaks = seq(0, 1, 0.02)) + xlim(c(0.5, 1)) + # el histograma es de ancho 0.02 y el número de simulaciones 4000 geom_line(data = teorica, aes(x = x, y = f_dens), colour = &quot;red&quot;, size = 1.3) Ejemplo Supongamos que las \\(X_i\\)’s son independientes y exponenciales con tasa \\(\\lambda &gt; 0\\). ¿Cuál es la distribución de muestreo de la suma \\(S = X_1 + \\cdots + X_n\\)? Sabemos que la suma de exponenciales independientes es una distribución gamma con parámetros \\((n, \\lambda)\\), y esta es la distribución de muestreo de nuestra estadística \\(S\\) bajo las hipótesis que hicimos. Podemos checar este resultado con simulación, por ejemplo para una muestra de tamaño \\(n=15\\) con \\(\\lambda = 1\\): replicar_muestreo_exp &lt;- function(est = estimador_1, m, n = 15, lambda = 1){ valores_est &lt;- map_dbl(1:m, ~ est(rexp(n, lambda))) tibble(id_muestra = 1:m, estimacion = valores_est) } sim_estimador_1 &lt;- replicar_muestreo_exp(sum, 4000, n = 15) teorica &lt;- tibble(x = seq(0, 35, 0.001)) %&gt;% mutate(f_dens = dgamma(x, shape = 15, rate = 1)) # graficar aprox de distribución de muestreo ggplot(sim_estimador_1) + geom_histogram(aes(x = estimacion, y = ..density..), bins = 40) + geom_line(data = teorica, aes(x = x, y = f_dens), colour = &quot;red&quot;, size = 1.2) Teorema central del límite Si consideramos los ejemplos de arriba donde consideramos estimadores basados en una suma, total o una media —y en menor medida cuantiles muestrales—, vimos que las distribución de muestreo de las estadísticas que usamos tienden a tener una forma común. Estas son manifestaciones de una regularidad estadística importante que se conoce como el teorema central del límite: las distribuciones de muestreo de sumas y promedios son aproximadamente normales cuando el tamaño de muestra es suficientemente grande. Teorema central del límite Si \\(X_1,X_2, \\ldots, X_n\\) son independientes e idénticamente distribuidas con media \\(\\mu\\) y desviación estándar \\(\\sigma\\) finitas. Si el tamaño de muestra \\(n\\) es grande, entonces la distribución de muestreo de la media \\(\\bar{X}\\) es aproximadamente normal con media \\(\\mu\\) y desviación estándar \\(\\sigma/\\sqrt{n}\\), que escribimos como \\[\\bar{X} \\xrightarrow{} N \\left (\\mu, \\frac{\\sigma}{\\sqrt{n}} \\right)\\] Adicionalmente, la distribución de la media estandarizada converge a una distribución normal estándar cuando \\(n\\) es grande: \\[\\sqrt{n} \\, \\left( \\frac{\\bar{X}-\\mu}{\\sigma} \\right) \\xrightarrow{} N(0, 1)\\] El error estándar de \\(\\bar{X}\\) es \\(\\text{ee}(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\). Si tenemos una muestra, podemos estimar \\(\\sigma\\) con de la siguiente forma: \\[\\hat{\\sigma} =\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2}\\] o el más común (que explicaremos más adelante) \\[\\hat{s} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2}\\] Este hecho junto con el teorema del límite central nos dice cuál es la dispersión, y cómo se distribuyen las posibles desviaciones de la media muestral alrededor de la verdadera media poblacional. ¿Qué tan grande debe ser \\(n\\). Depende de cómo es la población. Cuando la población tiene una distribución muy sesgada, por ejemplo, \\(n\\) típicamente necesita ser más grande que cuando la población es simétrica si queremos obtener una aproximación “buena”. "],
["tareas.html", "Tareas 1. Anáslisis Exploratorio 2. Loess 3. Tipos de estudio y PGD 4. Pruebas de hipótesis visuales y permutación", " Tareas Las tareas se envían por correo a teresa.ortiz.mancera@gmail.com con título: fundamentos-tareaXX (donde XX corresponde al número de tarea, 01..). Las tareas deben incluir código y resultados (si conocen Rmarkdown es muy conveniente para este propósito). 1. Anáslisis Exploratorio Realicen los ejercicios del script 01_exploratorio.R que vimos la clase pasada (RStudio.cloud proyecto 01-exploratorio). Escriban las respuestas en un reporte que incluya código y resultados (puede ser word, html, pdf,…). 2. Loess La tarea 2 es el proyecto de RStudio.Cloud con este nombre, el ejercicio está en un archvio de R Markdown, para los que son nuevos con R Markdown deben escribir su código en los bloques de código (secciones grises entre ```{r} codigo ```) una vez que escriban sus respuestas pueden generar el reporte tejiendo el documento, para esto deben presionar el botón Knit. Envíen el reporte por correo electrónico (con título fundamentos-tarea02). Solución: Series de tiempo Podemos usar el suavizamiento loess para entender y describir el comportamiento de series de tiempo, en las cuales intentamos entender la dependencia de una serie de mediciones indexadas por el tiempo. Típicamente es necesario utilizar distintas componentes para describir exitosamente una serie de tiempo, y para esto usamos distintos tipos de suavizamientos. Veremos que distintas componentes varían en distintas escalas de tiempo (unas muy lentas, cono la tendencia, otras más rapidamente, como variación quincenal, etc.). En el siguiente ejemplo consideramos la ventas semanales de un producto a lo largo de 5 años. Veamos que existe una tendencia a largo plazo (crecimientos anuales) y también que existen patrones de variación estacionales. library(tidyverse) ventas &lt;- read.csv(&quot;./data/ventas_semanal.csv&quot;) head(ventas) ## period sales.kg ## 1 1 685.537 ## 2 2 768.234 ## 3 3 894.643 ## 4 4 773.501 ## 5 5 954.600 ## 6 6 852.853 ggplot(ventas, aes(x = period, y = sales.kg)) + geom_line(size = 0.3) Intentaremos usar suavizamiento para capturar los distintos tipos de variación que observamos en la serie. En primer lugar, si suavizamos poco (por ejemplo \\(\\aplha = 0.1\\)), vemos que capturamos en parte la tendencia y en parte la variación estacional. ggplot(ventas, aes(x = period, y = log(sales.kg))) + geom_line(size = 0.3) + geom_smooth(method = &quot;loess&quot;, span = 0.1, method.args = list(degree = 1), se = FALSE, size = 1, color = &quot;red&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Es mejor comenzar capturando la tendencia, y poco de la componente estacional: ggplot(ventas, aes(x = period, y = log(sales.kg))) + geom_line(size = 0.3) + geom_smooth(method = &quot;loess&quot;, span = 0.3, method.args = list(degree = 1), se = FALSE, size = 1, color = &quot;red&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ajuste.trend.1 &lt;- loess(log(sales.kg) ~ period, ventas, span = 0.3, degree = 1) ventas$trend.1 &lt;- ajuste.trend.1$fitted ventas$res.trend.1 &lt;- ajuste.trend.1$residuals Ahora calculamos los residuales de este ajuste e intentamos describirlos mediante un suavizamiento más fino. Verificamos que hemos estimado la mayor parte de la tendencia, e intentamos capturar la variación estacional de los residuales. ggplot(ventas, aes(x = period, y = res.trend.1)) + geom_line(size = 0.3) + geom_smooth(method = &quot;loess&quot;, span = 0.15, method.args = list(degree = 1), se = FALSE, size = 1, color = &quot;red&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ajuste.est1.1 &lt;- loess(res.trend.1 ~ period, ventas, span = 0.15, degree = 1) ventas$est1.1 &lt;- ajuste.est1.1$fitted ventas$res.est1.1 &lt;- ajuste.est1.1$residuals Y graficamos los residuales obtenidos después de ajustar el componente estacional para estudiar la componente de mayor frecuencia. ggplot(ventas, aes(x = period, y = res.est1.1)) + geom_line(size = 0.3) + geom_smooth(method = &quot;loess&quot;, span = 0.06, method.args = list(degree = 1), se = FALSE, size = 1, color = &quot;red&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ajuste.est2.1 &lt;- loess(res.est1.1 ~ period, ventas, span = 0.06, degree = 1) ventas$est2.1 &lt;- ajuste.est2.1$fitted ventas$res.est2.1 &lt;- ajuste.est2.1$residuals Ahora que tenemos nuestra primera estimación de cada una de las componentes, podemos regresar a hacer una mejor estimación de la tendencia. La ventaja de volver es que ahora podemos suavizar más sin que en nuestra muestra compita tanto la variación estacional. Por tanto podemos suavizar menos: ventas$sales.sin.est.1 &lt;- log(ventas$sales.kg) - ajuste.est1.1$fitted - ajuste.est2.1$fitted ggplot(ventas, aes(x = period, y = sales.sin.est.1)) + geom_line(size = 0.3) + geom_smooth(method = &quot;loess&quot;, span = 0.09, method.args = list(degree = 1), se = FALSE, size = 1, color = &quot;red&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ajuste.trend.2 &lt;- loess(sales.sin.est.1 ~ period, ventas, span = 0.08, degree = 1) ventas$trend.2 &lt;- ajuste.trend.2$fitted ventas$res.trend.2 &lt;- log(ventas$sales.kg) - ventas$trend.2 Y ahora nos concentramos en la componente anual. ventas$sales.sin.est.2 &lt;- log(ventas$sales.kg) - ajuste.trend.2$fitted - ajuste.est2.1$fitted ggplot(ventas, aes(x = period, y = sales.sin.est.2)) + geom_line(size = 0.3) + geom_smooth(method = &quot;loess&quot;, span = 0.15, method.args = list(degree = 2), se = FALSE, size = 1, color = &quot;red&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ajuste.est1.2 &lt;- loess(sales.sin.est.2 ~ period, ventas, span = 0.15, degree = 1) ventas$est1.2 &lt;- ajuste.est1.2$fitted ventas$res.est1.2 &lt;- ajuste.est1.2$residuals Finalmente volvemos a ajustar la componente de frecuencia más alta: ventas$sales.sin.est.3 &lt;- log(ventas$sales.kg) - ajuste.trend.2$fitted - ajuste.est1.2$fitted ggplot(ventas, aes(x = period, y = sales.sin.est.3)) + geom_line(size = 0.3) + geom_smooth(method = &quot;loess&quot;, span = 0.06, method.args = list(degree = 1), se = FALSE, size = 1, color = &quot;red&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ajuste.est2.2 &lt;- loess(sales.sin.est.3 ~ period, ventas, span = 0.06, degree = 1) ventas$est2.2 &lt;- ajuste.est2.2$fitted ventas$res.est2.2 &lt;- ajuste.est2.2$residuals Verificamos nuestra descomposición y visualizamos el ajuste: ventas$log.sales &lt;- log(ventas$sales.kg) ventas.2 &lt;- dplyr::select(ventas, period, trend.2, est1.2, est2.2, res.est2.2, log.sales) max(abs(apply(ventas.2[, 2:4], 1, sum) - ventas.2$log.sales)) ## [1] 0.240316 ventas.2.m &lt;- gather(ventas.2, componente, valor, -period) ventas.2.m.c &lt;- ventas.2.m %&gt;% group_by(componente) %&gt;% mutate( valor.c = valor - mean(valor), componente = factor(componente, c(&quot;trend.2&quot;, &quot;est1.2&quot;, &quot;est2.2&quot;, &quot;res.est2.2&quot;, &quot;log.sales&quot;)) ) ggplot(ventas.2.m.c, aes(x = period, y = valor.c)) + geom_vline(xintercept = c(0, 52 - 1, 52 * 2 - 1, 52 * 3 - 1, 52 * 4 - 1), color = &quot;gray&quot;) + geom_line(size = 0.3) + facet_wrap(~ componente, ncol = 1) Y vemos que es razonable describir los residuales con una distribución normal (con desviación estándar alrededor de 8% sobre el valor ajustado): sd(ventas$res.est2.2) ## [1] 0.08093687 ventas.ord &lt;- arrange(ventas, res.est2.2) ventas.ord$q.normal &lt;- qnorm((1:nrow(ventas) - 0.5) / nrow(ventas)) ggplot(ventas.ord, aes(x = q.normal, y = res.est2.2)) + geom_point(size = 1.2) + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Aún no estamos convencidos de que podemos hacer pooling de los residuales. Para checar esto, vemos la gráfica de dependencia de residuales. ggplot(ventas, aes(x = period, y = res.est2.2)) + geom_line(size = 0.3) + geom_point(size = 1.2) ¿Puedes notar alguna dependencia en los residuales? library(nullabor) ventas.res &lt;- dplyr::select(ventas, period, res.est2.2) ventas.null &lt;- lineup(null_dist(var = &#39;res.est2.2&#39;, dist = &#39;normal&#39;, params = list(mean = 0, sd = 0.08)), n = 10, ventas.res) ## decrypt(&quot;bhMq KJPJ 62 sSQ6P6S2 ua&quot;) ggplot(ventas.null, aes(x = period, y = res.est2.2)) + facet_wrap(~ .sample, ncol = 2) + geom_line(size = 0.3) + geom_vline(xintercept = c(0, 52 - 1, 52 * 2 - 1, 52 * 3 - 1, 52 * 4 - 1), color = &quot;gray&quot;) + geom_point(size = 1.2) Hay dos cosas que nos falta explicar, en primer lugar, las caídas alrededor de principios/finales de cada año (que son de hasta -0.2), y segundo que esta gráfica parece oscilar demasiado. La estructura que aún no hemos explicado se debe a que las semanas que caen en quincena tienden a tener compras más grandes que las que están justo antes de quincena o fin de mes. Por el momento detendremos el análisis aquí y explicamos un proceso iterativo para proceder en nuestro análisis exploratorio: Iterando ajuste de loess. Cuando queremos ajustar con tres componentes: tendencia, estacionalidad y residuales, podemos seguir el siguiente proceso, Ajustar la primera componente a los datos (tendencia). Ajustar la segunda componente a los residuales del paso anterior (estacionalidad). Restar de los datos originales la segunda componente ajustada (estacionalidad). Ajustar a los residuales del paso anterior una nueva componente (tendencia). Restar a los datos originales la componente ajustada en el paso anterior. Ajustar a los residuales del paso anterior una nueva componente (estacionalidad). Checar ajuste y si es necesario iterar de 3 a 6 con las nuevas componentes. La idea es que cada componente compite para explicar los datos (cada una gana más al bajar el parámetro \\(\\alpha\\)). El conflicto es que si suavizamos mucho cada componente (por ejemplo la tendencia), entonces parte de la variación que debería ir en ella queda en los residuales, y se intenta ajustar posteriormente por una componente distinta (estacionalidad). Sin embargo, si suavizamos poco, entonces parte de la variación de la segunda componente es explicada por el ajuste de la primera. Entonces, la solución es ir poco a poco adjudicando variación a cada componente. En nuestro ejemplo de arriba, podemos comenzar suavizando de menos el primer ajsute de la tendencia, luego ajustar estacionalidad, restar a los datos originales esta estacionalidad, y ajustar a estos datos una componente más suave de tandencia. Es posible suavizar más la tendencia justamente porque ya hemos eliminado una buena parte de la estacionalidad. 3. Tipos de estudio y PGD Para cada uno de los siguientes estudios, ubícalos en el recuadro y contesta lo que se pide. Envíen las respuestas por correo electrónico (con título fundamentos-tarea03). Inferencia estadística de acuerdo al tipo del diseño (Ramsey and Schafer 2012). En 1930 se realizó un experimento en 20,000 niños de edad escolar de Inglaterra. Los maestros fueron los responsables de asignar a los niños de manera aleatoria al grupo de tratamiento -que consistía en recibir 350 ml de leche diaria - o al grupo de control, que no recibía suplementos alimenticios. Se registraron peso y talla antes y después del experimento. El estudio descubrió que los niños que recibieron la leche ganaron más en peso en el lapso del estudio. Una investigación posterior descubrió que los niños del grupo control eran de mayor peso y talla que los del grupo de intervención, antes de iniciar el tratamiento. ¿Qué pudo haber ocurrido? ¿Podemos utilizar los resultados del estudio para inferir causalidad? Supongamos que de los registros de un conjunto de doctores se slecciona una muestra aleatoria de individuos americanos caucásicos y de americanos de ascendencia china, con el objetivo de comparar la presión arterial de las dos poblaciones. Supongamos que a los seleccionados se les pregunta si quieren participar y algunos rechazan. Se compara la distribución de presión arterial entre los que accedieron a participar. ¿En que cuadro cae este estudio? ¿Qué supuesto es necesario para permitir inferencias a las poblaciones muestreadas? Un grupo de investigadores reportó que el consumo moderado de alcohol estaba asociado con un menor riesgo de demencia (Mukamal et al. (2003)). Su muestra consistía en 373 personas con demencia y 373 sin demencia. A los participantes se les pregintó cuánta cerveza, vino, o licor consumían. Se observó que aquellos que consumían de 1-6 bebidas por semana tenían una incidencia menor de demencia comparado a aquellos que se abstenían del alcohol. ¿se puede inferir causalidad? Un estudio descubrió que los niños que ven más de dos horas diarias de televisión tienden a tener mayores niveles de colesterol que los que ven menos de dos horas diarias. ¿Cómo se pueden utilizar estos resultados? Más gente se enferma de gripa en temporada de invierno, ¿esto prueba que las temperaturas bajas ocasionan las gripas? ¿Qué otras variables podrían estar involucradas? ¿Cuál es la diferencia entre un experimento aleatorizado y una muestra aleatoria? 4. Pruebas de hipótesis visuales y permutación La tarea 4 es el proyecto de RStudio.Cloud con este nombre, el ejercicio está en un archvio de R Markdown. Envíen el reporte por correo electrónico (con título fundamentos-tarea04). Solución: Pruebas pareadas En este ejemplo buscamos comparar la diferencia entre dos medicinas para dormir. - ID es el identificador de paciente, y medicina_1 y medicina_2 son las horas extras de sueño vs. no usar medicina. - Examina los datos. library(tidyverse) dormir &lt;- sleep %&gt;% pivot_wider(names_from = group, names_prefix = &quot;medicina_&quot;, values_from = extra) dormir ## # A tibble: 10 x 3 ## ID medicina_1 medicina_2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.7 1.9 ## 2 2 -1.6 0.8 ## 3 3 -0.2 1.1 ## 4 4 -1.2 0.1 ## 5 5 -0.1 -0.1 ## 6 6 3.4 4.4 ## 7 7 3.7 5.5 ## 8 8 0.8 1.6 ## 9 9 0 4.6 ## 10 10 2 3.4 La pregunta de interés es si una medicina es mejor que otra para prolongar el sueño. Nótese que en este caso, no tenemos grupos, sino mediciones repetidas. Escribe la hipótesis nula. No hay diferencia entre las medicinas. Nuestra estadśtica de interés es media de las diferencias entre las medicinas. Calcula la diferencia observada. dif_obs &lt;- dormir %&gt;% mutate(dif = medicina_1 - medicina_2) %&gt;% summarise(dif_obs = mean(dif)) %&gt;% pull(dif_obs) dif_obs ## [1] -1.58 Hay variación entre los pacientes. ¿Tenemos evidencia para rechazar que son iguales? ¿Cómo hacemos nuestra distribución de referencia? Bajo la hipótesis nula las medicinas son intercambiables, debemos la columna de medicina por individuo pues cada uno debe tener un valor de medicina 1 y uno de medicina 2. dormir_larga &lt;- dormir %&gt;% pivot_longer(cols = contains(&quot;medicina&quot;)) # Hacemos una permutación dormir_perm &lt;- dormir_larga %&gt;% group_by(ID) %&gt;% mutate(name = sample(c(&quot;medicina_1&quot;, &quot;medicina_2&quot;))) %&gt;% ungroup() # Calculamos la estadística en esta permutación dormir_perm %&gt;% pivot_wider(names_from = &quot;name&quot;, values_from = &quot;value&quot;) %&gt;% mutate(dif = medicina_1 - medicina_2) %&gt;% summarise(media_dif = mean(dif)) %&gt;% pull(media_dif) ## [1] 0.18 # Lo hacemos una función calcula_est &lt;- function(){ dormir_perm &lt;- dormir_larga %&gt;% group_by(ID) %&gt;% mutate(name = sample(c(&quot;medicina_1&quot;, &quot;medicina_2&quot;))) %&gt;% ungroup() dormir_perm %&gt;% pivot_wider(names_from = &quot;name&quot;, values_from = &quot;value&quot;) %&gt;% mutate(dif = medicina_1 - medicina_2) %&gt;% summarise(media_dif = mean(dif)) %&gt;% pull(media_dif) } # La llamamos 1000 veces calcula_est() ## [1] 0.46 difs &lt;- rerun(1000, calcula_est()) %&gt;% flatten_dbl() Haz una gráfica de la distribución de referencia y grafica encima el valor observado en los datos originales. perms &lt;- tibble(sims = 1:1000, difs = difs) ggplot(perms, aes(x = difs)) + geom_histogram(binwidth = 0.15) + geom_vline(xintercept = dif_obs, color = &quot;red&quot;) Referencias "],
["referencias.html", "Referencias", " Referencias "]
]
